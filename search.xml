<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[博客导航]]></title>
    <url>%2F2018%2F05%2F20%2F%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA%2F</url>
    <content type="text"><![CDATA[论文笔记 目标检测Mask-R-CNN 实例分割 人脸识别]]></content>
  </entry>
  <entry>
    <title><![CDATA[Learning to Segment Every Thing]]></title>
    <url>%2F2018%2F05%2F20%2FMask-X-R-CNN%2F</url>
    <content type="text"><![CDATA[这篇文章建立在Mask R-CNN上，提出了一种新的半监督训练和权重迁移方程。在类别数量很大的训练集上每个图像中的实例都有box标记，但是只有一小部分有掩码标记，这篇文章提出的方法可以在这样的数据集上训练实例分割模型。主要的贡献就是训练Mask R-CNN去检测和分割3000种实例，box标记使用Visual Genome数据集，掩码标记来自于COCO数据集的80个类别。 Introduction实例分割可以预测出前景的分割掩码，如Mask R-CNN。但实际上，分割系统涉及的类别很少，而且目前的分割算法要求强大的监督学习，但是新类别的掩码标记代价太高，因此这种监督学习会受到限制。相比之下，bounding box的标记代价较低，所以问题来了：对于所有类别，在没有完整的掩码标记的情况下，是否有可能训练出一个高性能的实例分割模型呢？这篇文章就是在这样的动机下提出了一种部分监督的（partially supervised）实例分割任务，并且设计一种新的迁移学习方法去解决上面提到的问题。 partially supervised实例分割任务： 给定一个感兴趣的类别集合，其中一小部分子集拥有实例分割掩码，然而其他类别只有bounding box标记。 实例分割算法应该利用这些数据拟合出一个模型，它能够分割出所有属于感兴趣目标类别的实例。 把那些同时具有bounding box标价和掩码标记的样本称为强标记样本（strongly annotated examples），而只有bounding box标记的样本称为弱标记样本（weakly annotated examples）。 为了实现这种实例分割，提出了一种构建在Mask R-CNN上的迁移学习方法。Mask R-CNN将实例分割任务分解成两个子任务：bounding box目标检测和掩码预测。每个子任务都有一个专门的网络”head”，使用联合训练。本文提出的方法背后的直觉是：一旦经过训练，bounding box head的参数为每一个目标类别编码一个embedding，使得对于这一个类别的视觉信息迁移到分割任务的head上。 为此，设计了一个参数化的权重迁移方程，该方程通过训练预测一个类别的实例分割参数，作为这个类别的bounding box检测参数。权重迁移方程可以在Mask R-CNN上进行端对端的训练，使用类别标签和掩码标记作为监督。预测过程，权重迁移方程为每一个类别预测实例分割参数，因此使得模型可以分割所有类别的目标，包括那些训练时没有掩码的类别。 作者对两种设置进行了探索： 使用COCO数据集模拟半监督实例分割任务，具体过程是：将COCO数据集分类两个不相交的子集，一个子集拥有掩码标记，另一个子集只能访问bounding box标记。实验表明，在没有训练掩码的类别上，本文的方法将掩码AP值提高了40%。 使用Visual Genome(VG)数据集的3000个类别，训练大规模的实例分割模型。这个数据集对于大部分目标类别都有bounding box标记，但是定量评估较难，因为很多类别在语义上是重叠的，比如是近义词；并且标记并不详尽，因此精确率和召回率都很难衡量；除此之外VG数据集没有实例掩码。因此，本文使用VG数据集提供大规模实例分割模型的定性输出。 下图中绿色是训练过程中有掩码标记的类别，红色的是训练中只有bounding box标记的类别。 目标类别也许可以通过视觉空间的连续嵌入向量模拟，在这个空间中临近的向量通常在外观或语义本体上很接近。这篇文章的工作中，Mask R-CNN的box head参数包括了类别特定的外观信息，可以被视为是通过训练bounding box目标检测任务学习到的嵌入向量。类嵌入向量通过与视觉相关类共享外观信息使得本文中模型的迁移学习可行。本文的核心思想就是利用迁移学习将bounding box检测学习的知识迁移到实例分割任务中，使得对于没有掩码标记的类别，也能够很好地分割出实例。 Learning to Segment Every Thing设$C$为目标类别集合，对于这个集合要训练一个实例分割模型。大多数已有的方法假设这个集合中所有训练样本都被标记了实例掩码。本文放松了这个要求，假设$C=A \bigcup B$，其中集合A有掩码，而集合B只有bounding box标记，B集合中类别的样本关于实例分割任务是弱标记的。 实例分割模型比如Mask R-CNN，它有bounding box检测和掩码预测两个部分，本文提出$Mask^X R-CNN$方法将模型bounding box检测器获取的类别特定的信息迁移到实例掩码预测中。 Mask Prediction Using Weight TransferMask R-CNN可以被视为对Faster R-CNN检测模型的扩增，它带有一个小的FCN掩码预测分支。在预测阶段，掩码分支对每一个检测到的目标预测它的分割掩码，在训练阶段，掩码分支和Faster R-CNN中的标准的bounding box head联合训练。 在Mask R-CNN中，bounding box分支和掩码分支的最后一层都包含了类别特定的参数，分别用来实现bounding box分类和实例掩码预测。如下图： 本文不再独立地学习类别特定的bounding box参数和掩码参数，而是使用通用的、类别无关的权重迁移方程作为整个模型的一部分进行联合训练，从一个类别的bounding box参数去预测它的掩码参数。 对于一个给定类别$c$，设$w_{det}^c$为bounding box head最后一层类别特定的目标检测权重，$w_{seg}^c$为掩码分支类别特定的掩码权重。与Mask R-CNN不同，这里掩码权重不再作为模型参数，而是使用一个通用的权重预测方程$\tau$来对它参数化： $$w_{seg}^c=\tau(w_{det}^c;\theta)$$ 这里$\theta$是类别无关的可学习参数。同一个迁移方程被用到任何其他类别上，因此$\theta$的设置应该使得迁移方程对训练过程中没有掩码的那些类别具有很好的泛化性能。作者在这里认为这种泛化是可能的，因为类别特定的检测权重$w_{det}^c$可以被视为这个类别的一种基于外观的视觉嵌入（Visual embeddings）。这里我个人并不是非常理解视觉嵌入相关的知识，有待挖掘。 继续说这个迁移方程，它是用一个小的全连接神经网络实现的。下图阐明了权重迁移方程拟合Mask R-CNN到形成$Mask^X R-CNN$的过程，阴影部分是Mask R-CNN: 前面与Mask R-CNN相同，图像输入给ConvNet，然后经过RPN和RoIAlign，而后是两个分支，box head和mask head 不再单独地学习掩码参数$w_{seg}$，而是将它相应的box检测参数$w_{det}$输入给权重迁移方程，从而获得一个类别的掩码权重 对于训练，迁移方程只需要集合A中类别的掩码，但是在测试阶段，它可以应用在$A \bigcup B$的所有类别上 使用一个互补的类别无关的全连接多层感知机（MLP）扩增了mask head，这其实与Mask R-CNN中的FCN mask head是一种互补，后面会解释这种互补。 一个细节：bounding box head包含两种检测权重：RoI分类权重$w_{cls}^c$和bounding box回归权重$w_{box}^c$。这篇文章的实验只使用一种权重：$w_{det}^c=w_{cls}^c~or~w_{det}^c=w_{box}^c$，或者使用两种权重的拼接：$w_{det}^c=[w_{cls}^c,w_{box}^c]$。 说一下个人的理解：Mask R-CNN中做的是检测到一个box，然后利用类别特定的分割权重去预测这个box里实例的掩码。这两部分是并行分支，检测权重和分割权重都是类别特定的，分别编码了各自的特征空间，并且在监督学习下进行。在缺少某些类别的掩码标记时，就学习不到这个类别的掩码权重，从而无法预测到这类实例的掩码。这个问题在这篇文章中得到了很好的解决：作者加入一个小的全连接神经网络（迁移方程）打通了两个并行分支，学习的是这两种特征空间编码之间的映射，尽管分割任务的学习缺少某些类别的掩码标记，可是通过已有的数据，只要学习到这种映射关系，那么自然地就可以得到那些没有掩码标记的类别的分割权重。 Training训练bounding box head使用的是在$A \bigcup B$ 所有类别上的box 检测损失，但是只使用A中类别的掩码损失训练mask head。提出两种方案： 逐阶段训练（stage-wise）：Mask R-CNN可以被看做是Faster R-CNN加一个掩码分支，因此分成检测训练和分割训练，第一个阶段只使用$A \bigcup B$中的bounding box标记训练Faster R-CNN；第二个阶段保持卷积特征和box head固定，训练mask head。这样的话，每一个类别特定的检测权重$w_{det}^c$可以被视为固定的类嵌入向量（class embedding vectors），它在训练的第二阶段不需要更新。 端到端的联合训练：在Mask R-CNN中已经证明多任务训练比单独训练每一个任务会有更好的性能，前面提到的逐阶段训练方式可能会导致性能低下。理论上也可以直接使用两个集合的box损失和集合A的掩码损失，进行反向传播，然而这也许会导致在集合A与B之间类别特定的检测权重会有差异性。因为对于一个类别$c$如果它属于A，只有$w_{det}^c$会接收到掩码损失经过权重迁移方程回传的梯度。也就是说，A中那些类别的检测权重既能接收到bounding box损失的梯度，又能接收到掩码损失的梯度；而B中那些类别的检测权重只能收到bounding box损失的梯度。但是目的是要在两个集合之间得到同样的检测权重，从而使得在集合A上训练的类别特定的分割权重$w_{seg}^c$很好地泛化到集合B。因此采取了一个简单的方法：在反向传播掩码损失时，阻止关于$w_{det}^c$的梯度，也就是说，回传掩码损失的梯度时，只计算预测掩码权重关于迁移方程参数$\theta$的梯度，而不计算关于$w_{det}^c$的梯度：$w_{seg}^c=\tau(stop_grad(w_{det}^c);\theta)$。 Extension: Fused FCN+MLP Mask Heads在Mask R-CNN中使用类别无关的FCN head，将其作为baseline。 Mask R-CNN中考虑了两种mask head：一是FCN head，使用全卷积网络预测MxM的掩码；二是MLP head，使用全连接层组成的多层感知机预测掩码，这点与DeepMask更相似。在Mask R-CNN中，FCN head具有更高的mask AP值，然而这两种设计可能是互补的。直觉上，MLP掩码预测也许更好地捕获了目标的“要点”（全局），而FCN掩码预测也许更好地捕获了目标的细节（局部），比如目标边界。基础这样的观察，提出一种改进，将类别无关的FCN和权重迁移方程以及类别无关的MLP掩码预测融合。 融合K类类别无关的掩码预测（1XMxM）和类别特定的掩码预测(KxMxM)时，这两个分数被加到最后的KxMxM的输出上。这里，类别无关的掩码预测会被平铺K次。然后，这个KxMxM的掩码分数经过一个sigmoid单元被变为每一类的掩码概率，再被缩放到实际的bounding box尺寸作为最终那个bounding box的实例掩码。 ExperimentsResults and Comparison of Our Full Method作者做了很多对比试验，最终选择使用迁移方程+MLP以及类别无关的FCN head融合的模型，其中迁移方程的实现使用两种权重的拼接，2层的MLP，LeakyReLU作为激活函数，整个网络采取端对端的训练。 Large-Scale Instance Segmentation$Mask^X R-CNN$模型的训练，使用VG数据集，包含108077张图像，超过7000类同义词集合，标记了bounding box，但是没有掩码。训练中，选择3000个最常见的同义词集合作为类别集合$A \bigcup B$用来实例分割，它覆盖了COCO中的80个类别。因为这两个数据集有大量的重叠，因此在VG上训练时只采用没有在COCOval2017中出现的那些，VG中剩下的图像作为验证集。把VG中与COCO重叠的80个类作为集合A，带有掩码标记；剩下的2920个类别作为集合B，只有bounding box。 作者最终训练出了一个可以分割3000类实例的$Mask^X R-CNN$，如题“Learning to Segment Every Thing”。 Reference1.Learning to Segment Every Thing]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask-R-CNN]]></title>
    <url>%2F2018%2F05%2F06%2FMask-R-CNN%2F</url>
    <content type="text"><![CDATA[这篇论文提出了一种概念简单，灵活且通用的目标实例分割框架，在检测出图像中目标的同时，生成每一个实例的掩码（mask）。对Faster R-CNN进行扩展，通过添加与已存在的bounding box回归平行的一个分支，预测目标掩码，因而称为Mask R-CNN。这种框架训练简单，容易应用到其他任务，比如目标检测，人体关键点检测。 Introduction实例分割的挑战性在于要求正确地检测出图像中的所有目标，同时精确地分割每一个实例。这其中包含两点内容： 目标检测：检测出目标的bounding box，并且给出所属类别； 语义分割（semantic segmentation）：分类每一个像素到一个固定集合，不用区分实例。 Mask R-CNN对Faster R-CNN进行了扩展，在Faster R-CNN分类和回归分支的基础上，添加了一个分支网络去预测每一个RoI的分割掩码，把这个分支称为掩码分支。掩码分支是应用在每一个RoI上的一个小的FCN，以像素到像素的方式（pixel-to-pixel）预测分割掩码。 Faster R-CNN在网络的输入和输出之间没有设计像素到像素的对齐。在how RoIPool文中提到：实际上，应用到目标上的核心操作执行的是粗略的空间量化特征提取。为了修正错位，本文提出了RoIAlign，可以保留准确的空间位置，这个改变使得掩码的准确率相对提高了10%到50%。解耦掩码和分类也至关重要，本文对每个类别独立地预测二值掩码，这样不会跨类别竞争，同时依赖于网络的RoI分类分支去预测类别。 模型在GPU上运行每帧200ms，在8 GPU的机器上训练COCO数据集花费了一到两天。最后，通过COCO关键点数据集上的人体姿态估计任务来展示框架的通用性。通过将每个关键点视为一位有效编码（one-hot），即所有关键点编码成一个序列，但只有一个是1，其余都是0。只需要很少的修改，Mask R-CNN可以应用于人体关键点检测。不需要额外的技巧，Mask R-CNN超过了COCO 2016人体关键点检测比赛的冠军，同时运行速度可达5FPS。 Related Work早前的实例分割方法受R-CNN有效性的推动，基于分割proposal，也就是先提取分割候选区，然后进行分类，分割先于分类的执行。本文的方法是同时预测掩码和类别，更加简单和灵活。 FCIS（fully convolutional instance segmentation）用全卷积预测一系列位置敏感的输出通道，这些通道同时处理目标分类，目标检测和掩码，这使系统速度变得更快。但FCIS在重叠实例上出现系统错误，并产生虚假边缘。 另一类方法受语义分割的推动，将同类别的像素划分到不同实例中，这是一种分割先行的策略。Mask R-CNN与其相反，基于实例先行的策略（segmentation-first strategy）。 Mask R-CNNMask R-CNN在Faster R-CNN上加了一个分支，因此有三个输出：目标类别、bounding box、目标掩码。但是掩码输出与其他输出不同，需要提取目标更精细的空间布局。Mask R-CNN中关键的部分是像素到像素的对齐，这在Fast/Faster R-CNN里是缺失的。 首先回归一下Faster R-CNN：它包含两个阶段，第一阶段使用RPN提取候选的目标bounding box，第二阶段本质上是Fast R-CNN，使用RoI pooling从候选区域中提取特征，实现分类并得到最终的bounding box。 Mask R-CNN也是两个阶段：第一阶段与Faster R-CNN相同，RPN提取候选目标bounding box；第二阶段，除了并行地预测类别和候选框偏移，还输出每一个RoI的二值掩码（binary mask）。 损失函数 多任务损失：$$L=L_{cls}+L_{box}+L_{mask}$$ 掩码分支对每一个感兴趣区域产生$Km^2$维的输出，K是类别数目，K个分辨率为m×m的二值掩码也就是针对每一个类别产生了一个掩码。 对每一个像素应用sigmoid，所以掩码损失就是平均二分类交叉熵损失。如果一个RoI对应的ground truth是第k类，那么计算掩码损失时，只考虑第k个掩码，其他类的掩码对损失没有贡献。 掩码损失的定义允许网络为每个类别独立预测二值掩码。使用专门的分类分支去预测类别标签，类别标签用来选择输出掩码。 掩码表达 掩码编码了输入目标的空间布局。掩码的空间结构，可以通过卷积产生的那种像素到像素的对应关系来提取。 使用FCN为每个RoI预测一个m×m的掩码。这允许掩码分支中的每个层显式的保持m×m的目标空间布局，而不会将其缩成缺少空间维度的向量表示。 像素到像素的对应需要RoI特征（它们本身就是小特征图）被很好地对齐，以准确地保留显式的像素空间对应关系。 RoI Align首先说明为什么需要对齐，下图中左边是ground truth，右边是对左边的完全模仿，需要保持位置和尺度都一致。平移同变性（translation equivariance）就是输入的改变要使输出也响应这种变化。 分类要求平移不变的表达，无论目标位置在图中如何改变，输出都是那个标签 实例分割要求同变性：具体的来说，就是平移了目标，就要平移掩码；缩放了目标就要缩放掩码 全卷积网络FCN具有平移同变性，而卷积神经网络中由于全连接层或者全局池化层，会导致平移不变。 在Faster R-CNN中，提取一张完整图像的feature map，输入RPN里提取proposal，在进行RoI pooling前，要根据RPN给出的proposal信息在基础网络提取出的整个feature map上找到每个proposal对应的那一块feature map，具体的做法是：根据RPN给出的边框回归坐标，除以尺度因子16，因为vgg16基础网络四次池化缩放了16倍。这里必然会造成坐标计算会出现浮点数，而Faster R-CNN里对这个是进行了舍入，这是一次对平移同变性的破坏；同样的问题出现在后面的RoI pooling中，因为要得到固定尺寸的输出，所以对RoI对应的那块feature map划分了网格，也会出现划分时，对宽高做除法出现浮点数，这里和前面一样，简单粗暴地进行了舍入操作，这是第二次对平移同变性的破坏。如下图，网格的划分是不均匀的： 总之，Faster R-CNN破坏了像素到像素之间的这种平移同变性。RoI Align就是要在RoI之前和之后保持这种平移同变性，避免对RoI边界和里面的网格做量化。如下图： 针对输入的feature map找到对应的RoI，是通过$x/16$而不是像Faster R-CNN中$[x/16]$，$[\cdot]$代表舍入操作。所以可以看到第一幅图中RoI并没有落在整数的坐标上。 对RoI划分为2x2的网格（根据输出要求），每个小的网格里采样4个点，使用双线性插值根据临近的网格点计算这4个点的值，最后再对每一个网格进行最大池化或平均池化得到最终2x2的输出。 network下图中，是两个不同的network head，左图是ResNet C4，右边是FPN主干，这两种结构上都添加了一个掩码分支，反卷积使用2x2的卷积核，stride为2；除了输出层是1x1的卷积，其他部分的卷积都是3x3的。 实现细节 掩码损失只定义在正的RoI上 输入图像被缩放到短边为800，每个图像采样N个RoI（ResNet的N=64，FPN的N=512），batch size = 2，正负样本的比例为1:3。 测试中对于ResNet架构，生成300个proposal，FPN则是1000。 将得分最高的100个检测框输入掩码分支，对每一个RoI预测出K个掩码，但是最终只根据分类分支的预测结果选择相应的那一个类别的掩码。 mxm的浮点数掩码输出随后被缩放到RoI尺寸，然后以0.5的阈值进行二值化。 Reference Mask R-cNN]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Feature Pyramid Networks for Object Detection]]></title>
    <url>%2F2018%2F05%2F06%2FFPN%2F</url>
    <content type="text"><![CDATA[这篇文章提出特征金字塔网络（FPN），将分辨率高语义性弱的浅层特征和分辨率低语义性强的深层特征融合，形成了多级金字塔，在金字塔每一级上独立检测目标。FPN不仅对多尺度的目标检测具有很好的效果，还可以应用到分割任务中。 Introduction识别不同尺度的目标是计算机视觉的一项基本挑战，下面介绍四种利用特征的形式： 图像金字塔：构建在图像金字塔上的特征金字塔（简称为特征化的图像金字塔），如图（a）。这种情况下，图像被采样为多种尺度，然后生成不同尺度的特征。DMP就是使用密集的尺度采样获得了不错的效果。这种方法被大量用在手工设计的特征中。优点是：每一级的特征语义信息都比较强，缺点是预测时间长。在图像金子塔上端到端地训练深度卷积神经网络是不切实际的，因为内存消耗大，所以即使要用，也只用在做预测时。 利用卷积神经网络提取特征，使用最后一层的特征做预测。卷积神经网络可以提取高级的语义表达，对尺度变化有更好的鲁棒性，所以可以使用单尺度特征，如图（b）。 使用不同层的特征进行预测，如图（c）。SSD使用卷积神经网络多个层的特征分别做预测，如同一个特征化的图像金字塔。SSD重复利用前向传播过程中计算好的不同层特征，所以几乎没有带来额外代价。但SSD没有利用到足够底层的特征，因为底层特征语义信息弱，但是底层特征分辨率高，对检测小目标很重要。 本文提出的FPN，将低分辨率语义信息强的浅层特征与高分辨率语义信息弱的深层特征进行组合，构建特征金字塔，在金字塔的每一级上分别做预测，如图（d）。 还有一种相似的结构，采用自顶向下的方法和跳跃连接（skip connection），目的是生成单个具有较好分辨率的高级特征图，然后在这个特征图上做预测，如下面上半部分的图。本文与其结构很接近，但是利用它形成一个金字塔，在金字塔的每一级独立地进行预测。 Feature Pyramid NetworksFPN接受一个具有任意尺寸的单尺度图像作为输入，在多个层级以全卷积的形式生成不同尺寸比例的特征图。FPN的构建涉及三个部分：自底向上的路径（Bottom-up pathway），自顶向下的路径（top-down pathway），横向连接（lateral connection）。 自底向上的路径自底向上的路径就是主干网络（backbone）的前向计算，产生不同尺度的feature map，尺度的比例为2，即每次下采样都是缩小2倍。自底向上的过程，空间分辨率降低，但是语义性增加。 这里把那些会输出同样尺寸feature map的层归为一个stage。每个stage都定义了一级金字塔，每个stage的最后一层特征被选取出来。本文的backbone是ResNet，选取的是每一个stage最后一个残差块的输出，将Conv2，Conv3，Conv4，Conv5的输出定义为{C2，C3，C4，C5}。这些输出相对于输入图像，stride为{4，8，16，32}，即分辨率缩小的倍数。为啥不用C1呢，因为维度太高了，内存消耗大。 自顶向下和横向连接FPN通过自顶向下的方式，从语义信息丰富的层出发，构建出分辨率更高的层，将这些层的特征与浅层的特征通过横向连接融合。如下图，对于空间分辨率较粗糙的深层特征，进行2倍的上采样（最近邻），相应的浅层特征使用1x1的卷积降维，之后与上采样的特征通过逐元素相加合并。合并后的特征又通过3x3的卷积生成最终的feature map，即{P2，P3，P4，P5}，这一步降低了上采样的混叠效应（aliasing effect）。 FPN可以更加详细的用下图表示： 金字塔中所有的层都共享分类器和回归器，就像传统的图像金字塔那样。固定特征的通道数为256，因此所有额外的层输出都为256通道。这些额外的层没有使用非线性。 ApplicationsFPN for RPN回顾一下Faster R-CNN中的RPN： 预先定义了一组不同尺度和高宽比的anchor，覆盖不同形状的目标。 在最后一个单尺度的共享卷积层输出的feature map上，使用一个3×3的滑动窗（卷积核），随后是两路1x1的卷积，分别用来分类是否为目标以及回归bounding box，这里将这一部分称为网络的头（head）。 将FPN应用在RPN上的要点： 将单尺度的feature map替换为FPN，为每一级金字塔附加一个head，也是3x3的卷积和两路1x1的卷积。 每一级都是单尺度的anchor。因为网络头需要在每层金字塔的feature map上的所有位置滑动，就没必要在特定的一级使用多尺度的anchor了。{P2，P3，P4，P5}上的anchor面积分别为{32x32，64x64，128x128，256x256，512x512}。 跟随Faster R-CNN，每一级金字塔使用多个高宽比{1:2，1:1，2:1}。所以金字塔上一共是15种anchor：5种尺度x3种高宽比. 与Faster R-CNN一样，正样本是与ground truth有着最高IoU的，以及与任何ground truth有着高于0.7的IoU的anchor，与所有ground truth的IoU都小于0.3的anchor作为负样本。注意：ground truth是与anchor相关的，因此也就与金字塔的某一级相关了。 RPN头在金字塔的所有层上共享参数，作者做了不共享参数的实验，发现性能相似，说明FPN中金字塔的所有层共享相似的语义级别。 FPN for Fast R-CNNFaster R-CNN另一模块是Fast R-CNN中基于region的检测器，它使用RoI Pooling提取特征，也是在一个单尺度feature map上进行预测。 应用FPN时，使用前面一节描述的RPN生成多个感兴趣区域RoI，根据RoI在原始图像中的尺寸，选择尺度最正确的feature map，去提取这个RoI的feature。 若RoI在原始图像中高宽为w和h，那么对应的特征层为 224是标准的ImageNet预训练尺寸，$k_0$是当RoI的尺寸为224x224时，对应的特征级。使用ResNet为基础网络的Faster R-CNN，使用C4作为后续部分的输入feature，所以$k_0=4$。直觉上，公式表示当RoI的尺度变小时，比如变为112，那么它应该被映射到一个分辨率更精细的层（$k=3$）。 Extensions: Segmentation ProposalsFPN还可以扩展到分割任务中，下面使用FPN生成分割的proposal。特征金字塔的构建方式与应用FPN到目标检测中一样，但是维度设为128，原来是256。以全卷积的形式，使用5x5的滑动窗在特征图上滑动，产生14x14的掩码和目标分数。浅橙色是相应的图像尺寸，深橙色是目标尺寸，可以看出掩码也是在金字塔的不同级独立预测的。 Reference1.Understanding Feature Pyramid Networks for object detection (FPN)]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>instance segmentation</tag>
        <tag>feature pyramid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLOv3:An Incremental Improvement]]></title>
    <url>%2F2018%2F05%2F03%2FYOLOv3%2F</url>
    <content type="text"><![CDATA[这篇文章发表于2018年，比YOLOv2网络更大，但是更精确。在320x320的分辨率下，YOLOv3处理一幅图需要22ms，mAP为28.2，与SSD一样精确但是快3倍。主要的创新点有3个：类别预测上不再使用softmax而是使用独立的logistic回归，能实现多标签预测；类似于FPN，实现多尺度预测，将不同层的特征做了融合；提出更好的基础网络，加入残差块。 The DealBounding Box 预测 跟随YOLO9000，依然使用维度聚类和anchor box。 YOLOv3使用logistic回归为每一个bounding box预测一个目标分数，如果一个先验的box比其他box和ground truth的IoU大，这个分数就为1；如果一个先验box并不是最好的，而是与真实值的IoU超过某些阈值，就忽略掉这个预测，这点跟随Faster R-CNN，本文阈值采用0.5。 与Faster R-CNN不同，YOLOv3为每一个ground truth只匹配一个先验box（其实就是anchor box）。Faster R-CNN中一个ground truth匹配到两种anchor：每一个ground truth匹配到与它IoU最高的anchor，但是同时也把那些与它IoU大于0.7的anchor也当做正的，因此每一个ground truth会有多个与其匹配的anchor。 如果一个先验box没有被匹配到任何ground truth，那么它对于坐标回归或者类别预测没有贡献，只对目标预测（objectness）有用，也就是预测是否为目标时才有贡献。 类别预测使用多标签预测，每个bounding box都预测可能包含的类别。但是并不使用softmax，而是使用独立的logistic回归分类器，训练中使用二分类的交叉熵损失。更加复杂的数据集Open Image Dataset有很多重叠标签，比如女性和人就是包含关系，使用softmax其实强加了一个假设：每一个box恰好有一类，但通常不是这样的。比如一个box里包含了一个“women”，这个box还应该有“person”这个标签，因此多标签方法能够更好的拟合数据。 softmax每一个框只得到一个标签，对应分数最高的那个，不适合做多标签分类 softmax可以被多个独立的logistic分类器替代，且准确率不会下降 多尺度预测123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108layer filters size input output 0 conv 32 3 x 3 / 1 416 x 416 x 3 -&gt; 416 x 416 x 32 0.299 BFL OPs 1 conv 64 3 x 3 / 2 416 x 416 x 32 -&gt; 208 x 208 x 64 1.595 BFL OPs 2 conv 32 1 x 1 / 1 208 x 208 x 64 -&gt; 208 x 208 x 32 0.177 BFL OPs 3 conv 64 3 x 3 / 1 208 x 208 x 32 -&gt; 208 x 208 x 64 1.595 BFL OPs 4 res 1 208 x 208 x 64 -&gt; 208 x 208 x 64 5 conv 128 3 x 3 / 2 208 x 208 x 64 -&gt; 104 x 104 x 128 1.595 BFL OPs 6 conv 64 1 x 1 / 1 104 x 104 x 128 -&gt; 104 x 104 x 64 0.177 BFL OPs 7 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 1.595 BFL OPs 8 res 5 104 x 104 x 128 -&gt; 104 x 104 x 128 9 conv 64 1 x 1 / 1 104 x 104 x 128 -&gt; 104 x 104 x 64 0.177 BFL OPs 10 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 1.595 BFL OPs 11 res 8 104 x 104 x 128 -&gt; 104 x 104 x 128 12 conv 256 3 x 3 / 2 104 x 104 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 13 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 14 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 15 res 12 52 x 52 x 256 -&gt; 52 x 52 x 256 16 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 17 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 18 res 15 52 x 52 x 256 -&gt; 52 x 52 x 256 19 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 20 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 21 res 18 52 x 52 x 256 -&gt; 52 x 52 x 256 22 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 23 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 24 res 21 52 x 52 x 256 -&gt; 52 x 52 x 256 25 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 26 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 27 res 24 52 x 52 x 256 -&gt; 52 x 52 x 256 28 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 29 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 30 res 27 52 x 52 x 256 -&gt; 52 x 52 x 256 31 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 32 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 33 res 30 52 x 52 x 256 -&gt; 52 x 52 x 256 34 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 35 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 36 res 33 52 x 52 x 256 -&gt; 52 x 52 x 256 37 conv 512 3 x 3 / 2 52 x 52 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 38 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 39 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 40 res 37 26 x 26 x 512 -&gt; 26 x 26 x 512 41 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 42 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 43 res 40 26 x 26 x 512 -&gt; 26 x 26 x 512 44 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 45 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 46 res 43 26 x 26 x 512 -&gt; 26 x 26 x 512 47 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 48 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 49 res 46 26 x 26 x 512 -&gt; 26 x 26 x 512 50 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 51 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 52 res 49 26 x 26 x 512 -&gt; 26 x 26 x 512 53 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 54 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 55 res 52 26 x 26 x 512 -&gt; 26 x 26 x 512 56 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 57 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 58 res 55 26 x 26 x 512 -&gt; 26 x 26 x 512 59 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 60 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 61 res 58 26 x 26 x 512 -&gt; 26 x 26 x 512 62 conv 1024 3 x 3 / 2 26 x 26 x 512 -&gt; 13 x 13 x1024 1.595 BFL OPs 63 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFL OPs 64 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFL OPs 65 res 62 13 x 13 x1024 -&gt; 13 x 13 x1024 66 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFL OPs 67 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 68 res 65 13 x 13 x1024 -&gt; 13 x 13 x1024 69 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 70 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 71 res 68 13 x 13 x1024 -&gt; 13 x 13 x1024 72 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 73 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 74 res 71 13 x 13 x1024 -&gt; 13 x 13 x1024 75 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 76 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 77 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 78 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 79 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 80 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 81 conv 255 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 255 0.088 BFLOPs 82 detection # 75~81行是尺度1的预测，对13X13X1024的feature，使用255（3x(4+1+80)）个卷积核对每个位置去做预测 83 route 79 #取79行卷积对应的feature map 13 x 13 x 512 84 conv 256 1 x 1 / 1 13 x 13 x 512 -&gt; 13 x 13 x 256 0.044 BFLOPs 85 upsample 2x 13 x 13 x 256 -&gt; 26 x 26 x 256 86 route 85 61 #取85和61行的feature map做拼接得到下面 26 x 26 x 768的feature map 87 conv 256 1 x 1 / 1 26 x 26 x 768 -&gt; 26 x 26 x 256 0.266 BFLOPs 88 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 89 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs 90 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 91 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs 92 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 93 conv 255 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 255 0.177 BFLOPs 94 detection #尺度2的预测是在尺度1的feature map基础上做的 95 route 91 96 conv 128 1 x 1 / 1 26 x 26 x 256 -&gt; 26 x 26 x 128 0.044 BFLOPs 97 upsample 2x 26 x 26 x 128 -&gt; 52 x 52 x 128 98 route 97 36 99 conv 128 1 x 1 / 1 52 x 52 x 384 -&gt; 52 x 52 x 128 0.266 BFLOPs 100 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 101 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs 102 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 103 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs 104 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 105 conv 255 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 255 0.353 BFLOPs 106 detection #尺度3的预测是在尺度2的feature map基础上做的 YOLOv3检测网络输入为416x416，5倍下采样后，最后一层卷积的输出是13x13。使用了一种与特征金字塔网络（FPN）相似的概念，YOLOv3在3个不同的尺度进行预测： 尺度1：在基础的特征提取网络上添加了几个卷积层，在最后一个卷积层输出的13x13的feature map上预测一个3-d的tensor编码bounding box，目标和类别的预测。实验中，对于COCO数据集，每一个尺度预测3个box，所以tensor的尺寸就是NxNx[3x(4+1+80)]，feature map大小为NxN，每个box有4个坐标，1个目标分数，80个类别分数。 尺度2：提取尺度1的13x13的feature map，上采样2倍；然后在网络的浅层中取一个26x26的feature map，并把它与上采样后的feature map进行拼接（concatenation）。这个方法能够获得上采样的feature map上更加有意义的语义信息和浅层feature map上细粒度（finer-grained）的信息。添加几个卷积层去处理这个组合后的26x26x768的feature map，最终预测一个相似的tensor，但比尺度1大2倍。 尺度3：在尺度2的26x26的feature map基础上，上采样2倍得到52x52的feature map；取浅层的52x52的feature map做拼接，在组合后的52x52的feature map上做预测。 使用K-means聚类，只选择了9个聚类和3个尺度，然后按照聚类中心的大小平均划分3种尺度，最后3个尺度对应于尺度1的预测，以此类推。在COCO数据集上，9个聚类中心为：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。 特征提取设计了一种新的特征提取网络，这个网络是YOLOv2中Darknet-19和残差网络的混合。特征提取网络连续使用3x3和1x1卷积层，但是加入了快捷连接（shortcut connection），所以明显更大一些。因为有53个卷积层，因此称其为DarkNet-53。精确率比ResNet-101更好，与ResNet-152相当，但是更高效。 训练在完整的图像上训练，没有使用难负样本挖掘或者其他操作。使用多尺度训练，数据增强，批量归一化。 How We Do在COCO数据集的mAP度量标准下，YOLOv3与SSD的变体DSSD相当，但仍然落后于RetinaNet，但速度更快。但是在旧的度量标准下，即IoU为0.5时的mAP，YOLOv3与RetinaNet相当，远超DSSD。但是当IoU增加时，YOLOv3的性能明显降低，说明YOLOv3其实是在努力获得与目标对齐的box。 过去，YOLO在尽力解决小目标问题，现在这一情况有了逆转。当使用多标签预测时，可以看到YOLOv3，在小目标上有着相对高的AP值（APs），然而在中等和大目标上性能相对较差。 Things We Tried That Didn’t Work anchor box偏移量预测：降低了模型的稳定性 使用线性激活取代逻辑回归激活函数直接预测x，y偏移量：mAP下降了几个点 Focal loss：降低了2个点的mAP，可能是因为YOLOv3对Focal loss尽力解决的问题已经足够鲁棒。因为YOLOv3有独立的目标分数预测和条件类概率预测。 双IoU阈值和：Faster R-CNN在训练中使用了2个IoU阈值，anchor box与ground truth的IoU超过0.7的也作为正样本，而[0.3,0.7]的被忽略掉。小于0.3的作为负样本。尝试在YOLOv3中这样做，但是结果并不好。 Reference 论文原文 目标检测|YOLOv2原理与实现(附YOLOv3) YOLOv3项目主页]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO9000:Better, Faster, Stronger]]></title>
    <url>%2F2018%2F05%2F02%2FYOLOv2%2F</url>
    <content type="text"><![CDATA[这篇文章发表在2016年，提出了YOLO的第二个版本。YOLOv2在67FPS的检测速度下，可以在VOC2007上达到76.8的mAP。在40FPS的速度下，有78.6的mAP，比使用ResNet的Faster R-CNN和SSD更好。YOLO9000可以检测超过9000种目标。 Introduction当前目标检测数据集比起图像分类数据集而言，目标种类很少。因此本文利用已有的大量图像分类数据并扩展到目标检测系统中。将不同的数据集组合在一起，提出了一种联合训练算法，允许在检测和分类数据上训练目标检测器，利用检测图像学习精确地定位目标，同时利用分类图像扩展模型对多类别的识别能力。使用这种方法训练YOLO9000，可以获得一个可以检测9000种不同类别目标的实时检测器。 首先，在YOLO的基础上改进产生YOLOv2，然后使用组合的数据集和联合训练算法来训练ImageNet图像分类数据和COCO目标检测数据得到YOLO9000。 Better相比于最先进的目标检测系统而言，YOLO有一些缺点：与Fast R-CNN相比，YOLO产生大量的定位误差；与基于region proposal的方法相比，YOLO有较低的召回率。所以，本文的改进也主要是在这两方面。 YOLOv2的目标是保持很快的检测速度，达到更高的准确度。将过去工作中的思想与本文的新概念融合，以提高YOLO的性能。 Batch Normalization：在YOLO的所有卷积层增加BN，mAP提升了2%。使用BN，可以去掉dropout而不过拟合。 高分辨率分类：原始的YOLO训练的分类网络输入是224x224，并且为训练检测任务将分辨率增加到448。这意味着网络必须同时切换到学习目标检测并调整到新的输入分辨率上。 YOLOv2中，首先在ImageNet上，对分类网络在448x448的分辨率下进行10个epoch的微调。相当于给了网络一些时间去调整卷积核，使得在高分辨率输入下工作得更好。 然后在检测数据集上微调网络。高分辨率的分类网络使得mAP有近4%的提升。 使用Anchor Box的卷积：YOLO中使用全连接层直接在顶层的特征图上预测bounding box的坐标。但是Faster R-CNN使用RPN，在特征图的每一个位置预测anchor box的偏移量和置信度，预测偏移量简化了问题，并且使得网络更容易学习。本文移除掉YOLO中的全连接层，使用anchor box去预测bounding box。 首先移除掉一个池化层，提高卷积层的输出分辨率，然后缩小网络将输入尺寸改为416而不是448×448，目的是希望得到的特征图中位置为奇数，这样就只有一个中心单元格。那些大目标倾向于占据图像的中心，所以在中心只有一个位置能很好预测这些目标。YOLO的卷积层下采样32倍，416的输入图像最终得到13x13的feature map。 因为YOLO是由每个cell来负责预测类别，每个cell对应的2个bounding box 负责预测坐标 。YOLOv2中，不再让类别的预测与每个cell（空间位置）绑定一起，而是让全部放到anchor box中。 跟随YOLO，在目标预测上，仍然预测ground truth和候选框的IoU，表示是否存在目标；类别预测上，预测类别概率，即存在目标的前提下目标的类别。 使用anchor box准确度上有一点降低。YOLO对每幅图只预测98个box，但是使用anchor box后，每幅图预测超过1000个box。 维度聚类：使用anchor box时遇到两个问题，一是box的尺寸是手工选取的，网络可以学习调整box，但假如选择更好的先验box，那么会使网络更容易预测出好的结果。 取代手工选取anchor box尺寸，使用k-means在训练集的bounding box上进行聚类，自动地找到好的先验box。这里并不使用欧氏距离，因为这会导致较大的box比较小的box产生更大的误差。希望得到的先验box能提高IoU分数。因此定义距离度量为：$d(box,centroid) = 1 - IoU(box,centroid)$。 直接位置预测：使用anchor box时遇到的第二个问题就是模型不稳定，特别是在早期的迭代时。不稳定性主要来源于box的(x,y)坐标。在RPN中，网络预测值$t_x$和$t_y$与中心坐标计算为$x=t_xw_a-x_a$，$y=t_yh_a-y_a$。$t_x= 1$的预测将使框向右移动anchor box的宽度， $t_x=-1$的预测将使其向左移动相同的量。 这种公式是不受约束的，因此不管预测box的位置，任何anchor box可以在图像中的任何点结束。使用随机初始化模型需要很长时间才能稳定到预测出可感知的偏移。 本文遵循YOLO的方法并预测相对于网格单元位置的bounding box坐标。这将ground truth限制在0和1之间。我们使用逻辑激活函数来约束网络的预测落在该范围内。 网络在feature map的每个网格单元上预测5个bounding box，每个box包含5个坐标：$t_x,t_y,t_w,t_h,t_o$。假如网格相对图像左上角偏移$(c_x,c_y)$，先验的bounding box宽高为$p_w,p_h$，则预测为： $b_x = \sigma(t_x)+c_x$ $b_y = \sigma(t_y)+c_y$ $b_w = p_we^{t_w}$ $b_h = p_he^{t_h}$ $P_r(object)*IoU(b,object) = \sigma(t_o)$使用维度聚类和直接预测bounding box中心位置，比Faster R-CNN中的anchor box方法提升了近5%的mAP。 细粒度特征（Fine-Grained Features）：改进的YOLO在13x13的feature map上预测，尽管这对大目标是足够的，但是对于定位更小的目标需要细粒度的特征。本文添加一个传递层（passthrough layer），与ResNet的恒等映射相似，传递层通过将相邻的特征图堆叠到不同的通道，而不是空间位置，将较高分辨率和较低分辨率的feature map相连。将26×26×512特征映射转换为13×13×2048特征映射，这样可以直接与原始的特征拼接。这一点改进获得了1%的性能提升。 多尺度训练：由于使用anchor box，因此改变输入的分辨率为416。本文并不固定输入图像的尺寸，而是在每几次迭代中改变网络的输入图像尺寸。每10个batch，网络随机选择一个新的输入图像尺寸，由于下采样的因子是32，所以输入尺寸都是32的倍数：{320，352，…，608}。这个策略迫使网络学习在不同的输入分辨率下预测好的结果，意味着相同的网络可以实现不同分辨率的检测。在288的分辨率下，YOLOv2的速度是90FPS，mAP几乎和Faster R-CNN一样好，高分辨率下，在VOC2007数据集上YOLOv2达到78.6的mAP，仍然保证实时。 Faster由于VGG16还是过于复杂，YOLO框架中采用基于GoogLeNet的自定义网络，准确率略低于使用VGG16，但计算量更少。 本文设计了一个新的分类模型Darknet-19，共19个卷积层和5个最大池化层： 与VGG模型相似，使用3x3的卷积核，并且在每个池化步骤后将通道数量加倍。 跟随Network in Network(NIN)，使用全局平均池化做预测，用1x1的卷积在3x3的卷积之间压缩特征。 使用批量归一化来稳定训练，加速收敛，正则化模型。 训练分类任务：在ImageNet的1000类数据集上训练160个epoch，输入的分辨率为224。然后在更大的尺寸448上，微调网络，训练10个epoch。 训练检测任务：修改分类网络，将最后的卷积层移除掉，添加3个3x3的卷积层，每一层有1024个卷积核，跟随1个1x1的卷积层，卷积核数量与检测任务需要的输出相对应。对于VOC数据集来说，预测包含5个bounding box，每一个包含5个坐标和20个类别，所以一共有125个卷积核(5x(20+5))。除此之外，还添加了一个传递层，从最后的3x3x512的层连接到倒数第二层，以便于模型可以使用细粒度特征。 Stronger训练期间，混合了检测和分类数据集。当网络看到标记为检测的图像时，可以反向传播整个YOLOv2的损失，当看到一个分类图像时，只传播网络结构中特定的分类部分损失。 这种方法带来了一些挑战，检测数据集只有常见目标和标签，而分类数据集有着更广泛的标签。ImageNet有着100多种狗，如果想要在两种任务的数据集上联合训练，需要一种连贯的方式去合并标签。 分类任务中一般使用softmax层，计算所有可能类别的概率，这假设了类别之间是互斥的。因此，需要一个多标签的模型来综合数据集，使类别之间不相互包含。 层次分类：ImageNet的标签是从WordNet中提取的，WordNet是一个语言数据库，用于构建概念及其关系。WordNet被构造为有向图，但是本文并不使用完整的图结构，而是根据ImageNet中的概念构建层次树来简化问题，最终的结果是WordTree。使用WordTree实现分类，预测在每个节点的条件概率，也就是预测给定同义词集合的每个下位词的概率。例如，在“terrier”节点，预测：$P_r(Norfolk~terrier|terrier)$，$P_r(Yorkshire~terrier|terrier)$… 如果想要计算一个特定节点的绝对概率，只需遵循通过树到达根节点的路径，并乘以条件概率。因此，如果我们想知道图片是否是诺福克梗犬，只需要计算：$$\begin{aligned}P_r(Norfolk~terrier)&amp;= P_r(Norfolk~terrier|terrier) \times P_r(terrier|hunting~dog) \&amp;… \times P_r(mammal|animal) \times P_r(animal|physical~object)\end{aligned}$$ 对于分类任务，假设图像中包含目标，因此$P_r(physical~object) = 1$。 Darknet-19模型的训练使用1000类ImageNet数据集构建的WordTree。为了构建WordTree1k，添加中间节点将标签空间从1000扩展到1369。在训练期间，沿着树传播ground truth标签，以便如果图像被标记为“诺福克梗犬”，它也被标记为“狗”和“哺乳动物”等。为了计算条件概率，模型预测了1369个值的向量，并且计算所有同义词集合的softmax，如下图： 以这种方式实现分类具有一些益处。在新的或未知的目标类别上性能降低微小。例如，如果网络看到一只狗的图片，但不确定它是什么类型的狗，它仍然会预测具有高置信度的“狗”，但在下义词（更为具体的内容，比如某种类型的狗）上具有较低的置信度。 这个公式也用于检测。不是假设每个图像都有一个目标，而是使用YOLOv2的目标预测器给出$P_r(physical~object)$的值。检测器预测bounding box和概率数。遍历树，在每个分裂中采用最高置信度路径，直到达到某个阈值，预测出目标类别。 联合分类和检测要训练一个极大尺度的检测器，因此使用COCO数据集和来自完整ImageNet的前9000类创建组合数据集。还需要评估本文的方法，所以添加ImageNet还没有包括的类别。WordTree数据集相应的具有9418个类。ImageNet是一个更大的数据集，因此对COCO进行过采样来平衡数据集，使ImageNet与COCO数量只有4：1的倍数。使用这个数据集训练YOLO9000，使用基本的YOLOv2架构，但只有3个先验box，而不是5，以限制输出大小。 当网络看到检测图像时，反向传播正常的损失。对于分类损失，只在标签的相应层级或高于标签的相应层级反向传播误差。例如，如果标签是“狗”，会在树中“德国牧羊犬”和“金毛猎犬”的预测中分配误差，因为没有这些信息。 看到分类图像时，只反向传播分类损失，为了实现这一点，只找到对这个类别预测的概率最高的bounding box，并且只计算它的预测树上的损失。这里假设预测框与ground truth的IoU至少为0.3，并且基于该假设反向传播目标类别损失。 Reference 论文原文 YOLOv2]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks]]></title>
    <url>%2F2018%2F04%2F25%2Fmtcnn%2F</url>
    <content type="text"><![CDATA[提出一种深度级联的多任务框架，利用检测和对齐的固有相关性去增强它们的性能。实际中，利用有三阶段精细设计的深度卷机网络的级联结构，由粗到精地检测和对齐人脸。 Introduction人脸识别中视觉的变化，比如遮挡，姿态变化和极端的光照条件，会给人脸检测和对齐带来巨大挑战。AdaBoost和Haar-Like特征训练的级联分类器虽然可以达到比较高的效率，但是大量研究表明这类检测器在人脸有着较大的视觉变化时，检测精度会大大降低。DPM（deformable part models）用于人脸检测也可以达到非常好的性能，然而计算代价太大，并且在训练时可能要求大量的标注。 人脸对齐领域的方法可以大致划分为两类：基于回归的方法和模板匹配方法。过去大部分的人脸检测和对齐方法都忽视了这两种任务之间的固有联系。 另一方面，挖掘难样本对于增强检测器的性能是至关重要的。传统的方法都是离线模式去挖掘，对于人脸检测任务来说，需要一种在线的难阳本挖掘方法，这样可以自动适应当前的训练状态。 本文中，通过多任务学习使用统一的级联CNNs集成这两种任务。提出的CNNs包含三个阶段：第一阶段，使用浅层的CNN(fast Proposal Network (P-Net))生成候选窗口；第二阶段，通过更加复杂的CNN(Refinement Network (R-Net))去精炼窗口，拒绝掉大量的非人脸的窗口；第三阶段，使用更加强大的CNN(Output Network (O-Net))去再次精修结果并输出5个landmark位置。 贡献： 提出一种级联的CNNs框架做人脸检测和对齐，设计了一种轻量的CNNs结构用于实时性能。 提出一种在线难样本挖掘（online hard sample mining）方法去提高性能。 ApproachOverall Framework首先将给定图像缩放到不同的尺度建立图像金字塔，这将是后面三阶段级联框架的输入。 第一阶段：采用全卷积神经网络，即P-Net，去获得候选窗体和bounding box回归向量。同时，候选窗体根据估计的bounding box向量进行校准。然后，利用NMS方法合并高度重叠的候选框。 第二阶段：所有的候选框被输入R-Net，进一步拒绝掉大量的错误候选框，同样使用bounding box回归校正候选框，并实施NMS。 第三阶段：和第二阶段相似，但是目的是利用更多的监督去判断人脸区域，并输出5个landmark位置。 CNN Architectures多个CNN被用于人脸检测，但其性能可能受到以下情况的限制： 卷积层中的卷积核缺乏多样性，限制他们的识别能力； 对比多类识别检测和分类任务，人脸检测是一个二分类问题，因此每一层需要的卷积核较少。所以本文减少卷积核数量，并将5*5的卷积核大小改为3*3的，以此在增加深度来提高性能的同时减少计算。 Training 本算法从三个方面对CNN检测器进行训练：人脸分类、bounding box回归、landmark定位（关键点定位）。 人脸分类： 二分类问题，使用交叉熵损失函数： bounding box 回归： 回归问题，使用欧氏距离计算的损失函数： landmark 定位： 回归问题，使用欧氏距离计算损失函数： 在每个CNN中实现的是不同的任务，所以在学习过程中有几种类型的训练图像：人脸，非人脸，部分对齐的人脸。在这种场合下，上面的式子不能使用，比如，对于背景区域的样本，只需要计算检测损失，其他两种损失设置为0，所以使用一些系数，总体的学习目标表示为： Online Hard sample mining： 每一个mini-batch中，对从所有的样本前向运算得到的损失排序，选择前70%作为难样本。在反向传播中，只计算来自于这些难样本的梯度。这意味着在训练中忽视掉那些对增强检测器性能帮助甚小的简单样本。 Experiments在训练中有四种数据： 负样本：与任何ground truth faces的IoU低于0.3的。 正样本：与一个ground truth face的IoU高于0.65的。 Part faces：与一个ground truth face的IoU在0.4~0.65之间的。 Landmark faces：标定了5个landmark的。负样本和正样本用于人脸分类，正样本和part faces用于bounding box回归，landmark faces用于landmark定位。上面的样本比例：3：1：1：2。 数据的收集方法如下： P-Net：随机地从WIDER FACE数据集中裁切一些图像块，收集正样本，负样本，part人脸。从CelebA数据库裁切人脸作为landmark人脸。 R-Net：使用框架的第一阶段在WIDER FACE中检测人脸，收集正样本，负样本和part人脸，同时从CelebA中检测landmark人脸。 O-Net：与R-Net相似的方法收集数据。但是是使用前两个阶段去检测人脸和收集数据。]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>face detection</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FaceNet:A Unified Embedding for Face Recognition and Clustering]]></title>
    <url>%2F2018%2F04%2F10%2FFaceNet%2F</url>
    <content type="text"><![CDATA[这篇文章是Google发表在CVPR2015上的文章，提出了FaceNet直接学习人脸图像到一个紧凑的欧氏空间的映射，空间距离直接对应于面部相似度的测量。主要的创新点就是提出使用triplet loss，并且得到的是128维的特征。在LFW数据集上达到了99.63%的准确率，在YouTube Faces DB达到了95.12%的准确率。 Introduction这篇文章设计了一个统一的系统，用来做人脸验证和人脸识别，以及聚类。 人脸验证（face verification）：是不是同一个人？一对一 人脸识别（face recognition）：是哪一个人？一对多 人脸聚类（face clustering）：在这些人脸中寻找一类人，比如血亲，双胞胎等 提出使用深度卷积神经网络学习人脸到欧式空间的映射，以使欧式空间中的L2平方距离可以直接对应于人脸相似度：在欧氏空间中同一个人的面部特征有着较小的距离，而不同的人的面部特征有着较大的距离。 上图展示了光照和角度变换下，图像对之间的距离。距离为0的话意味着人脸是相同的，而距离为4.0意味着是两个不同的身份。纵向是不同人的图像对，横向是同一个人的图像对，可以看到，取1.1的阈值，可以正确地分类是否为同一个人。 这样解决前面的三种任务就很直接了：人脸验证就只涉及两个图片之间距离的阈值；人脸识别变成了一个K-NN问题；人脸聚类可以通过现有的（off-the-shelf）技术，比如k-means或者凝聚聚类（agglomerative clustering）。 先前的基于深度网络的人脸识别使用一个分类层在已知人脸身份的训练集上进行学习，然后使用中间的瓶颈层（bottleneck layer）作为特征表达去泛化识别性能。这些方法不够直接也不够高效，因为要寄希望于bottleneck layer，让特征表达对新的人脸有很好的泛化性能，并且使用bottleneck layer，特征表达的尺寸也有1000s的维度。 FaceNet使用基于LMNN的三元组损失（triplet-based loss）直接训练出紧凑的128-D的特征。三元组包括：两个匹配的人脸缩略图、一个不匹配的人脸缩略图。loss的目标就是通过距离间隔（margin）将正样本对和负样本分离开。 Method对比两种核心的网络结构：一个是Visualizing and UnderstandingConvolutional Networks这篇文章中的网络，另一个是Inception。FaceNet的结构如图。包含一个batch input layer和一个深度CNN，而后是L2标准化（L2 Normalization），产生所谓的Face Embedding，这个奇怪的词汇face Embedding就是人脸图像到欧氏空间的映射，前面做的就是特征提取。训练过程中最后还有一个triplet loss。 Triplet Loss这种embedding使用$f(x) \in R^d$表达，将输入图像$x$嵌入到$d$维的欧式空间中。此外，这里限制它在$d$维超球面，即$||f(x)||_2 = 1$。 在该空间内，要确保对于某个特定的人，他的anchor图像$x_i^a$与所有其他的正样本$x_i^p$距离近，与任何负样本$x_i^n$距离远。即类内距离加上间隔小于类间距离。表示为：$$||f(x_i^a) - f(x_i^p)||^2_2 + \alpha &lt; ||f(x_i^a) - f(x_i^n)||^2_2~~~~ \forall(f(x_i^a), f(x_i^p), f(x_i^n)) \in \tau$$$\alpha$是用在正负样本对之间的间隔margin。不仅要使anchor与正样本之间的距离小于它和负样本之间的距离，而且要小到某种程度。（SVM中margin不也是这个作用？）。$\tau$是训练集中所有可能的三元组（triplet），共有$N$组。 损失定义为：$$L = \sum_i^N \left[ ||f(x_i^a) - f(x_i^p)||^2_2 - ||f(x_i^a) - f(x_i^n)||-2^2 + \alpha \right]_+$$这个式子右下角有个$+$号，表示：对于每一组三元组，如果不满足目标不等式时会产生一个为正数的差，损失值就是这个差值，而如果满足目标不等式，损失值会小于0，就取损失值为0。就是对于不满足条件的三元组，进行优化，满足的就不管了。所以目标就是最小化这个损失，让anchor靠近postive而远离negtative，如下图： Triplet Selection如何选择三元组是训练的关键。太简单的三元组对训练网络没有帮助，只有那些比较难的三元组才会改善网络性能，加速收敛。所以关键的是选择出那些违背上面不等式的三元组。 给定$x_i^a$，要选择的难的正样本需要使anchor和正样本之间的距离大$$argmax_{x_i^p}||f(x_i^a) - f(x_i^p)||_2^2 $$ 相似地，选择难的负样本要使anchor和负样本之间的距离小$$argmin_{x_i^n}||f(x_i^a) - f(x_i^n)||_2^2$$ 文章中说选择最难的负样本会导致在训练早期得到局部最优解，因此不选择最难的负样本，而是选择较难的（semi-hard），也就是说负样本比正样本远离anchor，同时anchor-negtative的距离接近anchor-postive的距离。 生成三元组时，每个mini-batch中对每个身份选择40个人脸，并且随机采样一些负样本人脸。文中mini-batch是1800个样本。 训练时使用SGD和AdaGrad优化。学习率开始设为0.05，$\alpha$设为0.02。好吧，模型随机初始化，以超强的自信心在CPU集群上丧心病狂地训练了1000~2000小时，也就是41~83天。训练了500个小时后，loss和准确率的变化才减慢。]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepID:Deep Learning Face Representation from Predicting 10,000 Classes]]></title>
    <url>%2F2018%2F04%2F10%2FDeepID%2F</url>
    <content type="text"><![CDATA[这篇文章提出使用深度学习去学习到一个高级的特征表达集合DeepID用于人脸验证。DeepID特征是从深度卷积神经网络的最后一个隐含层神经元激励提取到的。并且这些特征是从人脸的不同区域中提取的，用来形成一个互补的过完备的人脸特征表达。 Introduction当前有着最优表现性能的人脸验证算法采用的是过完备的低级别特征，并且使用的是浅层模型。本文提出使用深度模型来学习高级的人脸特征集，也就是，把一个训练样本分入10000个身份中的一个。高维空间的操作虽然更有难度，但学习到的特征表达有更好的泛化性能。尽管是通过识别任务学习的，但是这些特征也可用于人脸验证或者数据集中没有出现过的人脸。 特征提取过程如下：卷积神经网络通过学习，将训练集中所有人脸根据他们的身份进行分类。使用多个ConvNet，每一个提取最后一个隐含层神经元的激励作为特征（Deep hidden IDentity features, DeepID）。每一个ConvNet取一个人脸patch作为输入并且提取底层（bottom layers）的局部低级特征（low-level），随着更多全局的高级特征逐渐在顶层（top layer）形成，特征的数量沿着特征提取级联（feature extraction cascade）持续减少。在级联的最后形成了一个高度紧凑的160-d DeepID特征，它包含了丰富的身份信息，用来预测一个数量庞大的身份类别。 分类所有的身份而不是训练二分类基于两点考虑 分类训练样本到多类别中的一个比起实现二分类更加困难。这样可以充分利用神经网络强大的学习能力提取有效的人脸特征用于识别。 多分类给卷积神经网络潜在地增加了强正则化，这有助于形成共享的隐含的特征表达，更好地分类所有的身份。 限制DeepID的维度明显小于要预测的类别数，是学习高度紧凑的具有辨识力特征的关键。进而拼接从不同的人脸区域提取到的DeepID形成互补的过完备特征表达。 在仅使用弱对齐的人脸的情况下，在LFW数据集上达到了97.45%的人脸验证准确率。同时也观察到，随着训练身份数量的增加，验证的性能也在稳步提升。 Deep ConvNets这里的卷积神经网络共有4个卷积层，前3个每个都跟随着最大池化。卷积层之后是全连接的DeepID layer和softmax layer。DeepID layer是固定的160维，输出层softmax的维度随着预测类别数目而变化。 最后一个隐含层通过全连接与第三个和第四个卷积层相连，目的是可以得到多尺度的特征，因为第四个卷积层的特征比第三个更加的全局。这对于特征学习很关键，随着级联中连续的下采样，第四个卷积层包含了太少的神经元，变成了信息传播的瓶颈（bottleneck），所以在第三个和最后一个隐含层之间加入这种旁路连接（bypassing connection），即跳过某些层，最后一个隐含层减少了第四个卷积层可能的信息损失。 Feature extraction人脸对齐部分只用了5个点，两个眼睛的中心，鼻尖，两个嘴角。从人脸图像中选取10个区域，3种尺度，RGB和灰度图像一共60（10x3x2）个人脸patch来做特征提取。下图展示了10个人脸区域和具有3种尺度的两个特定人脸区域。 训练60个ConvNet，每一个ConvNet都从特定的patch和它的水平翻转中提取160维的DeepID向量。除了两眼中心和两个嘴角附近的patch不用自身翻转，而是使用对称的patch，也就是说左眼中心的patch的翻转是通过翻转右眼中心的patch得到的。这样每个ConvNet得到的整个DeepID的长度是$160\times2\times60$。 Face verification使用联合贝叶斯（Joint Bayesian）方法进行人脸验证，同时训练了一个神经网络做对比试验。输入层接受DeepID特征，输入的特征被分为60组，每一组包含了使用特定的ConvNet和patch对的640个特征（一个patch是320维的，验证需要两个人脸图像，因此就是一对patch）。同一组的特征是高度相关的。 局部连接层（locally-connected layer）的神经元只和一组特征连接，学习它们的局部相关性，同时降维。第二个隐含层式全连接层，学习全局的联系。输出只有1个神经元，以全连接的方式和前一层相连。隐含层用的激活函数是ReLU，输出层时sigmoid。 由于对输入的神经元不能使用dropout，因为输入的是高度紧凑的特征，并且是分布式表达，必须共同使用从而表达身份。但是高维的特征如果不用dropout会容易出现梯度弥散的问题。所以先训练如下图所示的60个子网络，每一个都以单组特征作为输入。然后使用子网络第一层的权重初始化原始网络中对应的部分，再微调原始网络第二和第三层。 Experiments数据集：在CelebFaces数据集（87628幅图像，每个人平均16张）上训练模型，在LFW上进行测试。训练时，80%的数据即4349张用来学习DeepID，使用剩下的学习人脸验证模型。 人脸验证：在学习联合贝叶斯模型之前，使用PCA降维将特征维度降为150维。验证的性能在很大的维度范围内都可以保持稳定。 Multiscale ConvNets如前面所示，将第三个卷积层最大池化后的部分直接连接到最后一个隐含层，通过去掉这个连接和带上这个连接进行比较，发现准确率从95.35%提升到了96.05。 Learning effective features指数级增加身份类别，通过top-1误差率观察分类能力，根据测试集的确认精确度观察学习到的隐藏层特征的性能。发现同时对大量的身份进行分类对学习到具有判别性和紧致的隐藏特征是关键。 Over-complete representation为了评估少个patch的组合对性能的贡献大。分别选择了1,5,15,30,60个patch组合的特征训练人脸验证模型。结果表明：提取更多的特征，性能更好。 Reference 论文原文]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rapid Object Detection using a Boosted Cascade of Simple Features]]></title>
    <url>%2F2018%2F03%2F07%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[这篇文章是人脸检测的经典，提出一种基于机器学习的视觉目标检测方法，主要有三点贡献：第一，引入“积分图”概念，可以被检测器用来快速计算特征。第二，学习算法基于AdaBoost，可以从很大的集合中筛选出少量的关键视觉特征并形成更加高效的分类器。第三，以“级联”形式逐渐合并复杂分类器的方法，该方法使得图像的背景区域被很快丢弃，从而将更多的计算放在可能是目标的区域上。 Introduction本文建立了一个正面的人脸检测系统，常规700 MHz英特尔奔腾III，人脸检测速度达到了每秒15帧。其他的人脸检测系统提高帧率是利用视频序列中的图像差异，或者彩色图像中像素的颜色，本文的系统仅仅利用灰度图像信息就实现了高帧率。 三点贡献： 提出积分图，快速地计算特征。在一幅图像中，每个像素使用很少的一些操作，就可以计算得到积分图。任何一个Haar-like特征可以在任何尺度或位置上被计算出来，且是在固定时间内。 提出了一种通过AdaBoost选择少量重要特征的方法。在任何的图像子窗口中，Haar-like特征数量是非常大的，通常远大于像素数量。为了获得快速的分类，学习算法必须排除掉获取的特征中的大部分，关注少量关键特征。通过对AdaBoost程序简单修改：约束弱学习器，使得返回的弱分类器只依赖于一个简单特征。因此，boosting过程的每一个阶段，选择一个新的弱分类器，这些阶段可以被视为特征选择过程。 提出了一种以级联的方式逐渐合并更加复杂的分类器的方法，通过关注图像中那些更有希望的区域，这大大地提高了检测速度。 这些没有被最初的分类器排除的子窗口，由接下来的一系列分类器处理，每个分类器都比其前一个更复杂。如果一个子窗口被任何分类器拒绝了，则它就不再被进一步处理。 Features使用三种特征： 双矩形特征：其值定义为两个矩形区域里像素和的差。这两个区域有着相同的尺寸和形状，并且水平或垂直连接。如图A和B。 三矩形特征：其值定义为两个外矩形像素和减去中间矩形像素和。如图C。 四矩形特征：其值定义为对角线上的矩形对的差。如图D。 Integral Image积分图：某个位置上的左边和上边的像素点的和。位置(x,y)上的积分图像包含点(x,y)上边和左边的像素和。如下式，ii(i,y)是(x,y)位置的积分图，i(x,y)是原始图像的像素值。 使用下面的两个式子迭代，其中s是累计行和，s(x,-1)=0，ii(-1,y)=0： 使用积分图像可以把任意一个矩形用四个数组引用计算，例如下图中的D，位置1上的积分图的值是矩形A的像素和，位置2上积分图的值是A+B，位置3则是A+C，位置4是A+B+C+D，矩形D的和可以计算为：4+1-(2+3)： 由上图也可以看出，两个矩形像素和之间的差可以通过8个数组引用来计算。因为双矩形特征涉及到两个相邻矩形的和，所以仅用6个数组引用就可以计算出结果。同理三矩形特征用8个，四矩形特征用9个。 Learning Classification Functions本文中，AdaBoost的一个变体被用于选择一个小集合的特征并且训练分类器。原始的AdaBoost学习算法被用于加强简单（弱）分类器的性能。每一个图像子窗口相关的特征超过180000，远超过像素的数量。而这些特征中只有一小部分可以被组合形成一个有效的分类器。所以主要的挑战是找到这小部分的特征。 弱学习器用来选择能将正负样本最好的分离的单个特征。对于每一个特征，弱学习器确定最优的阈值分类函数，以使被误分类的样本数量最少。弱分类器hj(x)包括：特征fj，阈值θj，和一个正负校验pj，表示不等号的方向，x是24×24的图像子窗口。 学习算法如下： Learning Result 对于人脸检测的任务，由AdaBoost选择的最初的矩形特征是有意义的且容易理解。选定的第一个特征的重点是眼睛区域往往比鼻子和脸颊区域更黑。此特征相对于检测子窗口较大，并且某种程度上不受面部大小和位置的影响。第二个特征选择依赖于眼睛的所在位置比鼻梁更暗 The Attentional Cascade本章提出的构建级联分类器的算法，它能增加检测性能从而从根本上减少计算时间。主要观点是构建一种优化分类器，其规模越小就越高效。这种分类器在检测几乎所有的正样本时剔除许多负子窗口（即，优化分类器阈值可以调整使得false negative率接近零）。在调用较复杂的分类器之前，我们使用相对简单的分类器来剔除大多数子窗口，以实现低false negative率。 在检测过程中，整体形式是一个退化决策树，称之为“级联”(cascade)。从第一个分类得到的有效结果能触发第二个分类器，它已经调整达到非常高的检测率。再得到一个有效结果使得第二个分类器触发第三个分类器，以此类推。在任何一个点的错误结果都导致子窗口立刻被剔除。 级联阶段的构成首先是利用AdaBoost训练分类器，然后调整阈值使得false negative最大限度地减少。注意，默认AdaBoost的阈值旨在训练数据中产生低错误率。一般而言，一个较低的阈值会产生更高的检测速率和更高的false positive率。 一个双特征强分类器通过降低阈值，达到最小的false negatives后，可以构成一个优秀的第一阶段分类器。测量一个训练集时，阈值可以进行调整，最后达到100%的人脸检测率和40%的正误视率。 Training a Cascade of Classifiers在实践中用一个非常简单的框架产生一个有效的高效分类器。级联中的每个阶段降低了false negatives并且减小了检测率。现在的目标旨在最小化false negatives和最大化检测率。调试每个阶段，不断增加特征，直到检测率和false negatives的目标实现（这些比率是通过将探测器在验证集上测试而得的）。同时添加阶段，直到总体目标的false negatives和检测率得到满足为止。 ResultScanning the Detector最终的检测器在多个尺度和位置上扫描图像。尺度缩放是缩放检测器自身而不是缩放图像。这个过程有效是因为特征可以在任意尺度下被评估。使用1.25的间隔可以得到良好结果。 检测器也在位置上扫描。后续位置的获得是通过将窗口平移⊿个像素获得的。这个平移过程受检测器的尺度影响：若当前尺度是s，窗口将移动[s⊿]，这里[]是指取整操作。⊿的选择不仅影响到检测器的速度还影响到检测精度。我们展示的结果是取了⊿=1.0。通过设定⊿=1.5，我们实现一个有意义的加速，而精度只有微弱降低。 Integration of Multiple Detections因为最终检测器对于平移和尺度的微小改变是不敏感的，在扫描一幅图像时每个人脸通常会得到多检测结果，一些类型的false positives也是如此。在实际应用中每个人脸返回一个最终检测结果才显得比较有意义。 在这些试验中，用非常简便的模式合并检测结果。首先把一系列检测分割成许多不相交的子集。若两个检测结果的边界区重叠了，那么它们就是相同子集的。每个不相交的集合产生单个最终检测结果。最后的边界区的角落定义为一个集合中所有检测结果的角落平均值 A simple voting scheme to further improve results运行三个检测器的结果（一个本文描述的38层检测器加上两个类似的检测器），输出投票得票数高的结果。在提高检测率的同时也消除很多false positives率，且随检测器独立性增强而提高。由于它们的误差之间存在关联，所以对于最佳的单一检测器，检测率是有一个适度提高。 Reference 论文原文 AdaBoost中利用Haar特征进行人脸识别算法分析与总结]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>face detection</tag>
      </tags>
  </entry>
</search>
