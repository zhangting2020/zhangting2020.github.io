<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mask-R-CNN]]></title>
    <url>%2F2018%2F05%2F06%2FMask-R-CNN%2F</url>
    <content type="text"><![CDATA[这篇论文发表在ICCV2017上，提出了一种概念简单，灵活且通用的目标实例分割框架，在检测出图像中目标的同时，生成每一个实例的掩码（mask）。对Faster R-CNN进行扩展，通过添加与已存在的bounding box回归平行的一个分支，预测目标掩码，因而称为Mask R-CNN。这种框架训练简单，容易应用到其他任务，比如目标检测，人体关键点检测。 Introduction实例分割的挑战性在于要求正确地检测出图像中的所有目标，同时精确地分割每一个实例。这其中包含两点内容： 目标检测：检测出目标的bounding box，并且给出所属类别； 语义分割（semantic segmentation）：分类每一个像素到一个固定集合，不用区分实例。 Mask R-CNN对Faster R-CNN进行了扩展，在Faster R-CNN分类和回归分支的基础上，添加了一个分支网络去预测每一个RoI的分割掩码，把这个分支称为掩码分支。掩码分支是应用在每一个RoI上的一个小的FCN，以像素到像素的方式（pixel-to-pixel）预测分割掩码。 Faster R-CNN在网络的输入和输出之间没有设计像素到像素的对齐。在how RoIPool文中提到：实际上，应用到目标上的核心操作执行的是粗略的空间量化特征提取。为了修正错位，本文提出了RoIAlign，可以保留准确的空间位置，这个改变使得掩码的准确率相对提高了10%到50%。解耦掩码和分类也至关重要，本文对每个类别独立地预测二值掩码，这样不会跨类别竞争，同时依赖于网络的RoI分类分支去预测类别。 模型在GPU上运行每帧200ms，在8 GPU的机器上训练COCO数据集花费了一到两天。最后，通过COCO关键点数据集上的人体姿态估计任务来展示框架的通用性。通过将每个关键点视为一位有效编码（one-hot），即所有关键点编码成一个序列，但只有一个是1，其余都是0。只需要很少的修改，Mask R-CNN可以应用于人体关键点检测。不需要额外的技巧，Mask R-CNN超过了COCO 2016人体关键点检测比赛的冠军，同时运行速度可达5FPS。 Related Work早前的实例分割方法受R-CNN有效性的推动，基于分割proposal，也就是先提取分割候选区，然后进行分类，分割先于分类的执行。本文的方法是同时预测掩码和类别，更加简单和灵活。 FCIS（fully convolutional instance segmentation）用全卷积预测一系列位置敏感的输出通道，这些通道同时处理目标分类，目标检测和掩码，这使系统速度变得更快。但FCIS在重叠实例上出现系统错误，并产生虚假边缘。 另一类方法受语义分割的推动，将同类别的像素划分到不同实例中，这是一种分割先行的策略。Mask R-CNN与其相反，基于实例先行的策略（segmentation-first strategy）。 Mask R-CNNMask R-CNN在Faster R-CNN上加了一个分支，因此有三个输出：目标类别、bounding box、目标掩码。但是掩码输出与其他输出不同，需要提取目标更精细的空间布局。Mask R-CNN中关键的部分是像素到像素的对齐，这在Fast/Faster R-CNN里是缺失的。 首先回归一下Faster R-CNN：它包含两个阶段，第一阶段使用RPN提取候选的目标bounding box，第二阶段本质上是Fast R-CNN，使用RoI pooling从候选区域中提取特征，实现分类并得到最终的bounding box。 Mask R-CNN也是两个阶段：第一阶段与Faster R-CNN相同，RPN提取候选目标bounding box；第二阶段，除了并行地预测类别和候选框偏移，还输出每一个RoI的二值掩码（binary mask）。 损失函数 多任务损失：$$L=L_{cls}+L_{box}+L_{mask}$$ 掩码分支对每一个感兴趣区域产生$Km^2$维的输出，K是类别数目，K个分辨率为m×m的二值掩码也就是针对每一个类别产生了一个掩码。 对每一个像素应用sigmoid，所以掩码损失就是平均二分类交叉熵损失。如果一个RoI对应的ground truth是第k类，那么计算掩码损失时，只考虑第k个掩码，其他类的掩码对损失没有贡献。 掩码损失的定义允许网络为每个类别独立预测二值掩码。使用专门的分类分支去预测类别标签，类别标签用来选择输出掩码。 掩码表达 掩码编码了输入目标的空间布局。掩码的空间结构，可以通过卷积产生的那种像素到像素的对应关系来提取。 使用FCN为每个RoI预测一个m×m的掩码。这允许掩码分支中的每个层显式的保持m×m的目标空间布局，而不会将其缩成缺少空间维度的向量表示。 像素到像素的对应需要RoI特征（它们本身就是小特征图）被很好地对齐，以准确地保留显式的像素空间对应关系。 RoI Align首先说明为什么需要对齐，下图中左边是ground truth，右边是对左边的完全模仿，需要保持位置和尺度都一致。平移同变性（translation equivariance）就是输入的改变要使输出也响应这种变化。 分类要求平移不变的表达，无论目标位置在图中如何改变，输出都是那个标签 实例分割要求同变性：具体的来说，就是平移了目标，就要平移掩码；缩放了目标就要缩放掩码 全卷积网络FCN具有平移同变性，而卷积神经网络中由于全连接层或者全局池化层，会导致平移不变。 在Faster R-CNN中，提取一张完整图像的feature map，输入RPN里提取proposal，在进行RoI pooling前，要根据RPN给出的proposal信息在基础网络提取出的整个feature map上找到每个proposal对应的那一块feature map，具体的做法是：根据RPN给出的边框回归坐标，除以尺度因子16，因为vgg16基础网络四次池化缩放了16倍。这里必然会造成坐标计算会出现浮点数，而Faster R-CNN里对这个是进行了舍入，这是一次对平移同变性的破坏；同样的问题出现在后面的RoI pooling中，因为要得到固定尺寸的输出，所以对RoI对应的那块feature map划分了网格，也会出现划分时，对宽高做除法出现浮点数，这里和前面一样，简单粗暴地进行了舍入操作，这是第二次对平移同变性的破坏。如下图，网格的划分是不均匀的： 总之，Faster R-CNN破坏了像素到像素之间的这种平移同变性。RoI Align就是要在RoI之前和之后保持这种平移同变性，避免对RoI边界和里面的网格做量化。如下图： 针对输入的feature map找到对应的RoI，是通过$x/16$而不是像Faster R-CNN中$[x/16]$，$[\cdot]$代表舍入操作。所以可以看到第一幅图中RoI并没有落在整数的坐标上。 对RoI划分为2x2的网格（根据输出要求），每个小的网格里采样4个点，使用双线性插值根据临近的网格点计算这4个点的值，最后再对每一个网格进行最大池化或平均池化得到最终2x2的输出。 network]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Feature Pyramid Networks for Object Detection]]></title>
    <url>%2F2018%2F05%2F06%2FFPN%2F</url>
    <content type="text"><![CDATA[这篇文章提出特征金字塔网络（FPN），将分辨率高语义性弱的浅层特征和分辨率低语义性强的深层特征融合，形成了多级金字塔，在金字塔每一级上独立检测目标。FPN不仅对多尺度的目标检测具有很好的效果，还可以应用到分割任务中。 Introduction识别不同尺度的目标是计算机视觉的一项基本挑战，下面介绍四种利用特征的形式： 图像金字塔：构建在图像金字塔上的特征金字塔（简称为特征化的图像金字塔），如图（a）。这种情况下，图像被采样为多种尺度，然后生成不同尺度的特征。DMP就是使用密集的尺度采样获得了不错的效果。这种方法被大量用在手工设计的特征中。优点是：每一级的特征语义信息都比较强，缺点是预测时间长。在图像金子塔上端到端地训练深度卷积神经网络是不切实际的，因为内存消耗大，所以即使要用，也只用在做预测时。 利用卷积神经网络提取特征，使用最后一层的特征做预测。卷积神经网络可以提取高级的语义表达，对尺度变化有更好的鲁棒性，所以可以使用单尺度特征，如图（b）。 使用不同层的特征进行预测，如图（c）。SSD使用卷积神经网络多个层的特征分别做预测，如同一个特征化的图像金字塔。SSD重复利用前向传播过程中计算好的不同层特征，所以几乎没有带来额外代价。但SSD没有利用到足够底层的特征，因为底层特征语义信息弱，但是底层特征分辨率高，对检测小目标很重要。 本文提出的FPN，将低分辨率语义信息强的浅层特征与高分辨率语义信息弱的深层特征进行组合，构建特征金字塔，在金字塔的每一级上分别做预测，如图（d）。 还有一种相似的结构，采用自顶向下的方法和跳跃连接（skip connection），目的是生成单个具有较好分辨率的高级特征图，然后在这个特征图上做预测，如下面上半部分的图。本文与其结构很接近，但是利用它形成一个金字塔，在金字塔的每一级独立地进行预测。 Feature Pyramid NetworksFPN接受一个具有任意尺寸的单尺度图像作为输入，在多个层级以全卷积的形式生成不同尺寸比例的特征图。FPN的构建涉及三个部分：自底向上的路径（Bottom-up pathway），自顶向下的路径（top-down pathway），横向连接（lateral connection）。 自底向上的路径自底向上的路径就是主干网络（backbone）的前向计算，产生不同尺度的feature map，尺度的比例为2，即每次下采样都是缩小2倍。自底向上的过程，空间分辨率降低，但是语义性增加。 这里把那些会输出同样尺寸feature map的层归为一个stage。每个stage都定义了一级金字塔，每个stage的最后一层特征被选取出来。本文的backbone是ResNet，选取的是每一个stage最后一个残差块的输出，将Conv2，Conv3，Conv4，Conv5的输出定义为{C2，C3，C4，C5}。这些输出相对于输入图像，stride为{4，8，16，32}，即分辨率缩小的倍数。为啥不用C1呢，因为维度太高了，内存消耗大。 自顶向下和横向连接FPN通过自顶向下的方式，从语义信息丰富的层出发，构建出分辨率更高的层，将这些层的特征与浅层的特征通过横向连接融合。如下图，对于空间分辨率较粗糙的深层特征，进行2倍的上采样（最近邻），相应的浅层特征使用1x1的卷积降维，之后与上采样的特征通过逐元素相加合并。合并后的特征又通过3x3的卷积生成最终的feature map，即{P2，P3，P4，P5}，这一步降低了上采样的混叠效应（aliasing effect）。 FPN可以更加详细的用下图表示： 金字塔中所有的层都共享分类器和回归器，就像传统的图像金字塔那样。固定特征的通道数为256，因此所有额外的层输出都为256通道。这些额外的层没有使用非线性。 ApplicationsFPN for RPN回顾一下Faster R-CNN中的RPN： 预先定义了一组不同尺度和高宽比的anchor，覆盖不同形状的目标。 在最后一个单尺度的共享卷积层输出的feature map上，使用一个3×3的滑动窗（卷积核），随后是两路1x1的卷积，分别用来分类是否为目标以及回归bounding box，这里将这一部分称为网络的头（head）。 将FPN应用在RPN上的要点： 将单尺度的feature map替换为FPN，为每一级金字塔附加一个head，也是3x3的卷积和两路1x1的卷积。 每一级都是单尺度的anchor。因为网络头需要在每层金字塔的feature map上的所有位置滑动，就没必要在特定的一级使用多尺度的anchor了。{P2，P3，P4，P5}上的anchor面积分别为{32x32，64x64，128x128，256x256，512x512}。 跟随Faster R-CNN，每一级金字塔使用多个高宽比{1:2，1:1，2:1}。所以金字塔上一共是15种anchor：5种尺度x3种高宽比. 与Faster R-CNN一样，正样本是与ground truth有着最高IoU的，以及与任何ground truth有着高于0.7的IoU的anchor，与所有ground truth的IoU都小于0.3的anchor作为负样本。注意：ground truth是与anchor相关的，因此也就与金字塔的某一级相关了。 RPN头在金字塔的所有层上共享参数，作者做了不共享参数的实验，发现性能相似，说明FPN中金字塔的所有层共享相似的语义级别。 FPN for Fast R-CNNFaster R-CNN另一模块是Fast R-CNN中基于region的检测器，它使用RoI Pooling提取特征，也是在一个单尺度feature map上进行预测。 应用FPN时，使用前面一节描述的RPN生成多个感兴趣区域RoI，根据RoI在原始图像中的尺寸，选择尺度最正确的feature map，去提取这个RoI的feature。 若RoI在原始图像中高宽为w和h，那么对应的特征层为 224是标准的ImageNet预训练尺寸，$k_0$是当RoI的尺寸为224x224时，对应的特征级。使用ResNet为基础网络的Faster R-CNN，使用C4作为后续部分的输入feature，所以$k_0=4$。直觉上，公式表示当RoI的尺度变小时，比如变为112，那么它应该被映射到一个分辨率更精细的层（$k=3$）。 Extensions: Segmentation ProposalsFPN还可以扩展到分割任务中，下面使用FPN生成分割的proposal。特征金字塔的构建方式与应用FPN到目标检测中一样，但是维度设为128，原来是256。以全卷积的形式，使用5x5的滑动窗在特征图上滑动，产生14x14的掩码和目标分数。浅橙色是相应的图像尺寸，深橙色是目标尺寸，可以看出掩码也是在金字塔的不同级独立预测的。 Reference1.Understanding Feature Pyramid Networks for object detection (FPN)]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>instance segmentation</tag>
        <tag>feature pyramid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks]]></title>
    <url>%2F2018%2F04%2F25%2Fmtcnn%2F</url>
    <content type="text"><![CDATA[提出一种深度级联的多任务框架，利用检测和对齐的固有相关性去增强它们的性能。实际中，利用有三阶段精细设计的深度卷机网络的级联结构，由粗到精地检测和对齐人脸。 Introduction人脸识别中视觉的变化，比如遮挡，姿态变化和极端的光照条件，会给人脸检测和对齐带来巨大挑战。AdaBoost和Haar-Like特征训练的级联分类器虽然可以达到比较高的效率，但是大量研究表明这类检测器在人脸有着较大的视觉变化时，检测精度会大大降低。DPM（deformable part models）用于人脸检测也可以达到非常好的性能，然而计算代价太大，并且在训练时可能要求大量的标注。 人脸对齐领域的方法可以大致划分为两类：基于回归的方法和模板匹配方法。过去大部分的人脸检测和对齐方法都忽视了这两种任务之间的固有联系。 另一方面，挖掘难样本对于增强检测器的性能是至关重要的。传统的方法都是离线模式去挖掘，对于人脸检测任务来说，需要一种在线的难阳本挖掘方法，这样可以自动适应当前的训练状态。 本文中，通过多任务学习使用统一的级联CNNs集成这两种任务。提出的CNNs包含三个阶段：第一阶段，使用浅层的CNN(fast Proposal Network (P-Net))生成候选窗口；第二阶段，通过更加复杂的CNN(Refinement Network (R-Net))去精炼窗口，拒绝掉大量的非人脸的窗口；第三阶段，使用更加强大的CNN(Output Network (O-Net))去再次精修结果并输出5个landmark位置。 贡献： 提出一种级联的CNNs框架做人脸检测和对齐，设计了一种轻量的CNNs结构用于实时性能。 提出一种在线难样本挖掘（online hard sample mining）方法去提高性能。 ApproachOverall Framework首先将给定图像缩放到不同的尺度建立图像金字塔，这将是后面三阶段级联框架的输入。 第一阶段：采用全卷积神经网络，即P-Net，去获得候选窗体和bounding box回归向量。同时，候选窗体根据估计的bounding box向量进行校准。然后，利用NMS方法合并高度重叠的候选框。 第二阶段：所有的候选框被输入R-Net，进一步拒绝掉大量的错误候选框，同样使用bounding box回归校正候选框，并实施NMS。 第三阶段：和第二阶段相似，但是目的是利用更多的监督去判断人脸区域，并输出5个landmark位置。 CNN Architectures多个CNN被用于人脸检测，但其性能可能受到以下情况的限制： 卷积层中的卷积核缺乏多样性，限制他们的识别能力； 对比多类识别检测和分类任务，人脸检测是一个二分类问题，因此每一层需要的卷积核较少。所以本文减少卷积核数量，并将5*5的卷积核大小改为3*3的，以此在增加深度来提高性能的同时减少计算。 Training 本算法从三个方面对CNN检测器进行训练：人脸分类、bounding box回归、landmark定位（关键点定位）。 人脸分类： 二分类问题，使用交叉熵损失函数： bounding box 回归： 回归问题，使用欧氏距离计算的损失函数： landmark 定位： 回归问题，使用欧氏距离计算损失函数： 在每个CNN中实现的是不同的任务，所以在学习过程中有几种类型的训练图像：人脸，非人脸，部分对齐的人脸。在这种场合下，上面的式子不能使用，比如，对于背景区域的样本，只需要计算检测损失，其他两种损失设置为0，所以使用一些系数，总体的学习目标表示为： Online Hard sample mining： 每一个mini-batch中，对从所有的样本前向运算得到的损失排序，选择前70%作为难样本。在反向传播中，只计算来自于这些难样本的梯度。这意味着在训练中忽视掉那些对增强检测器性能帮助甚小的简单样本。 Experiments在训练中有四种数据： 负样本：与任何ground truth faces的IoU低于0.3的。 正样本：与一个ground truth face的IoU高于0.65的。 Part faces：与一个ground truth face的IoU在0.4~0.65之间的。 Landmark faces：标定了5个landmark的。负样本和正样本用于人脸分类，正样本和part faces用于bounding box回归，landmark faces用于landmark定位。上面的样本比例：3：1：1：2。 数据的收集方法如下： P-Net：随机地从WIDER FACE数据集中裁切一些图像块，收集正样本，负样本，part人脸。从CelebA数据库裁切人脸作为landmark人脸。 R-Net：使用框架的第一阶段在WIDER FACE中检测人脸，收集正样本，负样本和part人脸，同时从CelebA中检测landmark人脸。 O-Net：与R-Net相似的方法收集数据。但是是使用前两个阶段去检测人脸和收集数据。]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>CNN</tag>
        <tag>face detection</tag>
      </tags>
  </entry>
</search>
