<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[两步实现博客评论的添加]]></title>
    <url>%2F2018%2F06%2F01%2FGitment%2F</url>
    <content type="text"><![CDATA[只需要简单的两步，就能用Gitment为hexo搭建的博客实现评论功能。 注册 OAuth Application注册一个OAuth Application。最重要的是填对callback URL，比如你的博客主页地址。其他的内容不重要。注册成功会得到一个client ID和一个 client secret。 引入 Gitment 创建一个repository，用来存储评论。这个可以新建，也可以是自己博客的那个仓库。我自己新建了一个仓库，名为GitComment。 引入 Gitment 创建一篇博客，在本地的文件夹中会产生一个a.md的文件，也就是博文内容写入的文件。 这个文件中前面都是自己的博文，只需要把下面的代码添加到这个文件的最后123456789101112131415&lt;div id="container"&gt;&lt;/div&gt;&lt;link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"&gt;&lt;script src="https://imsun.github.io/gitment/dist/gitment.browser.js"&gt;&lt;/script&gt;&lt;script&gt;var gitment = new Gitment(&#123; id: '页面 ID', // 可选。默认为 location.href owner: '你的 GitHub ID', repo: '存储评论的 repo', oauth: &#123; client_id: '你的 client ID', client_secret: '你的 client secret', &#125;,&#125;)gitment.render('container')&lt;/script&gt; 完成上面的两步后，只需要发布自己的博客就可以了。要注意的是，每一篇需要添加评论的博文文章源码的最后，都要添加上面的代码。发布之后，还需点进博文网页，在评论处点击初始化。如果遇到有问题，可以参考Gitment评论功能接入踩坑教程。 Reference 使用 GitHub Issues 搭建评论系统 Gitment评论功能接入踩坑教程 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InstanceFCN:Instance-sensitive Fully Convolutional Networks]]></title>
    <url>%2F2018%2F05%2F31%2FInstanceFCN%2F</url>
    <content type="text"><![CDATA[这篇文章发表于2016年，提出了一种全卷积网络，可以计算一组实例敏感的分数图，每一个分数图都是实例中一个相对位置上每个像素分类的结果。之后实例敏感的分数图经过一个简单的组合模块，输出每一个位置的候选实例。 IntroductionFCN并不能直接得到实例分割的结果。因此这篇文章提出了端对端的全卷积网络分割候选实例。在InstanceFCN中，与FCN一样的是每一个像素仍然代表了一个分类器，但是不像FCN对一个类别只产生一个分数图，而是计算一组实例敏感分数图（Instance-sensitive score map），其中每一个像素是一个目标实例相对位置的分类器。 如下图，产生了9个分数图，分别对应了3x3网格中的每一个相对位置，可以看到6号分数图在目标实例的右边有着较高的分数。通过组合这些分数图的输出可以得到目标的实例分割结果。 在这篇文章之前，还有DeepMask实例分割。DeepMask是一种实例分割proposal方法，将一个图像滑动窗映射到$m^2-d$的向量（比如，m为56），这个向量代表了$m \times m$分辨率的掩码，这是通过$m^2-d$的全连接层计算的。与DeepMask不同，InstanceFCN没有与掩码尺寸相关的层，并且每一个像素是低维的分类器。 Instance-sensitive FCNs for Segment ProposalFrom FCN to InstanceFCNInstance-sensitive score maps对于FCN语义分割来说，如果图像中就只有一个实例，那么语义分割的结果就很好地表示了实例掩码，但是像上面的图中，两个目标有部分重合的区域，FCN是无法区分开的，所以只要能把这种重叠部分区分好，那么问题其实就解决了。因此InstanceFCN基于这样的分析，引入了相对位置的概念。既然原始的FCN中，每一个输出像素是一个类别的分类器，那么提出一种新的FCN，每一个输出的像素是实例中相对位置的分类器。文中定义了9（$k^2$）个相对位置，所以FCN输出9个实例敏感的分数图。 Instance assembling module上面的实例敏感分数图只是代表了实例中相对位置的分数，还没有得到实例分割结果，因此后续还需要一个实例组合模块（Instance assembling module）。这个模块没有可学习参数，具体要做的事情就是复制粘贴：在这组分数图上使用一个$m \times m$的滑动窗，在这个滑动窗中，每一个$\frac m k \times \frac m k$的子窗口直接从相应的分数图中同样的子窗口复制那一部分数值。之后这组子窗口按照相对位置拼起来就得到了$m \times m$的结果。 Local Coherence局部一致性的意思是，对一幅自然图像中的一个像素而言，当两个相邻的窗口中进行评估时，预测结果极有可能是相同的。当窗口被平移一小步时，并不需要完全地重新计算预测。如下图，把一个蓝色的窗口平移一小步，得到平移后的窗口为红色，图像中相同的那个黄色像素点将会得到相同的预测，因为它是从相同的分数图中复制而来的（除了在相对位置的划分区附近的几个像素 ）。这就允许当掩码分辨率为$m^2$时，可以保存大量的参数。这与DeepMask的机制不同，DeepMask基于滑动的全连接层，当窗口平移一步时，图像中同一个像素是由全连接层两个不同的通道预测的。所以当在两个相邻窗口评估时，同一个像素的预测通常不会相同。 通过利用局部一致性，网络中卷积层的尺寸和维度就独立于掩码分辨率了。这不仅降低了掩码预测层的计算代价，而且更重要的是减少了掩码回归的参数量，减少对于小数据集过拟合的风险。 Algorithm and ImplementationNetwork architecture 使用VGG-16做特征提取，其中13个卷积层可以应用在任意尺寸图像上。 做了一些修改：将最大池化层pool4的stride从2改为1，conv5_1到conv5_3中相应的卷积核通过“hole algorithm”调整。经过调整后的VGG网络conv5_3特征图的有效stride是8。减小的stride直接决定了分数图的分辨率。 在特征图的顶部，有两个全卷积分支，一个用来估计分割实例，另一个用来为实例打分。 对于第一个分支，采用512-d的卷积层（带有ReLu激活函数）去转换特征，然后使用3x3的卷积层去生成一组实例敏感的分数图，最后的卷积层有$k^2$个输出通道，对应着$k^2$个实例敏感的分数图。在实例敏感的分数图顶部，使用组合模块在分辨率为$m \times m$（m为21）的滑动窗中生成目标实例。 对于第二个分支，使用3x3的512-d的卷积层后面跟随一个1x1的卷积层。这个1x1的卷积层是逐像素的逻辑回归，用于分类以这个像素为中心的滑动窗是实例或不是实例。所以这个分支的输出是目标（objectness）分数图，其中一个分数对应生成一个实例的滑动窗。 Training网络是端到端进行训练的，前向传播中，计算一组实例敏感的分数图以及目标分数图，之后，采样256个滑动窗，从这些滑动窗中组合实例用于计算损失函数，损失函数定义如下： $$\sum_i(L(p_i, p^*_i) + \sum_j L(S_{i,j}, S^*_{i,j}))$$ 其中，$i$是采样窗口的索引，$p_i$是窗口中实例的预测分数，如果窗口中是正样本，那么这个分数就为1，否则为0。$S_i$是窗口中组合的分割实例，$S_i^*$是分割实例的ground truth，$j$是窗口中像素的索引，$L$是逻辑回归损失。256个采样窗口中，正负样本比例是1:1。 Inference推断过程就是对输入图像生成实例敏感分数图以及目标分数图，之后组合模块通过在分数图上应用滑动窗产生每一个位置的分割实例。每一个实例与目标分数图中的一个分数相关联。在多尺度问题上， 是把图像缩放到不同尺度，然后计算每一个尺度上的实例。最终得到的是二值掩码，应用NMS生成最后的分割proposal。 Experiments定量分析可以参考论文，这里展示一些效果图。 简评主要的关键点在于实例敏感的分数图，其实个人感觉就是R-FCN中提到的位置敏感分数图，但是R-FCN是在这篇文章之后出的。之所以可以使用这些分数图解决实例分割问题，是因为同一个像素点，如果它在实例中的相对位置的不同，那么它将会对应着不同编号的分数图，所以它在不同的实例中，有着不同的语义。总之，这是一个很好的思路，不过作者所说的端对端仅仅是针对分割proposal而言的，如果要得到实例的类别，其实是需要后续的网络处理的，这两个任务之间是分离的。之后作者又出了一篇FCIS，与这篇文章不同的是，FCIS是RPN+Position Sensitive ROI Pooling + Inside/Outside Score maps ，因为这2种分数图的联合，使得分割与检测同步进行，整个网络是端对端的。 Reference 论文原文 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络为什么具有平移不变性？]]></title>
    <url>%2F2018%2F05%2F30%2FTransform-Invariance%2F</url>
    <content type="text"><![CDATA[在我们读计算机视觉的相关论文时，经常会看到平移不变性这个词，本文将介绍卷积神经网络中的平移不变性是什么，以及为什么具有平移不变性。 什么是平移不变性不变性不变性意味着即使目标的外观发生了某种变化，但是你依然可以把它识别出来。这对图像分类来说是一种很好的特性，因为我们希望图像中目标无论是被平移，被旋转，还是被缩放，甚至是不同的光照条件、视角，都可以被成功地识别出来。 所以上面的描述就对应着各种不变性： 平移不变性：Translation Invariance 旋转/视角不变性：Ratation/Viewpoint Invariance 尺度不变性：Size Invariance 光照不变性：Illumination Invariance 平移不变性/平移同变性在欧几里得几何中，平移是一种几何变换，表示把一幅图像或一个空间中的每一个点在相同方向移动相同距离。比如对图像分类任务来说，图像中的目标不管被移动到图片的哪个位置，得到的结果（标签）应该是相同的，这就是卷积神经网络中的平移不变性。 平移不变性意味着系统产生完全相同的响应（输出），不管它的输入是如何平移的 。平移同变性（translation equivariance）意味着系统在不同位置的工作原理相同，但它的响应随着目标位置的变化而变化 。比如，实例分割任务，就需要平移同变性，目标如果被平移了，那么输出的实例掩码也应该相应地变化。最近看的FCIS这篇文章中提到，一个像素在某一个实例中可能是前景，但是在相邻的一个实例中可能就是背景了，也就是说，同一个像素在不同的相对位置，具有不同的语义，对应着不同的响应，这说的也是平移同变性。 为什么卷积神经网络具有平移不变性简单地说，卷积+最大池化约等于平移不变性。 卷积：简单地说，图像经过平移，相应的特征图上的表达也是平移的。下图只是一个为了说明这个问题的例子。输入图像的左下角有一个人脸，经过卷积，人脸的特征（眼睛，鼻子）也位于特征图的左下角。 假如人脸特征在图像的左上角，那么卷积后对应的特征也在特征图的左上角。 在神经网络中，卷积被定义为不同位置的特征检测器，也就意味着，无论目标出现在图像中的哪个位置，它都会检测到同样的这些特征，输出同样的响应。比如人脸被移动到了图像左下角，卷积核直到移动到左下角的位置才会检测到它的特征。 池化：比如最大池化，它返回感受野中的最大值，如果最大值被移动了，但是仍然在这个感受野中，那么池化层也仍然会输出相同的最大值。这就有点平移不变的意思了。 所以这两种操作共同提供了一些平移不变性，即使图像被平移，卷积保证仍然能检测到它的特征，池化则尽可能地保持一致的表达。 Reference How is a convolutional neural network able to learn invariant features? Why and how are convolutional neural networks translation-invariant? var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCIS:Fully Convolutional Instance-aware Semantic Segmentation]]></title>
    <url>%2F2018%2F05%2F30%2FFCIS%2F</url>
    <content type="text"><![CDATA[这篇文章发表于2016年，提出了第一个端到端的实例分割模型，同时检测和分割实例。通过引入位置敏感的内/外分数图，底层的卷积表达被两个子任务以及所有的感兴趣区域完全地共享。 IntroductionFCN接受一幅任意尺寸的图像，通过一系列的卷积，最终为每一个像素产生关于每个类别的概率，从而实现了简单、高效、端对端的语义分割。但是FCN并不能解决实例分割任务，因为卷积具有平移不变性，同一个像素点接收的响应（类别分数）是相同的，与它在上下文中的相对位置无关。简单点说，因为卷积的平移不变性，图像中物体无论在哪个位置，它所对应的类别是固定的，所以语义分割中，每个像素点只能对应一种语义。实例分割任务需要在区域级上操作，并且同一个像素在不同的区域中具有不同的语义，比如在这个区域中，它可能是前景，但在另一个区域中可能就是背景。 现有的方法解决这个问题主要是通过在3个stage中采取不同的子网络： 利用FCN在整幅图像上生成中间特征图和共享特征图 使用一个池化层将共享特征图中的每一个RoI处理成固定尺寸的特征图 在最后的网络中使用一个或多个全连接层把每一个RoI的特征图转换为每一个RoI的掩码。 这种方案存在几个问题： RoI池化将每一个RoI转换为相同的尺度，feature warping和resizing损失了空间细节，得到的特征表达降低了分割的精确度。特别是对大目标而言，其特征图会被缩小后再处理，那么一些小的部件可能就会丢失。 在不使用局部权重共享的情况下，全连接层参数是过多的。训练和测试的代价也会较大。 FCIS中针对上面的问题处理方式是： 去掉RoI池化，使用位置敏感分数图，将一组不同位置的分数图通过组合，得到RoI的特征 底层的特征是完全共享的，所有的RoI都是直接从计算好的特征图中取出来；全连接层被去掉，使用了全卷积网络，参数量减少。 下图中（a）是传统的用于语义分割的FCN，得到一个分数图，用来预测每一个像素所属的类别，这种方法并不知道像素是什么实例的。（b）是Instance-sensitive fully convolutional networks这篇文章的方法，InstanceFCN使用3x3的位置敏感分数图编码相对位置信息。它的下游网络对segment proposal分类。检测和分割是分离的，不是端对端的。（c）是FCIS，位置敏感的内/外分数图被联合使用，同时进行目标分割和检测。 Our ApproachPosition sensitive Score Map Parameterization在FCN中，分类器被用于产生每一个像素属于目标类别的概率，这是平移不变的，而且其所属的目标实例是未知的。那么对于相邻的两个目标，有时候一个像素在这个目标上可能是前景，但是在另一个目标上可能就是背景了，在不同的位置对应着不同的语义。所以像FCN那样使用一个分数图并不足以区分这种情形。 为了引入平移同变性，InstanceFCN中采用了$k^2$个位置敏感分数图，如图中共9个分数图。一组分数图对应的是RoI中不同位置的分数，比如第一个分数图对应了RoI中均匀划分的3x3的网格中左上角的那一块的分数。 为什么使用位置敏感分数图可以带来平移同变性呢？比如下图中两个人物分别对应着两个不同的RoI，红点部分是两个RoI重叠的部分，这幅图像产生的inside分数图是左边的9个，被两个RoI共享。位置敏感分数图中同一个像素点，在不同的相对位置，有着不同的分数。对每个RoI，分别使用位置敏感的RoI池化组成最终的RoI的inside/outside分数图。可以看到红色点对应的分数是不一样的，它在左边人物的RoI中前景分数很高（白），而在右边人物的RoI中前景分数就很低。同一个像素在不同的位置对应着不同的语义，这就是平移同变性。 Joint Mask Prediction and Classification使用位置敏感分数图可以解决实例分割问题，但是实例的类别是未知的，过去的方法都是使用一个子网络去解决分类问题，FCIS中是通过2类分数图inside/outside score map来联合解决分割和分类问题的： inside score map：像素在某个相对位置属于某个目标实例，并且在目标边界内 outside score map：像素在某个相对位置属于某个目标实例，并且在目标边界外 同一组位置敏感分数图，以及底层卷积被目标检测和分割两个子任务共享。对于RoI中的每一个像素，有两个任务：检测，它是否属于某一个目标的bounding box（detection+/detection-）；分割，它是否属于某个实例，即是否在实例边界内（segmentation+/segmentation-）。简单的方式是独立地训练两个分类器，两个分数图是分开使用的，这种情况下，两个分类器是两个1x1的卷积层。 但是FCIS通过一个联合规则（joint formulation），融合了inside/outside score map这两种分数图，如果一个像素在一个RoI中是前景，那么inside分数就会比较高，而outside分数就会比较低。对一个像素来说总共有三种情形： 高的inside分数和低的outside分数：即detection+，segmentation+ 低的inside分数和高的outside分数：detection+，segmentation- 低的inside分数和低的outside分数：detection-，segmentation- 这里并没有出现两种分数都很高的情况，作者没有提到，其实也可以这么理解，因为对于一个像素来说，只可能是上面的3种情况中的一种，两者分数都很高代表了这个像素点在实例边界内，同时在实例边界外，这是不可能的。 两种分数图之后会被联合使用： 对于检测，使用逐像素的max操作区分前两种情况和第三种情况。之后跟随一个在所有类别上的softmax操作，然后通过对所有像素的概率进行平均池化，得到整个RoI的检测分数。 对于分割，在每个像素上使用softmax区分第一种和第二种情况，RoI的前景掩码（概率值）是每个像素对于每个类别的分割分数的并集。 检测和分割这两个分数集合来自两个1x1卷积层。inside/outside分类器被联合训练，因为它们接收来自于分割和检测损失的梯度。 这种方法有许多可取的特性：每一个RoI组分没有自由参数；特征图是通过一个FCN产生的，没有涉及到特征的warping，缩放以及全连接层；所有的特征和分数图都遵守原始图像的高宽比；FCN的局部权重共享特性被保持并且作为一种正则化机制；所有的RoI计算是简单、快速的，甚至可以是忽略不计的。 An End to End SolutionFCIS使用ResNet-101，去掉最后用于分类的全连接层，只训练前面的卷积层，最后的特征图是2048通道的，通过1x1的卷积降维到1024。ResNet中，特征图分辨率的stride是32，对于实例分割来说太过粗略了。为了降低stride，使用了”hole algorithm”，conv5第一个block的stride从2降到1，所以最终特征stride为16。为了保持感受野，conv5所有的卷积层dilation都设置为2。 下图是网络结构示意图： 首先是基础网络提取特征，作者为了与其他方法公平比较，在conv4之后使用RPN生成RoIs。RPN也是全卷积的。 使用1x1的卷积从conv5的特征图生成$2k^2(C+1)$的分数图：C类+1个背景；默认地，$k=7$，每一类2组分数图，每一组都是$k^2$个。因为最终的特征图相比原始图像缩小了16倍，因此在特征图上，每一个RoI相当于被投影进16倍小的区域中。 并列的1x1的卷积层，通道数为$4k^2$，添加在conv5特征图后用来估计bounding box的偏移量。 随后经过前面提到的联合公式，将每一组的$k^2$个分数图进行组合，得到最终的内/外分数图。 最后对内/外分数图进行逐像素的softmax得到实例掩码；逐像素的max-&gt;softmax-&gt;平均池化-&gt;投票，得到RoI所属的类别。 Inference 对一张输入图像，RPN产生300个分数最高的RoI，然后它们通过bounding box分支产生精修过的新的300个RoI。对每一个RoI，最终得到它的分类分数和所有类别的前景掩码（概率）。 使用IoU阈值为0.3的NMS，过滤重叠的RoI。剩下的按照最高的类别分数进行分类。 前景的掩码通过掩码投票获得。具体来说，对一个RoI，从600个RoI中找到所有的与它IoU高于0.5的RoIs，根据像素对应的分类分数加权求平均，然后二值化作为输出。 Training 正负样本：如果一个RoI的box与最近的ground truth的IoU大于0.5，则这个RoI为正样本，否则为负样本 每个RoI有3项权重相等的损失： C+1类的softmax检测损失 softmax分割损失：ground truth和预测的前景掩码之间的损失，累加RoI所有像素上的损失然后通过RoI的尺寸进行标准化。 bbox回归损失。后两项损失只计算正样本的。 训练图像缩放到短边为600。 OHEM，每个mini-batch，一幅图像的300个RoI被进行前向传播，选择其中损失最高的128个RoI反向传播它们的误差梯度。 对于RPN，默认使用9个anchor（3个尺度，3个高宽比）。COCO数据集多用3个anchor 整个网络是联合训练的，实现特征共享 简评FCIS可以说是RPN + Position Sensitive ROI Pooling + Inside/Outside Score maps，将这几部分融合进一个网络，进行端到端的实例分割。主要的亮点就是 Inside/Outside分数图，将检测和分割两种任务关联了起来。 Reference 论文原文 关于FCIS的PPT-非原作者 CVPR上的文章讲解 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCN:Fully Convolutional Networks for Semantic Segmentation]]></title>
    <url>%2F2018%2F05%2F25%2FFCN%2F</url>
    <content type="text"><![CDATA[这篇文章发表于2015年，提出了使用全卷积网络解决语义分割任务，达到像素级的分类结果。 Introduction这篇文章是语义分割任务中，首次端到端训练FCN用于逐像素预测，并且采用监督学习和预训练模型的。语义分割任务在语义性和位置之间有一些矛盾：全局信息解决是什么的问题，局部信息解决在哪里的问题。深度特征的层级结构在局部到全局的金字塔上共同编码了位置和语义信息。这篇文章定义了一种跨层连接组合深的，粗略的层上的语义信息和浅的，精细的层上的外观信息。 Fully convolutional networks卷积网络因为全连接层或者全局池化层具有平移不变性，Mask R-CNN中也提到过：分类网络需要平移不变性，因为目标在任何位置，不应该影响对它的预测。但是对于检测或者分割任务而言，则需要平移同变性，因为目标位置的移动必须使得输出也相应改变。全卷积网络就具有平移同变性，这就非常适合分割任务。但是卷积网络输出的特征图分辨率是随着层数的加深越来越低的，这对分割任务是不利的，因为分割需要对图像边缘的像素进行精细地分类。 总之，对于传统的分类网络，运用到分割任务中，需要解决两个问题： 替换全连接层为卷积层，从而得到平移同变性的网络 需要将低分辨率的输出特征图恢复到原始图像尺寸，即上采样，从而得到更加精细的结果 Adapting classifiers for dense prediction典型的识别网络比如LeNet，AlexNet等，它们的全连接层具有固定的维度并且丢弃了空间坐标信息。但其实这些全连接层可以被看做是卷积核覆盖了整个输入区域的卷积。下图是全连接转换为卷积层的过程，使得分类网络输出一个热点图(heatmap)。其实就是把4096-d的特征转换为1x1x4096的tensor。 具体实现的方式是： 对于AlexNet，第一个全连接层的输入是7x7x512的，原始网络后面是4096个单元的全连接层，只需将其改为：使用4096个7x7x512的卷积核，就可以得到1x1x4096的输出，后面也是使用与输入尺寸相同的卷积核，就可以把所有全连接层替换为卷积层了。 输出的特征图与原始网络的输出是等价的，但是参数量会更少。另外将网络转换为全卷积网络后，输入的图像就可以是任意尺寸的。因为原始网络需要固定输入主要是因为全连接层是固定维度的，它必须接受固定维度的输入。 Upsampling is backwards strided convolution随着网络层数加深，特征图分辨率会越来越小（粗略）。而分割任务需要对应到原始图像尺寸，得到每个像素的类别，为了将特征图恢复到原始图像的分辨率，FCN使用了上采样。加入对于原始图像， 经过逐层卷积后，图像缩小了32倍，那么对于最后一层的输出，就需要32倍的上采样。 上采样的方式有两种：一是插值；二是反卷积。使用反卷积可以进行端到端的学习，相对于双线性插值来说，反卷积加激活函数可以学习到非线性的上采样。所以文中使用了反卷积做上采样，另外设计了跨层连接。不同的分辨率预测的精细程度是不同的，以下图中8x采样的预测特征图来看，它包含了3部分：conv7进行4x上采样+pool4进行2x上采样+pool3。作者对比了32倍，16倍以及8倍上采样的预测结果： 跨层结构具体如下图，如文中所说，它融合了深层较强的语义信息和浅层较精细的局部信息，是一种全局信息和局部信息的组合，即“Combining what and where”，达到对预测结果的精修。 Learnable Upsampling: “Deconvolution”Deconvolution并不是一个非常准确的表达，比较正确的表达应该是转置卷积(convolution transpose )。 下面看一个卷积过程，输入经过stride为2，pad为1，size为3x3的卷积核，得到2x2的输出： 对于卷积的逆过程，输入是2x2，输出为4x4，要进行2倍上采样： 卷积层的前向传播过程，就等同于转置卷积的反向传播过程；卷积层的反向传播过程，就等同于转置卷积的前向传播过程。 简评这篇文章实现了端到端的语义分割，“pixels in, pixels out”。给一张$W \times H$的输入图像，网络最终能输出$W \times H \times C$的预测，其中$C$代表了类别数。所以每一个像素点都可以被分类到一个类别，给语义分割任务提供了一个很好的思路。最终使用8x上采样的特征图能达到还不错的分割结果，但是多层特征图的融合，其实只是将全局和局部信息组合使用，对于结果有一定的精修作用。在这篇文章之后，有一篇对称反卷积网络，个人感觉，对称结构应该可以更好的解码，有待学习。 Reference 论文原文 CS231n var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>semantic segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DetNet:A Backbone network for Object Detection]]></title>
    <url>%2F2018%2F05%2F21%2FDetNet%2F</url>
    <content type="text"><![CDATA[这篇文章发表于2018年，现有的目标检测框架通常都是使用图像分类网络作为基础网络，但是图像分类与目标检测任务之间有几点不同：（1）像FPN，RetinaNet为了实现不同尺度目标的检测，图像分类网络通常涉及额外的阶段；（2）大的下采样因子带来的大的感受野，这对图像分类任务有利，但是会降低检测任务的性能。因此，这篇文章提出的DetNet是一种新的专门为目标检测任务设计的backbone网络。 Introduction最近的检测器比如FPN中图像分类网络涉及了额外的stage，DetNet也一样。具体有两个创新点： 不同于传统的图像分类预训练模型，DetNet即使在额外的stage依然能够保持较高的特征图空间分辨率。 高分辨率的特征图因为计算和内存代价给构建深度神经网络带来更多的挑战。为了解决这个问题，采用了低复杂度膨胀瓶颈结构（dilated bottleneck structure）。 通常，浅层特征分辨率高，但是感受野小，而深层特征分辨率低，感受野大。通过这些改进，DetNet不仅可以保持高分辨率特征图，而且可以保持大的感受野，这两点对于检测任务都很重要。 DetNetMotivation图像分类网络的设计原则对于目标检测中的定位过程是不利的，因为特征图的空间分辨率会逐渐的降低，比如VGG的stride是32。下图中A是使用传统的backbone的FPN，B是用于图像分类的传统backbone，C是DetNet。可以看出，在与FPN相同的一些stage上，DetNet具有更高的分辨率。（第一个stage未表示在图中）。 网络stage的数量是不同的。图B中，典型的分类网络设计5个stage，每一个stage使用2x的池化或者stride为2的卷积进行下采样。所以输出的特征图空间尺寸相比输入图像有32x的下采样。FPN采取了更多的stage，额外的stage P6被用于更大的目标，RetinaNet添加了P6，P7两个stage。很显然，这些额外的阶段是没有在ImageNet数据集上预训练过的。 大目标的弱可见性。stride为32的特征图语义信息更强，具有大的感受野，因此在图像分类中取得了好的结果。但是对于目标的定位是有害的，在FPN中使用更深的层去预测大目标，对于回归目标位置来说这些目标的边界太模糊了。如果再用上额外的stage，结果会更糟糕。 小目标的不可见性。大的stride导致小目标的丢失。由于空间分辨率的降低和大的上下文信息的集成，小目标的信息很容易被削弱。FPN在较浅的层预测小目标，然而浅层语义信息不强，不足以识别出目标的类别，因此FPN通过采用自底向上的旁路连接弱化了这个问题。但是，如果小目标在更深的层被丢失了，这些上下文线索也将同时丢失。 DetNet就是为解决这些问题而提出的。具有以下几点特性： stage的数量是直接为目标检测设计的，额外的stage也可以在分类数据集上进行预训练 即使比传统的图像分类有更多的stage，比如stage6或stage7，但是依然能保持特征图具有较高的分辨率，同时具有较大的感受野 DetNet Design前4个stage与ResNet-50相同。其他的实现细节： 引入额外的stage，比如P6，将被用于目标检测，和FPN中P6作用类似。同时在stage4之后固定空间分辨率为16x的下采样 因为固定了stage4之后的空间分辨率，为了添加新的stage，采取了一个膨胀瓶颈结构，在每一个stage的开始，使用1x1的卷积投影，如下面的图B。作者发现图B的模型对于多阶段的检测器比如FPN很重要。A，B是在DetNet实验中用到的不同bottleneck block。在传统的ResNet中，当特征图的空间分辨率没有改变时，bottleneck结构中的映射应该是简单的恒等映射，如图A。而DetNet中使用了1x1的卷积。作者认为即使在空间尺寸没有改变的情况下，1x1的卷积投影对于创建一个新的stage是有效的。实验结果也证明了mAP上会有提升。 使用带膨胀的瓶颈网络作为基础的网络block有效地增大感受野。由于膨胀卷积仍然是费时的，stage5和stage6与stage4保持相同的通道（对于瓶颈block是256的输入通道）。这与传统的backbone设计不同，传统的是在后续的stage加倍通道数 上图中“dilate 2”表示使用膨胀卷积，膨胀卷积如下图。其中2表示卷积核中点与点之间的距离，下图中距离为1。 作者使用FPN作为baseline，把PFN的backbone改为DetNet，因为在ResNet的stage4后没有降低空间分辨率，因此在FPN的旁路连接上，只需简单的求和，如图D与E。 ExperimentsDetector training and inference 图像的短边被缩放为800，长边限制在1333，batch size为16，同一批样本图像通过在右下方填充0得到相同大小。 通过在ImageNet上预训练的模型，进行网络初始化，在对检测器进行微调时，固定backbone中stage1的参数，BN层也是固定。 对于proposal的生成，提取分数最高的12000个，然后进行NMS获取2000个RoI用于训练。在测试阶段，使用6000/1000的设置：6000个最高分的proposal，NMS后1000个RoI。除此之外，还用到了Mask R-CNN中的RoI-Align。 Main Results 作者在ImageNet上训练了DetNet，与ResNet-50相比，DetNet多了一个额外的stage6，并且参数量也更大。但是可以看出，分类结果上，DetNet高于ResNet-50，同时比参数量更大的ResNet-101效果还要好。 接下来，作者从头训练了分别基于DetNet-59和ResNet-50的FPN，DetNet效果也更好。 DetNet在大目标的定位上有更好的性能，AP85那一列展示了DetNet-59比ResNet-50高出了5.5。 DetNet在找到丢失的小目标上也做的很好。在下表中小目标的平均召回率上可以看出DetNet的优势。然而在$AR_{85}$的小目标检测性能上可以看到，DetNet与ResNet相当，毕竟基于ResNet-50的FPN在检测小目标上已经使用了大的特征图。同时还可以看到$AR_{85}$的大目标检测性能，DetNet更好。这表明在DetNet有利于大目标定位。然而$AR_{50}$的大目标上性能上改善不大。总而言之，比起找到丢失的大目标，DetNet更擅长于找到精确的大目标 Comparison to State of the Art 目标检测 实例分割 简评这篇文章为解决分类任务和检测任务之间的gap，而提出了DetNet作为目标检测任务的backbone，为了精确地回归目标位置保持特征空间分辨率较高，同时为了减小大的特征图带了的计算代价，使用dilated bottleneck structure+1x1的卷积投影。实验结果显示DetNet擅于找到丢失的小目标，以及准确地定位大目标，同时在实例分割任务上也表现出了好的结果。尽管证明了dilated bottleneck structure+1x1的卷积投影效果好，却没有给出充分解释，毕竟ResNet中用1x1的卷积投影是为了匹配维度，而这里stage5以后，通道数已经不变了，为何还要使用1x1的卷积投影？在如何精确定位小目标，以及寻找丢失的大目标上，DetNet并未表现出优势，这是一个值得继续改进的地方。等看到源码可能会有更新的理解。 Reference 论文原文 各种卷积 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客导航]]></title>
    <url>%2F2018%2F05%2F20%2F%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA%2F</url>
    <content type="text"><![CDATA[为方便浏览本博客的信息，建立了博客导航。 论文笔记 目标检测 实例分割 语义分割 人脸识别 CNN 其他笔记 论文笔记 目标检测DetNetYOLOv3FPNDSSDYOLOv2SSDYOLOMTCNNR-FCNFaster R-CNNFast R-CNNSPPNetR-CNNAdaBoost+Haar-Like 实例分割Mask^X R-CNNMask-R-CNNFCISInstanceFCN 语义分割FCN 人脸识别FaceNetDeepIDCNN卷积神经网络为什么具有平移不变性？ 其他笔记两步实现博客评论的添加 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mask^X R-CNN:Learning to Segment Every Thing]]></title>
    <url>%2F2018%2F05%2F20%2FMask-X-R-CNN%2F</url>
    <content type="text"><![CDATA[这篇文章建立在Mask R-CNN上，提出了一种新的半监督训练和权重迁移方程。在类别数量很大的训练集上每个图像中的实例都有box标记，但是只有一小部分有掩码标记，这篇文章提出的方法可以在这样的数据集上训练实例分割模型。主要的贡献就是训练Mask R-CNN去检测和分割3000种实例，box标记使用Visual Genome数据集，掩码标记来自于COCO数据集的80个类别。 Introduction实例分割可以预测出前景的分割掩码，如Mask R-CNN。但实际上，分割系统涉及的类别很少，而且目前的分割算法要求强大的监督学习，但是新类别的掩码标记代价太高，因此这种监督学习会受到限制。相比之下，bounding box的标记代价较低，所以问题来了：对于所有类别，在没有完整的掩码标记的情况下，是否有可能训练出一个高性能的实例分割模型呢？这篇文章就是在这样的动机下提出了一种部分监督的（partially supervised）实例分割任务，并且设计一种新的迁移学习方法去解决上面提到的问题。 partially supervised实例分割任务： 给定一个感兴趣的类别集合，其中一小部分子集拥有实例分割掩码，然而其他类别只有bounding box标记。 实例分割算法应该利用这些数据拟合出一个模型，它能够分割出所有属于感兴趣目标类别的实例。 把那些同时具有bounding box标价和掩码标记的样本称为强标记样本（strongly annotated examples），而只有bounding box标记的样本称为弱标记样本（weakly annotated examples）。 为了实现这种实例分割，提出了一种构建在Mask R-CNN上的迁移学习方法。Mask R-CNN将实例分割任务分解成两个子任务：bounding box目标检测和掩码预测。每个子任务都有一个专门的网络”head”，使用联合训练。本文提出的方法背后的直觉是：一旦经过训练，bounding box head的参数为每一个目标类别编码一个embedding，使得对于这一个类别的视觉信息迁移到分割任务的head上。 为此，设计了一个参数化的权重迁移方程，该方程是关于一个类别的bounding box检测参数的函数，通过该方程可以预测出该类别的实例分割参数。权重迁移方程可以在Mask R-CNN上进行端对端的训练，使用类别标签和掩码标记作为监督。预测过程，权重迁移方程为每一个类别预测实例分割参数，因此使得模型可以分割所有类别的目标，包括那些训练时没有掩码的类别。 作者对两种设置进行了探索： 使用COCO数据集模拟半监督实例分割任务，具体过程是：将COCO数据集分类两个不相交的子集，一个子集拥有掩码标记，另一个子集只能访问bounding box标记。实验表明，在没有训练掩码的类别上，本文的方法将掩码AP值提高了40%。 使用Visual Genome(VG)数据集的3000个类别，训练大规模的实例分割模型。这个数据集对于大部分目标类别都有bounding box标记，但是定量评估较难，因为很多类别在语义上是重叠的，比如是近义词；并且标记并不详尽，因此精确率和召回率都很难衡量；除此之外VG数据集没有实例掩码。因此，本文使用VG数据集提供大规模实例分割模型的定性输出。 下图中绿色是训练过程中有掩码标记的类别，红色的是训练中只有bounding box标记的类别。 目标类别也许可以通过视觉空间的连续嵌入向量模拟，在这个空间中临近的向量通常在外观或语义本体上很接近。这篇文章的工作中，Mask R-CNN的box head参数包括了类别特定的外观信息，可以被视为是通过训练bounding box目标检测任务学习到的嵌入向量。类嵌入向量通过与视觉相关类共享外观信息使得本文中模型的迁移学习可行。本文的核心思想就是利用迁移学习将bounding box检测学习的知识迁移到实例分割任务中，使得对于没有掩码标记的类别，也能够很好地分割出实例。 Learning to Segment Every Thing设$C$为目标类别集合，对于这个集合要训练一个实例分割模型。大多数已有的方法假设这个集合中所有训练样本都被标记了实例掩码。本文放松了这个要求，假设$C=A \bigcup B$，其中集合A有掩码，而集合B只有bounding box标记，B集合中类别的样本关于实例分割任务是弱标记的。 实例分割模型比如Mask R-CNN，它有bounding box检测和掩码预测两个部分，本文提出$Mask^X R-CNN$方法将模型bounding box检测器获取的类别特定的信息迁移到实例掩码预测中。 Mask Prediction Using Weight TransferMask R-CNN可以被视为对Faster R-CNN检测模型的扩增，它带有一个小的FCN掩码预测分支。在预测阶段，掩码分支对每一个检测到的目标预测它的分割掩码，在训练阶段，掩码分支和Faster R-CNN中的标准的bounding box head联合训练。 在Mask R-CNN中，bounding box分支和掩码分支的最后一层都包含了类别特定的参数，分别用来实现bounding box分类和实例掩码预测。如下图： 本文不再独立地学习类别特定的bounding box参数和掩码参数，而是使用通用的、类别无关的权重迁移方程作为整个模型的一部分进行联合训练，从一个类别的bounding box参数去预测它的掩码参数。 对于一个给定类别$c$，设$w_{det}^c$为bounding box head最后一层类别特定的目标检测权重，$w_{seg}^c$为掩码分支类别特定的掩码权重。与Mask R-CNN不同，这里掩码权重不再作为模型参数，而是使用一个通用的权重预测方程$\tau$来对它参数化： $$w_{seg}^c=\tau(w_{det}^c;\theta)$$ 这里$\theta$是类别无关的可学习参数。同一个迁移方程被用到任何其他类别上，因此$\theta$的设置应该使得迁移方程对训练过程中没有掩码的那些类别具有很好的泛化性能。作者在这里认为这种泛化是可能的，因为类别特定的检测权重$w_{det}^c$可以被视为这个类别的一种基于外观的视觉嵌入（Visual embeddings）。这里我个人并不是非常理解视觉嵌入相关的知识，有待挖掘。 继续说这个迁移方程，它是用一个小的全连接神经网络实现的。下图阐明了权重迁移方程拟合Mask R-CNN到形成$Mask^X R-CNN$的过程，阴影部分是Mask R-CNN: 前面与Mask R-CNN相同，图像输入给ConvNet，然后经过RPN和RoIAlign，而后是两个分支，box head和mask head 不再单独地学习掩码参数$w_{seg}$，而是将它相应的box检测参数$w_{det}$输入给权重迁移方程，从而获得一个类别的掩码权重 对于训练，迁移方程只需要集合A中类别的掩码，但是在测试阶段，它可以应用在$A \bigcup B$的所有类别上 使用一个互补的类别无关的全连接多层感知机（MLP）扩增了mask head，这其实与Mask R-CNN中的FCN mask head是一种互补，后面会解释这种互补。 一个细节：bounding box head包含两种检测权重：RoI分类权重$w_{cls}^c$和bounding box回归权重$w_{box}^c$。这篇文章的实验只使用一种权重：$w_{det}^c=w_{cls}^c~or~w_{det}^c=w_{box}^c$，或者使用两种权重的拼接：$w_{det}^c=[w_{cls}^c,w_{box}^c]$。 说一下个人的理解：Mask R-CNN中做的是检测到一个box，然后利用类别特定的分割权重去预测这个box里实例的掩码。这两部分是并行分支，检测权重和分割权重都是类别特定的，分别编码了各自的特征空间，并且在监督学习下进行。在缺少某些类别的掩码标记时，就学习不到这个类别的掩码权重，从而无法预测到这类实例的掩码。这个问题在这篇文章中得到了很好的解决：作者加入一个小的全连接神经网络（迁移方程）打通了两个并行分支，学习的是这两种特征空间编码之间的映射，尽管分割任务的学习缺少某些类别的掩码标记，可是通过已有的数据，只要学习到这种映射关系，那么自然地就可以得到那些没有掩码标记的类别的分割权重。 Training训练bounding box head使用的是在$A \bigcup B$ 所有类别上的box 检测损失，但是只使用A中类别的掩码损失训练mask head。提出两种方案： 逐阶段训练（stage-wise）：Mask R-CNN可以被看做是Faster R-CNN加一个掩码分支，因此分成检测训练和分割训练，第一个阶段只使用$A \bigcup B$中的bounding box标记训练Faster R-CNN；第二个阶段保持卷积特征和box head固定，训练mask head。这样的话，每一个类别特定的检测权重$w_{det}^c$可以被视为固定的类嵌入向量（class embedding vectors），它在训练的第二阶段不需要更新。 端到端的联合训练：在Mask R-CNN中已经证明多任务训练比单独训练每一个任务会有更好的性能，前面提到的逐阶段训练方式可能会导致性能低下。理论上也可以直接使用两个集合的box损失和集合A的掩码损失，进行反向传播，然而这也许会导致在集合A与B之间类别特定的检测权重会有差异性。因为对于一个类别$c$如果它属于A，只有$w_{det}^c$会接收到掩码损失经过权重迁移方程回传的梯度。也就是说，A中那些类别的检测权重既能接收到bounding box损失的梯度，又能接收到掩码损失的梯度；而B中那些类别的检测权重只能收到bounding box损失的梯度。但是目的是要在两个集合之间得到同样的检测权重，从而使得在集合A上训练的类别特定的分割权重$w_{seg}^c$很好地泛化到集合B。因此采取了一个简单的方法：在反向传播掩码损失时，阻止关于$w_{det}^c$的梯度，也就是说，回传掩码损失的梯度时，只计算预测掩码权重关于迁移方程参数$\theta$的梯度，而不计算关于$w_{det}^c$的梯度：$w_{seg}^c=\tau(StopGrad(w_{det}^c);\theta)$。 Extension: Fused FCN+MLP Mask Heads在Mask R-CNN中使用类别无关的FCN head，将其作为baseline。 Mask R-CNN中考虑了两种mask head：一是FCN head，使用全卷积网络预测MxM的掩码；二是MLP head，使用全连接层组成的多层感知机预测掩码，这点与DeepMask更相似。在Mask R-CNN中，FCN head具有更高的mask AP值，然而这两种设计可能是互补的。直觉上，MLP掩码预测也许更好地捕获了目标的“要点”（全局），而FCN掩码预测也许更好地捕获了目标的细节（局部），比如目标边界。基础这样的观察，提出一种改进，将类别无关的FCN和权重迁移方程以及类别无关的MLP掩码预测融合。 融合K类类别无关的掩码预测（1XMxM）和类别特定的掩码预测(KxMxM)时，这两个分数被加到最后的KxMxM的输出上。这里，类别无关的掩码预测会被平铺K次。然后，这个KxMxM的掩码分数经过一个sigmoid单元被变为每一类的掩码概率，再被缩放到实际的bounding box尺寸作为最终那个bounding box的实例掩码。 ExperimentsResults and Comparison of Our Full Method作者做了很多对比试验，最终选择使用迁移方程+MLP以及类别无关的FCN head融合的模型，其中迁移方程的实现使用两种权重的拼接，2层的MLP，LeakyReLU作为激活函数，整个网络采取端对端的训练。 Large-Scale Instance Segmentation$Mask^X R-CNN$模型的训练，使用VG数据集，包含108077张图像，超过7000类同义词集合，标记了bounding box，但是没有掩码。训练中，选择3000个最常见的同义词集合作为类别集合$A \bigcup B$用来实例分割，它覆盖了COCO中的80个类别。因为这两个数据集有大量的重叠，因此在VG上训练时只采用没有在COCOval2017中出现的那些，VG中剩下的图像作为验证集。把VG中与COCO重叠的80个类作为集合A，带有掩码标记；剩下的2920个类别作为集合B，只有bounding box。 作者最终训练出了一个可以分割3000类实例的$Mask^X R-CNN$，如题“Learning to Segment Every Thing”。 Reference1.Learning to Segment Every Thing var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask-R-CNN]]></title>
    <url>%2F2018%2F05%2F06%2FMask-R-CNN%2F</url>
    <content type="text"><![CDATA[这篇论文提出了一种概念简单，灵活且通用的目标实例分割框架，在检测出图像中目标的同时，生成每一个实例的掩码（mask）。对Faster R-CNN进行扩展，通过添加与已存在的bounding box回归平行的一个分支，预测目标掩码，因而称为Mask R-CNN。这种框架训练简单，容易应用到其他任务，比如目标检测，人体关键点检测。 Introduction实例分割的挑战性在于要求正确地检测出图像中的所有目标，同时精确地分割每一个实例。这其中包含两点内容： 目标检测：检测出目标的bounding box，并且给出所属类别； 语义分割（semantic segmentation）：分类每一个像素到一个固定集合，不用区分实例。 Mask R-CNN对Faster R-CNN进行了扩展，在Faster R-CNN分类和回归分支的基础上，添加了一个分支网络去预测每一个RoI的分割掩码，把这个分支称为掩码分支。掩码分支是应用在每一个RoI上的一个小的FCN，以像素到像素的方式（pixel-to-pixel）预测分割掩码。 Faster R-CNN在网络的输入和输出之间没有设计像素到像素的对齐。在how RoIPool文中提到：实际上，应用到目标上的核心操作执行的是粗略的空间量化特征提取。为了修正错位，本文提出了RoIAlign，可以保留准确的空间位置，这个改变使得掩码的准确率相对提高了10%到50%。解耦掩码和分类也至关重要，本文对每个类别独立地预测二值掩码，这样不会跨类别竞争，同时依赖于网络的RoI分类分支去预测类别。 模型在GPU上运行每帧200ms，在8 GPU的机器上训练COCO数据集花费了一到两天。最后，通过COCO关键点数据集上的人体姿态估计任务来展示框架的通用性。通过将每个关键点视为一位有效编码（one-hot），即所有关键点编码成一个序列，但只有一个是1，其余都是0。只需要很少的修改，Mask R-CNN可以应用于人体关键点检测。不需要额外的技巧，Mask R-CNN超过了COCO 2016人体关键点检测比赛的冠军，同时运行速度可达5FPS。 Related Work早前的实例分割方法受R-CNN有效性的推动，基于分割proposal，也就是先提取分割候选区，然后进行分类，分割先于分类的执行。本文的方法是同时预测掩码和类别，更加简单和灵活。 FCIS（fully convolutional instance segmentation）用全卷积预测一系列位置敏感的输出通道，这些通道同时处理目标分类，目标检测和掩码，这使系统速度变得更快。但FCIS在重叠实例上出现系统错误，并产生虚假边缘。 另一类方法受语义分割的推动，将同类别的像素划分到不同实例中，这是一种分割先行的策略。Mask R-CNN与其相反，基于实例先行的策略（segmentation-first strategy）。 Mask R-CNNMask R-CNN在Faster R-CNN上加了一个分支，因此有三个输出：目标类别、bounding box、目标掩码。但是掩码输出与其他输出不同，需要提取目标更精细的空间布局。Mask R-CNN中关键的部分是像素到像素的对齐，这在Fast/Faster R-CNN里是缺失的。 首先回归一下Faster R-CNN：它包含两个阶段，第一阶段使用RPN提取候选的目标bounding box，第二阶段本质上是Fast R-CNN，使用RoI pooling从候选区域中提取特征，实现分类并得到最终的bounding box。 Mask R-CNN也是两个阶段：第一阶段与Faster R-CNN相同，RPN提取候选目标bounding box；第二阶段，除了并行地预测类别和候选框偏移，还输出每一个RoI的二值掩码（binary mask）。 损失函数 多任务损失：$$L=L_{cls}+L_{box}+L_{mask}$$ 掩码分支对每一个感兴趣区域产生$Km^2$维的输出，K是类别数目，K个分辨率为m×m的二值掩码也就是针对每一个类别产生了一个掩码。 对每一个像素应用sigmoid，所以掩码损失就是平均二分类交叉熵损失。如果一个RoI对应的ground truth是第k类，那么计算掩码损失时，只考虑第k个掩码，其他类的掩码对损失没有贡献。 掩码损失的定义允许网络为每个类别独立预测二值掩码。使用专门的分类分支去预测类别标签，类别标签用来选择输出掩码。 掩码表达 掩码编码了输入目标的空间布局。掩码的空间结构，可以通过卷积产生的那种像素到像素的对应关系来提取。 使用FCN为每个RoI预测一个m×m的掩码。这允许掩码分支中的每个层显式的保持m×m的目标空间布局，而不会将其缩成缺少空间维度的向量表示。 像素到像素的对应需要RoI特征（它们本身就是小特征图）被很好地对齐，以准确地保留显式的像素空间对应关系。 RoI Align首先说明为什么需要对齐，下图中左边是ground truth，右边是对左边的完全模仿，需要保持位置和尺度都一致。平移同变性（translation equivariance）就是输入的改变要使输出也响应这种变化。 分类要求平移不变的表达，无论目标位置在图中如何改变，输出都是那个标签 实例分割要求同变性：具体的来说，就是平移了目标，就要平移掩码；缩放了目标就要缩放掩码 全卷积网络FCN具有平移同变性，而卷积神经网络中由于全连接层或者全局池化层，会导致平移不变。 在Faster R-CNN中，提取一张完整图像的feature map，输入RPN里提取proposal，在进行RoI pooling前，要根据RPN给出的proposal信息在基础网络提取出的整个feature map上找到每个proposal对应的那一块feature map，具体的做法是：根据RPN给出的边框回归坐标，除以尺度因子16，因为vgg16基础网络四次池化缩放了16倍。这里必然会造成坐标计算会出现浮点数，而Faster R-CNN里对这个是进行了舍入，这是一次对平移同变性的破坏；同样的问题出现在后面的RoI pooling中，因为要得到固定尺寸的输出，所以对RoI对应的那块feature map划分了网格，也会出现划分时，对宽高做除法出现浮点数，这里和前面一样，简单粗暴地进行了舍入操作，这是第二次对平移同变性的破坏。如下图，网格的划分是不均匀的： 总之，Faster R-CNN破坏了像素到像素之间的这种平移同变性。RoI Align就是要在RoI之前和之后保持这种平移同变性，避免对RoI边界和里面的网格做量化。如下图： 针对输入的feature map找到对应的RoI，是通过$x/16$而不是像Faster R-CNN中$[x/16]$，$[\cdot]$代表舍入操作。所以可以看到第一幅图中RoI并没有落在整数的坐标上。 对RoI划分为2x2的网格（根据输出要求），每个小的网格里采样4个点，使用双线性插值根据临近的网格点计算这4个点的值，最后再对每一个网格进行最大池化或平均池化得到最终2x2的输出。 network下图中，是两个不同的network head，左图是ResNet C4，右边是FPN主干，这两种结构上都添加了一个掩码分支，反卷积使用2x2的卷积核，stride为2；除了输出层是1x1的卷积，其他部分的卷积都是3x3的。 实现细节 掩码损失只定义在正的RoI上 输入图像被缩放到短边为800，每个图像采样N个RoI（ResNet的N=64，FPN的N=512），batch size = 2，正负样本的比例为1:3。 测试中对于ResNet架构，生成300个proposal，FPN则是1000。 将得分最高的100个检测框输入掩码分支，对每一个RoI预测出K个掩码，但是最终只根据分类分支的预测结果选择相应的那一个类别的掩码。 mxm的浮点数掩码输出随后被缩放到RoI尺寸，然后以0.5的阈值进行二值化。 Reference Mask R-cNN var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
        <tag>instance segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Feature Pyramid Networks for Object Detection]]></title>
    <url>%2F2018%2F05%2F06%2FFPN%2F</url>
    <content type="text"><![CDATA[这篇文章提出特征金字塔网络（FPN），将分辨率高语义性弱的浅层特征和分辨率低语义性强的深层特征融合，形成了多级金字塔，在金字塔每一级上独立检测目标。FPN不仅对多尺度的目标检测具有很好的效果，还可以应用到分割任务中。 Introduction识别不同尺度的目标是计算机视觉的一项基本挑战，下面介绍四种利用特征的形式： 图像金字塔：构建在图像金字塔上的特征金字塔（简称为特征化的图像金字塔），如图（a）。这种情况下，图像被采样为多种尺度，然后生成不同尺度的特征。DMP就是使用密集的尺度采样获得了不错的效果。这种方法被大量用在手工设计的特征中。优点是：每一级的特征语义信息都比较强，缺点是预测时间长。在图像金子塔上端到端地训练深度卷积神经网络是不切实际的，因为内存消耗大，所以即使要用，也只用在做预测时。 利用卷积神经网络提取特征，使用最后一层的特征做预测。卷积神经网络可以提取高级的语义表达，对尺度变化有更好的鲁棒性，所以可以使用单尺度特征，如图（b）。 使用不同层的特征进行预测，如图（c）。SSD使用卷积神经网络多个层的特征分别做预测，如同一个特征化的图像金字塔。SSD重复利用前向传播过程中计算好的不同层特征，所以几乎没有带来额外代价。但SSD没有利用到足够底层的特征，因为底层特征语义信息弱，但是底层特征分辨率高，对检测小目标很重要。 本文提出的FPN，将低分辨率语义信息强的浅层特征与高分辨率语义信息弱的深层特征进行组合，构建特征金字塔，在金字塔的每一级上分别做预测，如图（d）。 还有一种相似的结构，采用自顶向下的方法和跳跃连接（skip connection），目的是生成单个具有较好分辨率的高级特征图，然后在这个特征图上做预测，如下面上半部分的图。本文与其结构很接近，但是利用它形成一个金字塔，在金字塔的每一级独立地进行预测。 Feature Pyramid NetworksFPN接受一个具有任意尺寸的单尺度图像作为输入，在多个层级以全卷积的形式生成不同尺寸比例的特征图。FPN的构建涉及三个部分：自底向上的路径（Bottom-up pathway），自顶向下的路径（top-down pathway），横向连接（lateral connection）。 自底向上的路径自底向上的路径就是主干网络（backbone）的前向计算，产生不同尺度的feature map，尺度的比例为2，即每次下采样都是缩小2倍。自底向上的过程，空间分辨率降低，但是语义性增加。 这里把那些会输出同样尺寸feature map的层归为一个stage。每个stage都定义了一级金字塔，每个stage的最后一层特征被选取出来。本文的backbone是ResNet，选取的是每一个stage最后一个残差块的输出，将Conv2，Conv3，Conv4，Conv5的输出定义为{C2，C3，C4，C5}。这些输出相对于输入图像，stride为{4，8，16，32}，即分辨率缩小的倍数。为啥不用C1呢，因为维度太高了，内存消耗大。 自顶向下和横向连接FPN通过自顶向下的方式，从语义信息丰富的层出发，构建出分辨率更高的层，将这些层的特征与浅层的特征通过横向连接融合。如下图，对于空间分辨率较粗糙的深层特征，进行2倍的上采样（最近邻），相应的浅层特征使用1x1的卷积降维，之后与上采样的特征通过逐元素相加合并。合并后的特征又通过3x3的卷积生成最终的feature map，即{P2，P3，P4，P5}，这一步降低了上采样的混叠效应（aliasing effect）。 FPN可以更加详细的用下图表示： 金字塔中所有的层都共享分类器和回归器，就像传统的图像金字塔那样。固定特征的通道数为256，因此所有额外的层输出都为256通道。这些额外的层没有使用非线性。 ApplicationsFPN for RPN回顾一下Faster R-CNN中的RPN： 预先定义了一组不同尺度和高宽比的anchor，覆盖不同形状的目标。 在最后一个单尺度的共享卷积层输出的feature map上，使用一个3×3的滑动窗（卷积核），随后是两路1x1的卷积，分别用来分类是否为目标以及回归bounding box，这里将这一部分称为网络的头（head）。 将FPN应用在RPN上的要点： 将单尺度的feature map替换为FPN，为每一级金字塔附加一个head，也是3x3的卷积和两路1x1的卷积。 每一级都是单尺度的anchor。因为网络头需要在每层金字塔的feature map上的所有位置滑动，就没必要在特定的一级使用多尺度的anchor了。{P2，P3，P4，P5}上的anchor面积分别为{32x32，64x64，128x128，256x256，512x512}。 跟随Faster R-CNN，每一级金字塔使用多个高宽比{1:2，1:1，2:1}。所以金字塔上一共是15种anchor：5种尺度x3种高宽比. 与Faster R-CNN一样，正样本是与ground truth有着最高IoU的，以及与任何ground truth有着高于0.7的IoU的anchor，与所有ground truth的IoU都小于0.3的anchor作为负样本。注意：ground truth是与anchor相关的，因此也就与金字塔的某一级相关了。 RPN头在金字塔的所有层上共享参数，作者做了不共享参数的实验，发现性能相似，说明FPN中金字塔的所有层共享相似的语义级别。 FPN for Fast R-CNNFaster R-CNN另一模块是Fast R-CNN中基于region的检测器，它使用RoI Pooling提取特征，也是在一个单尺度feature map上进行预测。 应用FPN时，使用前面一节描述的RPN生成多个感兴趣区域RoI，根据RoI在原始图像中的尺寸，选择尺度最正确的feature map，去提取这个RoI的feature。 若RoI在原始图像中高宽为w和h，那么对应的特征层为 224是标准的ImageNet预训练尺寸，$k_0$是当RoI的尺寸为224x224时，对应的特征级。使用ResNet为基础网络的Faster R-CNN，使用C4作为后续部分的输入feature，所以$k_0=4$。直觉上，公式表示当RoI的尺度变小时，比如变为112，那么它应该被映射到一个分辨率更精细的层（$k=3$）。 Extensions: Segmentation ProposalsFPN还可以扩展到分割任务中，下面使用FPN生成分割的proposal。特征金字塔的构建方式与应用FPN到目标检测中一样，但是维度设为128，原来是256。以全卷积的形式，使用5x5的滑动窗在特征图上滑动，产生14x14的掩码和目标分数。浅橙色是相应的图像尺寸，深橙色是目标尺寸，可以看出掩码也是在金字塔的不同级独立预测的。 Reference1.Understanding Feature Pyramid Networks for object detection (FPN) var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
        <tag>instance segmentation</tag>
        <tag>feature pyramid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DSSD:Deconvolutional Single Shot Detector]]></title>
    <url>%2F2018%2F05%2F04%2FDSSD%2F</url>
    <content type="text"><![CDATA[这篇文章将额外的上下文信息引入到目标检测中，首先将ResNet-101和SSD组合，，然后利用反卷积层扩增这个组合的网络，为目标检测引入大尺度的上下文信息，提高准确率，特别是对小目标的检测。Deconvolutional single shot detector，因此简称DSSD。实验结果比R-FCN要好。 Introduction这篇文章最大的贡献就是为目标检测引入了额外的上下文信息，从而提高了准确率。首先将ResNet-101和SSD组合，然后利用反卷积层扩增这个组合的网络，为目标检测引入大尺度的上下文信息，提高准确率，特别是对小目标的检测。但是这个思想实现起来并不容易，所以作者添加了额外的学习转换的阶段，特别是反卷积中前馈连接的模块和新输出的模块，使得这个新方法有效。 YOLO先计算一个全局的feature map，然后使用全连接层在一组固定的区域上做预测。SSD对一幅图像提取不同尺度的feature map，对这些feature map每一个位置应用一组默认框，使用卷积做预测，得到了精确度和速度之间一种好的权衡。 当考虑为目标检测提高准确度时，很自然地会想到使用更好的特征提取网络，添加更多的上下文信息，特别是对小目标，提高bounding box预测的空间分辨率。SSD是基于VGG基础网络做特征提取的，但是ResNe-101目前更好。在目标检测研究之外，有一种编码-解码（encoder-decoder ）网络，其中网络中间层加入了瓶颈层（bottlenexk layer），用来对输入图像编码，后面再进行解码（就是卷积和反卷积），这样形成的宽-窄-宽的网络结构很像沙漏，FCN就是类似结构，本文就利用反卷积层实现了上下文信息的扩充。 Related Work主要的目标检测方法SPPnet，Fast R-CNN，Faster R-CNN，R-FCN和YOLO都是使用最顶层的卷积层去学习在不同尺度下检测目标，虽然很强大，但是它给单一层带来了巨大的负担，因为要为所有可能的目标尺度和形状建模。提高检测准确率的方法主要有几种： 组合不同层的特征，然后利用组合的特征做预测。但是这样做增加了内存消耗，并且降低了速度。 使用不同层的特征预测不同尺度的目标，因为不同层特征图的点有不同的感受野，很显然高层的特征有着大的感受野适合预测大尺寸目标，底层特征有着小的感受野适合预测小尺寸目标。SSD就是这么做的，不同层预测特定尺度的目标。但是缺点是浅层的特征图语义信息少，小目标检测的性能就不太好。通过使用反卷积层和跳跃连接（skip connection，像ResNet那样）可以对反卷积的feature map加入更多语义信息，有助于小目标检测。 本文提出使用一种编码-解码的沙漏式结构在做预测前传递上下文信息。反卷积层不仅解决了在卷积神经网络中随着层数加深，feature map分辨率缩减的问题，而且为预测引入了上下文信息。 DSSD model回归一下SSD，采用VGG作为基础网络，去掉尾部的一些层，加入了一些卷积层。下图是采用ResNet-101的SSD，如蓝色部分，SSD加入了5个额外的卷积层，ResNet-101中的conv3_x和conv5_x，以及这些添加的层，共7个层被用来为预定义的默认框预测分数和偏移量。预测的实现使用的是3x3xChannel的卷积核，一个卷积核针对一个类别分数，对于bounding box预测也是，一个卷积核针对一个坐标。 Using Residual 101 in place of VGG第一个改进就是使用ResNet-101取代VGG作为基础网络。在conv5_x block后添加了一些层，但只是添加这些层本身并不能改善结果，因此添加了一个额外的预测模块，使用conv3_x和添加的层做预测。 上图中蓝色的层就是卷积层，与SSD layer类似，是在基础网络之后添加的一些额外的层。 圆形为反卷积模块，这一模块一方面要对前一层的feature map做反卷积增加空间分辨率，另一方面还融合了卷积层的feature map 红色的层是反卷积层，是前面反卷积后的feature map和卷积层的feature map组合后的结果 Prediction module第二个改进就是添加额外的预测模块。在SSD中，目标方程直接应用在选择的feature map上，并且由于巨大的梯度，在conv4_3上使用了L2归一化。MS-CNN指出改进每个任务的子网络有助于提升准确度，因此跟随这个思想，为预测层添加了一个残差块。作者尝试了四种变体： (a) 原始SSD的方法，直接在feature map上做预测 (b) 带有跳跃连接的残差块 (c) 相比于(b)把恒等映射换成了1x1卷积 (d) 两个连续的残差块这四种中(c)的效果最好，其中PM：Prediction module。 Deconvolutional SSD为了引入更高级别的上下文信息，将检测移动到原始的SSD后面的反卷积层上，形成一个不对称的沙漏结构。这些反卷积层逐渐增大了feature map的分辨率。尽管沙漏模型(hourglass)在编码和解码阶段包含了对称的层，但是这里把解码层做的非常浅，原因有两个： 检测是视觉中的基本任务，可能会需要为下游任务提供信息，所以速度是一个重要的因素。 图像分类任务中，没有包含解码阶段的预训练模型。预训练的模型比起随机初始化的模型，可以使检测器精度更高，收敛更快。所以解码层只能随机初始化从头训练。 Deconvolution Module为了帮助网络浅层和反卷积层的feature map做信息融合，引入了反卷积模块，如DSSD结构中圆形部分，详细的结构如下图。做了以下修改： 批量归一化（BN）层加在每一个卷积层之后 使用学习好的反卷积层代替双线性插值的上采样 最后测试了不同的组合方法：逐元素求和（element-wise sum）和逐元素乘积（element-wise product）。实验表明后者带来了最好的准确率，见Prediction module部分的表。 上图是反卷积模块，将前一层的feature map反卷积，然后与卷积层的feature map做融合（逐元素相乘），得到具有更多上下文信息和更大分辨率的feature map用来做预测。需要注意的是：- 在反卷积模块中，所有卷积和反卷积操作，卷积个数都依赖于反卷积层的特征图的通道数- 卷积层和ReLu激活层之间有BN层## Training几乎和SSD训练方式一样：- 匹配默认框：对于每一个ground turth box，把它与IoU最好的一个默认框匹配，同时与IoU大于0.5的那些默认框匹配。未匹配的那些默认框，根据置信度损失选择高的那些，使得负样本和正样本比例为3:1。- 最小化联合定位损失（Smooth L1）和置信度损失（softmax）- 数据扩增：随机裁切原始图像，随机光度失真，随机翻转裁切的patch，最新版的SSD中数据扩增有助于小目标的检测，因此在DSSD中也采用了。对先验的默认框做了一些小的改变：使用K-means，以默认框的面积作为特征，对VOC中的数据进行聚类。从2开始，逐渐增加聚类中心个数，判断误差是否减小20%，最终收敛在7个聚类中心。因为SSD把输入图像缩放为正方形，但是大部分图像是比较宽的，所以也不奇怪bounding box是高高的。从表中可以看出，大部分box的高宽比落在1~3之间。因为在原始的SSD中，高宽比为2和3的默认框更加有用，因此加入一个1.6的比例，然后每一个预测层使用3种高宽比的默认框：（1.6，2.0，3.0）。# Experiments- Residual-101在ILSVRC CLS-LOC数据集上预训练。- 改变了conv5阶段的stride，从32改为16，提高feature map的分辨率。- 第一个卷积层stride为2，被改为了1。- Conv5阶段所有的卷积层卷积核尺寸大于1，增加膨胀（dilation），从1增加到2，修复由于降低stride引起的洞。 上图是PASCAL VOC2007上的测试结果，当输入的图像尺寸比较小的时候，把vgg换成resnet效果相似，但是提高输入图像的尺度的话，把vgg替换成resnet-101效果会更好，作者猜测对于Resnet这样非常深的网络，需要更大尺度的输入来让深层的feature map仍然保持较强的空间信息。更重要的是，DSSD比相应的SSD的效果要更好，DSSD对于那些具有特定背景信息的物体和小目标表现出了大的提升。 Inference Time为了加速预测过程，使用一些公式移除了BN层，使得速度提高了1.2~1.5倍，并且降低了内存消耗： 卷积层的输出通过减均值，再除以方差与$\varepsilon$的和，然后缩放并且根据训练过程学习的参数做平移：$$y = scale \left( \frac {(wx+b)-\mu} {\sqrt{var} + \varepsilon} \right) + shift$$ 卷积层的权重和偏置$$\hat w = scale \left(\frac w {\sqrt{var} + \varepsilon} \right), \hat b = scale \left(\frac {b-\mu} {\sqrt{var} + \varepsilon} \right) + shift$$ 移除了变量相关的BN层，第一个式子概括为$$y = \hat wx + \hat b$$ DSSD因为使用了更深的基础网络，预测模块和反卷积层，以及更多的默认框，所以速度自然比SSD慢。作者提到一种加速DSSD的方式是使用简单的双线性插值上采样替代反卷积层。 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLOv3:An Incremental Improvement]]></title>
    <url>%2F2018%2F05%2F03%2FYOLOv3%2F</url>
    <content type="text"><![CDATA[这篇文章发表于2018年，比YOLOv2网络更大，但是更精确。在320x320的分辨率下，YOLOv3处理一幅图需要22ms，mAP为28.2，与SSD一样精确但是快3倍。主要的创新点有3个：类别预测上不再使用softmax而是使用独立的logistic回归，能实现多标签预测；类似于FPN，实现多尺度预测，将不同层的特征做了融合；提出更好的基础网络，加入残差块。 The DealBounding Box 预测 跟随YOLO9000，依然使用维度聚类和anchor box。 YOLOv3使用logistic回归为每一个bounding box预测一个目标分数，如果一个先验的box比其他box和ground truth的IoU大，这个分数就为1；如果一个先验box并不是最好的，而是与真实值的IoU超过某些阈值，就忽略掉这个预测，这点跟随Faster R-CNN，本文阈值采用0.5。 与Faster R-CNN不同，YOLOv3为每一个ground truth只匹配一个先验box（其实就是anchor box）。Faster R-CNN中一个ground truth匹配到两种anchor：每一个ground truth匹配到与它IoU最高的anchor，但是同时也把那些与它IoU大于0.7的anchor也当做正的，因此每一个ground truth会有多个与其匹配的anchor。 如果一个先验box没有被匹配到任何ground truth，那么它对于坐标回归或者类别预测没有贡献，只对目标预测（objectness）有用，也就是预测是否为目标时才有贡献。 类别预测使用多标签预测，每个bounding box都预测可能包含的类别。但是并不使用softmax，而是使用独立的logistic回归分类器，训练中使用二分类的交叉熵损失。更加复杂的数据集Open Image Dataset有很多重叠标签，比如女性和人就是包含关系，使用softmax其实强加了一个假设：每一个box恰好有一类，但通常不是这样的。比如一个box里包含了一个“women”，这个box还应该有“person”这个标签，因此多标签方法能够更好的拟合数据。 softmax每一个框只得到一个标签，对应分数最高的那个，不适合做多标签分类 softmax可以被多个独立的logistic分类器替代，且准确率不会下降 多尺度预测123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108layer filters size input output 0 conv 32 3 x 3 / 1 416 x 416 x 3 -&gt; 416 x 416 x 32 0.299 BFL OPs 1 conv 64 3 x 3 / 2 416 x 416 x 32 -&gt; 208 x 208 x 64 1.595 BFL OPs 2 conv 32 1 x 1 / 1 208 x 208 x 64 -&gt; 208 x 208 x 32 0.177 BFL OPs 3 conv 64 3 x 3 / 1 208 x 208 x 32 -&gt; 208 x 208 x 64 1.595 BFL OPs 4 res 1 208 x 208 x 64 -&gt; 208 x 208 x 64 5 conv 128 3 x 3 / 2 208 x 208 x 64 -&gt; 104 x 104 x 128 1.595 BFL OPs 6 conv 64 1 x 1 / 1 104 x 104 x 128 -&gt; 104 x 104 x 64 0.177 BFL OPs 7 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 1.595 BFL OPs 8 res 5 104 x 104 x 128 -&gt; 104 x 104 x 128 9 conv 64 1 x 1 / 1 104 x 104 x 128 -&gt; 104 x 104 x 64 0.177 BFL OPs 10 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 1.595 BFL OPs 11 res 8 104 x 104 x 128 -&gt; 104 x 104 x 128 12 conv 256 3 x 3 / 2 104 x 104 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 13 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 14 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 15 res 12 52 x 52 x 256 -&gt; 52 x 52 x 256 16 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 17 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 18 res 15 52 x 52 x 256 -&gt; 52 x 52 x 256 19 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 20 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 21 res 18 52 x 52 x 256 -&gt; 52 x 52 x 256 22 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 23 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 24 res 21 52 x 52 x 256 -&gt; 52 x 52 x 256 25 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 26 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 27 res 24 52 x 52 x 256 -&gt; 52 x 52 x 256 28 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 29 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 30 res 27 52 x 52 x 256 -&gt; 52 x 52 x 256 31 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 32 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 33 res 30 52 x 52 x 256 -&gt; 52 x 52 x 256 34 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFL OPs 35 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFL OPs 36 res 33 52 x 52 x 256 -&gt; 52 x 52 x 256 37 conv 512 3 x 3 / 2 52 x 52 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 38 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 39 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 40 res 37 26 x 26 x 512 -&gt; 26 x 26 x 512 41 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 42 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 43 res 40 26 x 26 x 512 -&gt; 26 x 26 x 512 44 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 45 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 46 res 43 26 x 26 x 512 -&gt; 26 x 26 x 512 47 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 48 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 49 res 46 26 x 26 x 512 -&gt; 26 x 26 x 512 50 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 51 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 52 res 49 26 x 26 x 512 -&gt; 26 x 26 x 512 53 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 54 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 55 res 52 26 x 26 x 512 -&gt; 26 x 26 x 512 56 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 57 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 58 res 55 26 x 26 x 512 -&gt; 26 x 26 x 512 59 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFL OPs 60 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFL OPs 61 res 58 26 x 26 x 512 -&gt; 26 x 26 x 512 62 conv 1024 3 x 3 / 2 26 x 26 x 512 -&gt; 13 x 13 x1024 1.595 BFL OPs 63 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFL OPs 64 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFL OPs 65 res 62 13 x 13 x1024 -&gt; 13 x 13 x1024 66 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFL OPs 67 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 68 res 65 13 x 13 x1024 -&gt; 13 x 13 x1024 69 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 70 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 71 res 68 13 x 13 x1024 -&gt; 13 x 13 x1024 72 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 73 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 74 res 71 13 x 13 x1024 -&gt; 13 x 13 x1024 75 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 76 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 77 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 78 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 79 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs 80 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 81 conv 255 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 255 0.088 BFLOPs 82 detection # 75~81行是尺度1的预测，对13X13X1024的feature，使用255（3x(4+1+80)）个卷积核对每个位置去做预测 83 route 79 #取79行卷积对应的feature map 13 x 13 x 512 84 conv 256 1 x 1 / 1 13 x 13 x 512 -&gt; 13 x 13 x 256 0.044 BFLOPs 85 upsample 2x 13 x 13 x 256 -&gt; 26 x 26 x 256 86 route 85 61 #取85和61行的feature map做拼接得到下面 26 x 26 x 768的feature map 87 conv 256 1 x 1 / 1 26 x 26 x 768 -&gt; 26 x 26 x 256 0.266 BFLOPs 88 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 89 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs 90 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 91 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs 92 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 93 conv 255 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 255 0.177 BFLOPs 94 detection #尺度2的预测是在尺度1的feature map基础上做的 95 route 91 96 conv 128 1 x 1 / 1 26 x 26 x 256 -&gt; 26 x 26 x 128 0.044 BFLOPs 97 upsample 2x 26 x 26 x 128 -&gt; 52 x 52 x 128 98 route 97 36 99 conv 128 1 x 1 / 1 52 x 52 x 384 -&gt; 52 x 52 x 128 0.266 BFLOPs 100 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 101 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs 102 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 103 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs 104 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 105 conv 255 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 255 0.353 BFLOPs 106 detection #尺度3的预测是在尺度2的feature map基础上做的 YOLOv3检测网络输入为416x416，5倍下采样后，最后一层卷积的输出是13x13。使用了一种与特征金字塔网络（FPN）相似的概念，YOLOv3在3个不同的尺度进行预测： 尺度1：在基础的特征提取网络上添加了几个卷积层，在最后一个卷积层输出的13x13的feature map上预测一个3-d的tensor编码bounding box，目标和类别的预测。实验中，对于COCO数据集，每一个尺度预测3个box，所以tensor的尺寸就是NxNx[3x(4+1+80)]，feature map大小为NxN，每个box有4个坐标，1个目标分数，80个类别分数。 尺度2：提取尺度1的13x13的feature map，上采样2倍；然后在网络的浅层中取一个26x26的feature map，并把它与上采样后的feature map进行拼接（concatenation）。这个方法能够获得上采样的feature map上更加有意义的语义信息和浅层feature map上细粒度（finer-grained）的信息。添加几个卷积层去处理这个组合后的26x26x768的feature map，最终预测一个相似的tensor，但比尺度1大2倍。 尺度3：在尺度2的26x26的feature map基础上，上采样2倍得到52x52的feature map；取浅层的52x52的feature map做拼接，在组合后的52x52的feature map上做预测。 使用K-means聚类，只选择了9个聚类和3个尺度，然后按照聚类中心的大小平均划分3种尺度，最后3个尺度对应于尺度1的预测，以此类推。在COCO数据集上，9个聚类中心为：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。 特征提取设计了一种新的特征提取网络，这个网络是YOLOv2中Darknet-19和残差网络的混合。特征提取网络连续使用3x3和1x1卷积层，但是加入了快捷连接（shortcut connection），所以明显更大一些。因为有53个卷积层，因此称其为DarkNet-53。精确率比ResNet-101更好，与ResNet-152相当，但是更高效。 训练在完整的图像上训练，没有使用难负样本挖掘或者其他操作。使用多尺度训练，数据增强，批量归一化。 How We Do在COCO数据集的mAP度量标准下，YOLOv3与SSD的变体DSSD相当，但仍然落后于RetinaNet，但速度更快。但是在旧的度量标准下，即IoU为0.5时的mAP，YOLOv3与RetinaNet相当，远超DSSD。但是当IoU增加时，YOLOv3的性能明显降低，说明YOLOv3其实是在努力获得与目标对齐的box。 过去，YOLO在尽力解决小目标问题，现在这一情况有了逆转。当使用多标签预测时，可以看到YOLOv3，在小目标上有着相对高的AP值（APs），然而在中等和大目标上性能相对较差。 Things We Tried That Didn’t Work anchor box偏移量预测：降低了模型的稳定性 使用线性激活取代逻辑回归激活函数直接预测x，y偏移量：mAP下降了几个点 Focal loss：降低了2个点的mAP，可能是因为YOLOv3对Focal loss尽力解决的问题已经足够鲁棒。因为YOLOv3有独立的目标分数预测和条件类概率预测。 双IoU阈值和：Faster R-CNN在训练中使用了2个IoU阈值，anchor box与ground truth的IoU超过0.7的也作为正样本，而[0.3,0.7]的被忽略掉。小于0.3的作为负样本。尝试在YOLOv3中这样做，但是结果并不好。 Reference 论文原文 目标检测|YOLOv2原理与实现(附YOLOv3) YOLOv3项目主页 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO9000:Better, Faster, Stronger]]></title>
    <url>%2F2018%2F05%2F02%2FYOLOv2%2F</url>
    <content type="text"><![CDATA[这篇文章发表在2016年，提出了YOLO的第二个版本。YOLOv2在67FPS的检测速度下，可以在VOC2007上达到76.8的mAP。在40FPS的速度下，有78.6的mAP，比使用ResNet的Faster R-CNN和SSD更好。YOLO9000可以检测超过9000种目标。 Introduction当前目标检测数据集比起图像分类数据集而言，目标种类很少。因此本文利用已有的大量图像分类数据并扩展到目标检测系统中。将不同的数据集组合在一起，提出了一种联合训练算法，允许在检测和分类数据上训练目标检测器，利用检测图像学习精确地定位目标，同时利用分类图像扩展模型对多类别的识别能力。使用这种方法训练YOLO9000，可以获得一个可以检测9000种不同类别目标的实时检测器。 首先，在YOLO的基础上改进产生YOLOv2，然后使用组合的数据集和联合训练算法来训练ImageNet图像分类数据和COCO目标检测数据得到YOLO9000。 Better相比于最先进的目标检测系统而言，YOLO有一些缺点：与Fast R-CNN相比，YOLO产生大量的定位误差；与基于region proposal的方法相比，YOLO有较低的召回率。所以，本文的改进也主要是在这两方面。 YOLOv2的目标是保持很快的检测速度，达到更高的准确度。将过去工作中的思想与本文的新概念融合，以提高YOLO的性能。 Batch Normalization：在YOLO的所有卷积层增加BN，mAP提升了2%。使用BN，可以去掉dropout而不过拟合。 高分辨率分类：原始的YOLO训练的分类网络输入是224x224，并且为训练检测任务将分辨率增加到448。这意味着网络必须同时切换到学习目标检测并调整到新的输入分辨率上。 YOLOv2中，首先在ImageNet上，对分类网络在448x448的分辨率下进行10个epoch的微调。相当于给了网络一些时间去调整卷积核，使得在高分辨率输入下工作得更好。 然后在检测数据集上微调网络。高分辨率的分类网络使得mAP有近4%的提升。 使用Anchor Box的卷积：YOLO中使用全连接层直接在顶层的特征图上预测bounding box的坐标。但是Faster R-CNN使用RPN，在特征图的每一个位置预测anchor box的偏移量和置信度，预测偏移量简化了问题，并且使得网络更容易学习。本文移除掉YOLO中的全连接层，使用anchor box去预测bounding box。 首先移除掉一个池化层，提高卷积层的输出分辨率，然后缩小网络将输入尺寸改为416而不是448×448，目的是希望得到的特征图中位置为奇数，这样就只有一个中心单元格。那些大目标倾向于占据图像的中心，所以在中心只有一个位置能很好预测这些目标。YOLO的卷积层下采样32倍，416的输入图像最终得到13x13的feature map。 因为YOLO是由每个cell来负责预测类别，每个cell对应的2个bounding box 负责预测坐标 。YOLOv2中，不再让类别的预测与每个cell（空间位置）绑定一起，而是让全部放到anchor box中。 跟随YOLO，在目标预测上，仍然预测ground truth和候选框的IoU，表示是否存在目标；类别预测上，预测类别概率，即存在目标的前提下目标的类别。 使用anchor box准确度上有一点降低。YOLO对每幅图只预测98个box，但是使用anchor box后，每幅图预测超过1000个box。 维度聚类：使用anchor box时遇到两个问题，一是box的尺寸是手工选取的，网络可以学习调整box，但假如选择更好的先验box，那么会使网络更容易预测出好的结果。 取代手工选取anchor box尺寸，使用k-means在训练集的bounding box上进行聚类，自动地找到好的先验box。这里并不使用欧氏距离，因为这会导致较大的box比较小的box产生更大的误差。希望得到的先验box能提高IoU分数。因此定义距离度量为：$d(box,centroid) = 1 - IoU(box,centroid)$。 直接位置预测：使用anchor box时遇到的第二个问题就是模型不稳定，特别是在早期的迭代时。不稳定性主要来源于box的(x,y)坐标。在RPN中，网络预测值$t_x$和$t_y$与中心坐标计算为$x=t_xw_a-x_a$，$y=t_yh_a-y_a$。$t_x= 1$的预测将使框向右移动anchor box的宽度， $t_x=-1$的预测将使其向左移动相同的量。 这种公式是不受约束的，因此不管预测box的位置，任何anchor box可以在图像中的任何点结束。使用随机初始化模型需要很长时间才能稳定到预测出可感知的偏移。 本文遵循YOLO的方法并预测相对于网格单元位置的bounding box坐标。这将ground truth限制在0和1之间。我们使用逻辑激活函数来约束网络的预测落在该范围内。 网络在feature map的每个网格单元上预测5个bounding box，每个box包含5个坐标：$t_x,t_y,t_w,t_h,t_o$。假如网格相对图像左上角偏移$(c_x,c_y)$，先验的bounding box宽高为$p_w,p_h$，则预测为： $b_x = \sigma(t_x)+c_x$ $b_y = \sigma(t_y)+c_y$ $b_w = p_we^{t_w}$ $b_h = p_he^{t_h}$ $P_r(object)*IoU(b,object) = \sigma(t_o)$使用维度聚类和直接预测bounding box中心位置，比Faster R-CNN中的anchor box方法提升了近5%的mAP。 细粒度特征（Fine-Grained Features）：改进的YOLO在13x13的feature map上预测，尽管这对大目标是足够的，但是对于定位更小的目标需要细粒度的特征。本文添加一个传递层（passthrough layer），与ResNet的恒等映射相似，传递层通过将相邻的特征图堆叠到不同的通道，而不是空间位置，将较高分辨率和较低分辨率的feature map相连。将26×26×512特征映射转换为13×13×2048特征映射，这样可以直接与原始的特征拼接。这一点改进获得了1%的性能提升。 多尺度训练：由于使用anchor box，因此改变输入的分辨率为416。本文并不固定输入图像的尺寸，而是在每几次迭代中改变网络的输入图像尺寸。每10个batch，网络随机选择一个新的输入图像尺寸，由于下采样的因子是32，所以输入尺寸都是32的倍数：{320，352，…，608}。这个策略迫使网络学习在不同的输入分辨率下预测好的结果，意味着相同的网络可以实现不同分辨率的检测。在288的分辨率下，YOLOv2的速度是90FPS，mAP几乎和Faster R-CNN一样好，高分辨率下，在VOC2007数据集上YOLOv2达到78.6的mAP，仍然保证实时。 Faster由于VGG16还是过于复杂，YOLO框架中采用基于GoogLeNet的自定义网络，准确率略低于使用VGG16，但计算量更少。 本文设计了一个新的分类模型Darknet-19，共19个卷积层和5个最大池化层： 与VGG模型相似，使用3x3的卷积核，并且在每个池化步骤后将通道数量加倍。 跟随Network in Network(NIN)，使用全局平均池化做预测，用1x1的卷积在3x3的卷积之间压缩特征。 使用批量归一化来稳定训练，加速收敛，正则化模型。 训练分类任务：在ImageNet的1000类数据集上训练160个epoch，输入的分辨率为224。然后在更大的尺寸448上，微调网络，训练10个epoch。 训练检测任务：修改分类网络，将最后的卷积层移除掉，添加3个3x3的卷积层，每一层有1024个卷积核，跟随1个1x1的卷积层，卷积核数量与检测任务需要的输出相对应。对于VOC数据集来说，预测包含5个bounding box，每一个包含5个坐标和20个类别，所以一共有125个卷积核(5x(20+5))。除此之外，还添加了一个传递层，从最后的3x3x512的层连接到倒数第二层，以便于模型可以使用细粒度特征。 Stronger训练期间，混合了检测和分类数据集。当网络看到标记为检测的图像时，可以反向传播整个YOLOv2的损失，当看到一个分类图像时，只传播网络结构中特定的分类部分损失。 这种方法带来了一些挑战，检测数据集只有常见目标和标签，而分类数据集有着更广泛的标签。ImageNet有着100多种狗，如果想要在两种任务的数据集上联合训练，需要一种连贯的方式去合并标签。 分类任务中一般使用softmax层，计算所有可能类别的概率，这假设了类别之间是互斥的。因此，需要一个多标签的模型来综合数据集，使类别之间不相互包含。 层次分类：ImageNet的标签是从WordNet中提取的，WordNet是一个语言数据库，用于构建概念及其关系。WordNet被构造为有向图，但是本文并不使用完整的图结构，而是根据ImageNet中的概念构建层次树来简化问题，最终的结果是WordTree。使用WordTree实现分类，预测在每个节点的条件概率，也就是预测给定同义词集合的每个下位词的概率。例如，在“terrier”节点，预测：$P_r(Norfolk~terrier|terrier)$，$P_r(Yorkshire~terrier|terrier)$… 如果想要计算一个特定节点的绝对概率，只需遵循通过树到达根节点的路径，并乘以条件概率。因此，如果我们想知道图片是否是诺福克梗犬，只需要计算：$$P_r(Norfolk~terrier) = P_r(Norfolk~terrier|terrier) … \times P_r(animal|physical~object)$$ 对于分类任务，假设图像中包含目标，因此$P_r(physical~object) = 1$。 Darknet-19模型的训练使用1000类ImageNet数据集构建的WordTree。为了构建WordTree1k，添加中间节点将标签空间从1000扩展到1369。在训练期间，沿着树传播ground truth标签，以便如果图像被标记为“诺福克梗犬”，它也被标记为“狗”和“哺乳动物”等。为了计算条件概率，模型预测了1369个值的向量，并且计算所有同义词集合的softmax，如下图： 以这种方式实现分类具有一些益处。在新的或未知的目标类别上性能降低微小。例如，如果网络看到一只狗的图片，但不确定它是什么类型的狗，它仍然会预测具有高置信度的“狗”，但在下义词（更为具体的内容，比如某种类型的狗）上具有较低的置信度。 这个公式也用于检测。不是假设每个图像都有一个目标，而是使用YOLOv2的目标预测器给出$P_r(physical~object)$的值。检测器预测bounding box和概率数。遍历树，在每个分裂中采用最高置信度路径，直到达到某个阈值，预测出目标类别。 联合分类和检测要训练一个极大尺度的检测器，因此使用COCO数据集和来自完整ImageNet的前9000类创建组合数据集。还需要评估本文的方法，所以添加ImageNet还没有包括的类别。WordTree数据集相应的具有9418个类。ImageNet是一个更大的数据集，因此对COCO进行过采样来平衡数据集，使ImageNet与COCO数量只有4：1的倍数。使用这个数据集训练YOLO9000，使用基本的YOLOv2架构，但只有3个先验box，而不是5，以限制输出大小。 当网络看到检测图像时，反向传播正常的损失。对于分类损失，只在标签的相应层级或高于标签的相应层级反向传播误差。例如，如果标签是“狗”，会在树中“德国牧羊犬”和“金毛猎犬”的预测中分配误差，因为没有这些信息。 看到分类图像时，只反向传播分类损失，为了实现这一点，只找到对这个类别预测的概率最高的bounding box，并且只计算它的预测树上的损失。这里假设预测框与ground truth的IoU至少为0.3，并且基于该假设反向传播目标类别损失。 Reference 论文原文 YOLOv2 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks]]></title>
    <url>%2F2018%2F04%2F25%2Fmtcnn%2F</url>
    <content type="text"><![CDATA[提出一种深度级联的多任务框架，利用检测和对齐的固有相关性去增强它们的性能。实际中，利用有三阶段精细设计的深度卷机网络的级联结构，由粗到精地检测和对齐人脸。 Introduction人脸识别中视觉的变化，比如遮挡，姿态变化和极端的光照条件，会给人脸检测和对齐带来巨大挑战。AdaBoost和Haar-Like特征训练的级联分类器虽然可以达到比较高的效率，但是大量研究表明这类检测器在人脸有着较大的视觉变化时，检测精度会大大降低。DPM（deformable part models）用于人脸检测也可以达到非常好的性能，然而计算代价太大，并且在训练时可能要求大量的标注。 人脸对齐领域的方法可以大致划分为两类：基于回归的方法和模板匹配方法。过去大部分的人脸检测和对齐方法都忽视了这两种任务之间的固有联系。 另一方面，挖掘难样本对于增强检测器的性能是至关重要的。传统的方法都是离线模式去挖掘，对于人脸检测任务来说，需要一种在线的难阳本挖掘方法，这样可以自动适应当前的训练状态。 本文中，通过多任务学习使用统一的级联CNNs集成这两种任务。提出的CNNs包含三个阶段：第一阶段，使用浅层的CNN(fast Proposal Network (P-Net))生成候选窗口；第二阶段，通过更加复杂的CNN(Refinement Network (R-Net))去精炼窗口，拒绝掉大量的非人脸的窗口；第三阶段，使用更加强大的CNN(Output Network (O-Net))去再次精修结果并输出5个landmark位置。 贡献： 提出一种级联的CNNs框架做人脸检测和对齐，设计了一种轻量的CNNs结构用于实时性能。 提出一种在线难样本挖掘（online hard sample mining）方法去提高性能。 ApproachOverall Framework首先将给定图像缩放到不同的尺度建立图像金字塔，这将是后面三阶段级联框架的输入。 第一阶段：采用全卷积神经网络，即P-Net，去获得候选窗体和bounding box回归向量。同时，候选窗体根据估计的bounding box向量进行校准。然后，利用NMS方法合并高度重叠的候选框。 第二阶段：所有的候选框被输入R-Net，进一步拒绝掉大量的错误候选框，同样使用bounding box回归校正候选框，并实施NMS。 第三阶段：和第二阶段相似，但是目的是利用更多的监督去判断人脸区域，并输出5个landmark位置。 CNN Architectures多个CNN被用于人脸检测，但其性能可能受到以下情况的限制： 卷积层中的卷积核缺乏多样性，限制他们的识别能力； 对比多类识别检测和分类任务，人脸检测是一个二分类问题，因此每一层需要的卷积核较少。所以本文减少卷积核数量，并将5*5的卷积核大小改为3*3的，以此在增加深度来提高性能的同时减少计算。 Training 本算法从三个方面对CNN检测器进行训练：人脸分类、bounding box回归、landmark定位（关键点定位）。 人脸分类： 二分类问题，使用交叉熵损失函数： bounding box 回归： 回归问题，使用欧氏距离计算的损失函数： landmark 定位： 回归问题，使用欧氏距离计算损失函数： 在每个CNN中实现的是不同的任务，所以在学习过程中有几种类型的训练图像：人脸，非人脸，部分对齐的人脸。在这种场合下，上面的式子不能使用，比如，对于背景区域的样本，只需要计算检测损失，其他两种损失设置为0，所以使用一些系数，总体的学习目标表示为： Online Hard sample mining： 每一个mini-batch中，对从所有的样本前向运算得到的损失排序，选择前70%作为难样本。在反向传播中，只计算来自于这些难样本的梯度。这意味着在训练中忽视掉那些对增强检测器性能帮助甚小的简单样本。 Experiments在训练中有四种数据： 负样本：与任何ground truth faces的IoU低于0.3的。 正样本：与一个ground truth face的IoU高于0.65的。 Part faces：与一个ground truth face的IoU在0.4~0.65之间的。 Landmark faces：标定了5个landmark的。负样本和正样本用于人脸分类，正样本和part faces用于bounding box回归，landmark faces用于landmark定位。上面的样本比例：3：1：1：2。 数据的收集方法如下： P-Net：随机地从WIDER FACE数据集中裁切一些图像块，收集正样本，负样本，part人脸。从CelebA数据库裁切人脸作为landmark人脸。 R-Net：使用框架的第一阶段在WIDER FACE中检测人脸，收集正样本，负样本和part人脸，同时从CelebA中检测landmark人脸。 O-Net：与R-Net相似的方法收集数据。但是是使用前两个阶段去检测人脸和收集数据。 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
        <tag>face detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-FCN:Object Detection via Region-based Fully Convolutional Networks]]></title>
    <url>%2F2018%2F04%2F15%2FR-FCN%2F</url>
    <content type="text"><![CDATA[这篇文章发表在CVPR2016上，提出位置敏感分数图（position-sensitive score map）去权衡图像分类中的平移不变性和目标检测中的平移变换性这种两难的境地。 Introduction比较流行的关于目标检测的深度网络可以通过ROI pooling layer分成两个子网络： a shared, “fully convolutional” subnetwork independent of RoIs.（独立于ROI的共享的、全卷积的子网络） an RoI-wise subnetwork that does not share computation.（不共享计算的ROI-wise（作用于各自RoI的）子网络） 工程上的图像分类结构（如Alexnet和VGG Nets）被设计为两个子网络——1个卷积子网络后面跟随1个空间池化层和多个全连接层。因此，图像分类网络中最后的空间池化层自然变成了目标检测网络中的RoI池化层。 目前最好的图像分类网络，例如残差网络（ResNets）和GoogleNets都是用fully convolutional设计的。在目标检测中都使用卷积层去构建一个共享的卷积网络是一件十分自然的事。但是这种简单的方法分类效果很好，检测效果却很差。为了解决这个问题，ResNet文中Faster R-CNN的RoI pooling layer被不自然地插入在两个卷积层之间，这创建出一个更深的RoI-wise子网络，改善了精度，但是各个RoI计算不共享所以速度慢。 图像分类任务更加喜欢平移不变性，图像中目标的移动不应该被区分看待。因此具有平移不变性的全卷积网络结构在分类上表现更佳。另一方面，目标检测需要一定程度上具有平移变化的定位表达。在一个候选框中目标的平移应该产生有意义的响应，这种响应可以描述候选框与目标重合的好坏程度。 这篇文章提出了基于区域的全卷积网络 (R-FCN)，为给FCN引入平移变化，用一组专门的卷积层构建位置敏感分数图 (position-sensitive score maps)。每个位置敏感的score map编码感兴趣区域的相对空间位置信息。在FCN顶部增加1个位置敏感的RoI pooling layer来监管这些分数图，这个层后面没有卷积或全连接层。 R-FCNOverview网络采用ResNet-101作为基础网络产生feature map，之后使用Faster R-CNN中的RPN提取候选区域，RPN本身就是全卷积网络，RPN和R-FCN之间共享特征。在R-FCN中，所有可以学习的权重层都是卷积层，并且是在整幅图上计算的。整个流程可以简单概括为：输入图像-&gt;基础网络提取feature map-&gt;专门的卷积层提取位置敏感的score map-&gt;位置敏感的RoI pooling-&gt;对结果投票（取平均）进而得到预测结果 首先输入图像经过基础网络得到整张图像的feature map，然后RPN给出RoIs。 得到feature map后，再经过最后一个卷积层，使用$k^2(C+1)$个卷积核为每一个类别产生$k^2$个position-sensitive score maps，所以对于C类的目标再加上一个背景类，输出层的通道数为：$k^2(C+1)$。 $k^2$个score map与k×k个空间网格相对应。网格描述了相对空间位置，例如k=3，则对应9个位置（左上，左中，…右下）。 R-FCN的最后是位置敏感的RoI池化层（position-sensitive RoI pooling layer）。这一层聚合上一个卷积层的输出，并且为每一个RoI生成得分，根据得分对这些感兴趣区域分类。 位置敏感的RoI池化是选择性池化：k=3，对每个类别生成了9个score map，每一个score map的通道是21。 将RoI对应的9x21通道的score map取出，每一个score map都对应了一个空间位置，共9个空间位置。但这里并不是对RoI对应的每一个score map进行池化，然后将结果组合，而是将RoI对应的那块score map划分为3x3的网格。（Fast R-CNN里的RoI pooling也是将feature map划分为网格） 从每个score map对应的空间位置上只取出一个小网格进行池化（文中使用的是平均池化，但最大池化也没问题）。例如下图中最后得到了3x3的feature map，其左上角（其实只是一个数值）黄色的部分就是对前一层第一个黄色的score map中RoI里3x3的网格中的左上角网格进行池化的结果。所以最后的feature map其实每一个位置的元素都是从前一层负责这个空间位置的score map里该空间位置的网格池化得来的。对所有的score map这样操作，再聚合起来就得到最后的k×kx(C+1)的feature map。 所有颜色的小方块就是一些位置敏感的分数，然后这些分数在RoI上投票（文中是取平均）得到一个C+1维的响应，之后使用softmax计算类别的得分判断是哪一类目标。 上述就完成了目标分类，回归位置采用的是相似的做法，在生成score map的最后一个卷积层处，也同时进行了另一路卷积，生成$4k^2$的feature map用于bounding box回归；然后针对每一个RoI使用位置敏感的RoI pooling通过平均池化产生4-d的向量。这里实际上是类别无关的位置回归，实际上也可以通过产生$4k^2(C+1)$的feature map得到类别特定的位置回归。 下图是可视化结果：如果一个候选框精确地与目标重合，$k^2$个bin中大多数都会被强烈地激活，投票分数就会高；相反，如果候选框没有正确地与目标重合，则RoI中的$k^2$个bin里，有一部分则不会被激活，投票分数就会低。 Backbone architecture本文中R-FCN是基于ResNet-101的，100个卷积层后跟随全局平均池化，然后加一个1000类的全连接层。 改动：移除了average pooling层和全连接层，只是用卷积层去计算feature map。ResNet-101中最后的卷积块是2048-d的，为了降维附加了一个随机初始化的1024-d的1×1的卷积层。然后使用k^2(C+1)通道的卷积层生成score map。 Reference 论文原文 R-FCN：基于区域的全卷积网络来检测物体 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD:Single Shot MultiBox Detector]]></title>
    <url>%2F2018%2F04%2F15%2FSSD%2F</url>
    <content type="text"><![CDATA[这篇文章发表在ECCV2016上，在既保证速度，又要保证精度的情况下，提出了SSD。使用single deep neural network，便于训练与优化，同时提高检测速度。SSD将bounding box的输出空间离散化为一组默认框，这些默认框在feature map每个位置有不同的高宽比和尺度。在预测时，网络对每一个默认框中存在的目标生成类别分数，并且调整边界框以更好地匹配目标形状。除此之外，网络对不同分辨率的feature map进行组合，以处理各种尺寸的目标。 Introduction目前流行的state-of-art的检测系统大致都是如下步骤，先生成一些假设的bounding boxes，然后在这些bounding boxes中提取特征，之后再经过一个分类器，来判断里面是不是物体，是什么物体。这些方法计算时间长，即使有的速度提升了，却是以牺牲检测精度来换取时间的。 这篇文章提出了第一个在bounding box预测上不需要重新采样像素和特征的基于深度网络的目标检测器。这使得速度和精度都有了改善。速度上的根本改进是因为消除了bounding box proposal和后续的像素或特征重采样阶段。 改进包括：使用小的卷积核去预测目标类别和边界框位置的偏移，使用单独的预测器（filter）解决不同高宽比的检测，并且将这些filter应用到网络后期的feature map上去实现多尺度的检测。 SSD的贡献： 比YOLO更快，更准确；和Faster R-CNN一样准确。 核心是使用小卷积核来预测特征图上固定的一组默认边界框的类别分数和位置偏移。 为了实现高检测精度，从不同尺度的特征图产生不同尺度的预测，并且得到不同高宽比的预测。 这些设计实现了简单的端到端训练和高精度的检测，即使输入相对低分辨率图像，也能在速度和精度之间取得更好的权衡。 The Single Shot Detector (SSD)以卷积形式，在不同尺度（例如8×8和4×4）的特征图中的每个位置上评估一组不同高宽比的默认框(default box)。 对于每个默认框，预测位置偏移和目标类别分数（c1，c2，...，cp）。 在训练时，首先将这些默认框匹配到真实标签框。例如，两个蓝色虚线默认框匹配到猫，一个红色虚线框匹配到狗，这些框为正，其余为负。模型损失是位置损失（例如Smooth L1）和置信度损失之间的加权和。 feature map尺寸例如是8×8或者4×4的 default box就是每一个位置上，一系列固定大小的box，即图中虚线所形成的一系列 boxes。同一个feature map上default box的aspect ratio不同，不同的feature map上default box有着不同的scale。 ModelSSD产生一组检测框和框中目标类别分数，接着使用非极大化抑制产生最终检测。本文在基础网络后添加辅助结构，产生了具有以下主要特征的检测： Multi-scale feature maps for detection：将卷积特征层添加到截断的基础网络的末尾。这些层尺寸逐渐减小，并且允许多个尺度的预测。检测的卷积模型对于每个特征层是不同的。 Convolutional predictors for detection：每个添加的特征层（或基础网络结构中的现有特征层）可以使用一组卷积核产生固定的预测集合。对于具有p个通道的大小为m×n的特征图，使用3×3×p卷积核卷积操作，产生类别分数或相对于默认框的坐标偏移。在应用卷积核运算的m×n个位置的每一处，产生一个输出值。bounding box偏移输出值是相对于默认框的，默认框位置则相对于特征图。 Default boxes and aspect ratios：对于网络顶层的多个feature map，将一组默认框与feature map每一个位置关联，每一个默认框的位置对于其在feature map中的位置是固定的。具体来说，对于在给定位置的k个框中每个框，计算c个类别分数和相对于原始默认框的4个偏移量。所以在特征图中的每个位置需要总共（c+4）k个卷积核，对于m×n特征图产生（c+4）kmn个输出。这里的默认框类似于Faster R-CNN中使用的anchor boxes，Faster R-CNN将anchor只用在基础网络的最后一个卷积层的输出上，但本文将其应用在不同分辨率的feature map中。在多个特征图中使用不同的默认框形状，可以有效地离散可能的输出框形状空间。 SSD与YOLO网络结构图对比如下： SSD模型在基础网络的末尾添加了几个特征层，这些层预测了对于不同尺度和高宽比的默认框的偏移及其相关置信度。YOLO是用全连接层预测结果，而SSD是用卷积层预测结果。 从上图中可以看出，SSD使用VGG-16作为basenet，conv4_3输出的feature map也被进行卷积操作预测结果，在VGG16的conv5_3后，用卷积层替代了全连接层，然后是SSD增加的额外的feature layer。最后网络输出了8732个候选框信息（38x38x4+19x19x6+10x10x6+5x5x6+3x3x4+1x4）。不同层级，不同尺度的feature map用作预测，所有的这些预测又经过了非极大值抑制得到最终结果。 Training 在训练时，SSD与那些用region proposals + pooling方法的区别是：SSD训练图像中的ground truth需要被匹配到default boxes上。如上面的图中，狗的ground truth是红色的bounding boxes，在进行标注的时候，要将红色的 ground truth box匹配到图（c）中一系列固定输出的boxes中的一个，即图（c）中的红色虚线框。SSD的输出是事先定义好的，一系列固定大小的bounding boxes。 Matching strategy：对于每一个ground truth box，选择一些位置不同，高宽比以及尺度不同的default box。首先把每个ground truth box与和它具有最高IoU的默认框匹配，确保每个真实标签框有一个匹配的default box。然后把这些default box与和它IoU高于阈值（0.5）的任何ground truth box匹配起来。添加这些匹配简化了学习问题：当存在多个重叠的default box时，网络可以预测多个较高的分数，而不是只能选取一个具有最大IoU的框。 Choosing scales and aspect ratios for default boxes： 大部分CNN网络在越深的层，feature map的尺寸会越来越小。这样做不仅仅是为了减少计算与内存的需求，还有个好处就是，最后提取的feature map就会有某种程度上的平移与尺度不变性。为了处理不同尺度的物体，这些网络将图像转换成不同的尺度，然后独立的通过CNN网络处理，再将这些不同尺度的图像结果进行综合。本文通过用单个网络中的若干不同层的特征图来进行预测，可以处理不同尺度的目标检测，同时还在所有目标尺度上共享参数。一些文献表明使用来自较低层的特征图可以提高语义分割质量，因为较低层保留的图像细节越多。添加从高层特征图下采样的全局上下文池化可以帮助平滑分割结果。受这些方法的启发，使用低层和高层的特征图进行检测预测。 网络中不同层级的特征图具有不同的感受野大小。这里的感受野，指的是输出的feature map上的一个节点，其对应输入图像上尺寸的大小。在SSD框架内，默认框不需要对应于每层的实际感受野。我们可以设计默认框的平铺，使得特定特征图，学习响应特定尺度的目标。 通过组合多个特征图在每个位置不同尺寸和宽高比的所有默认框的预测，得到了具有多样化的预测集合，覆盖各种目标尺寸和形状。例如狗被匹配到4×4特征图中的默认框，但不匹配到8×8特征图中的任何默认框。这是因为那些默认框具有不同的尺度但不匹配狗的bounding box，因此在训练期间被认为是负样本。 Hard negative mining：在匹配步骤之后，大多数默认框都是负样本，特别是当可能的默认框数量很大时。这导致了训练期间正负样本的严重不平衡。所以按照每个默认框的最高置信度对它们进行排序，并选择前面的那些，使得负样本和正样本之间的比率最多为3：1，以代替使用所有的负样本。这导致更快的优化和更稳定的训练。 Result Reference 论文原文 论文阅读：SSD SSD算法及Caffe代码详解 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO]]></title>
    <url>%2F2018%2F04%2F13%2FYOLO%2F</url>
    <content type="text"><![CDATA[过去的目标检测都是用分类器去实现检测，本文构造的检测器，将目标检测作为一种回归空间分离的边界框和相关类概率的问题。一次预测中，一个神经网络（single network）直接从整幅图像中预测出边界框和类别概率，是一种端到端的实现。 Introduction当前的检测系统是把检测作为分类问题。系统使用目标分类器，并且在图像中不同位置和尺度上进行判断。如R-CNN先提出候选框，然后对每个候选框使用分类器，分类之后精修边界框并且去除重复的框。这种方式速度慢并且很难优化，因为每一个独立的部分都必须单独训练。 本文将目标检测重新设计为一种单一的回归问题，直接从图像像素得到边界框坐标和类别概率。在图像中只需要看一次（You Only Look Once）就知道目标是否存在，存在于哪里。 YOLO的大致流程如下图：一个单卷积网络同时预测多个bounding box和相应的类别概率。首先对输入图像缩放到448×448，然后对其运行单卷积网络，最后使用非极大值抑制消除重复框。 Unified Detection YOLO中，输入图像被划分为S×S的网格，如果一个目标的中心落在某一个网格中，则该网格负责检测这个目标。 每一个网格预测出B个bounding box以及这些box对应的置信度分数，这些分数反映了模型有多大的把握认为这个box包含一个目标并且预测的bounding box有多精确。Bounding box信息包含5个数据值，分别是x,y,w,h,和confidence。其中x,y是指当前格子预测的物体的bounding box中心位置的坐标。w,h是bounding box的宽度和高度。confidence = Pr(Object)*IoU，如果该网格没有目标，则Pr(Object)为0，否则为1。所以当网格里有目标时，condifence应该与预测值和ground truth的IoU相等。 每一个网格也预测C个类别条件概率$Pr(Class_i|Obeject)$，表示在该网格包含目标的前提下，目标是某种类别的概率。不管一个网格预测多少个bounding box，总之对这个网格预测出C个类别条件概率。confidence是针对每个bounding box的，而类别条件概率是针对每个网格的。 上面中，组合每个网格预测的检测框和类条件概率，不仅得到了每个候选框的位置还得到了对应的类别概率。最后使用NMS消除重叠的框。 总结一下SSD的思想：将输入图像划分为SxS的网格，对每一个网格，预测B个bounding box，每个bounding box包含4个位置信息和1个bounding box置信度分数；同时对每一个网格还预测了C个类别条件概率。那么对一幅图，就会得到SxS(Bx5+C)的tensor。 作者在VOC数据上使用的是S=7，B=2，C=20，也就是最终得到一个7x7x30的tensor。 Network Design检测网络一共有24个卷积层和2个全连接层。其中可以看到1×1的降维层和3×3的卷积层的组合使用。 TrainingYOLO模型训练分为两步： 预训练。使用ImageNet中1000类数据训练YOLO网络的前20个卷积层 + 1个average池化层 + 1个全连接层。训练图像分辨率resize到224x224。 回到前面的网络图，加入4个卷积层和2个全连接层，构成YOLO网络。用上一步骤得到的前20个卷积层网络参数来初始化YOLO模型前20个卷积层的网络参数，然后用VOC中20类标注数据进行YOLO模型训练。检测要求细粒度的视觉信息，在训练检测模型时，将输入图像分辨率resize到448x448。 训练时，每个目标被匹配到对应的某个网格，训练过程中调整这个网格的类别概率，使真实目标类别的概率最高，其它的尽可能小，在每个网格预测的bounding box中找到最好的那个，并且调整它的位置，提高置信度，同时降低其它候选框的置信度。对于没有匹配任何目标的网格，降低这些网格中候选框的置信度，不用调整候选框位置和类别概率。 ## LossYOLO使用平方和误差作为loss函数来优化模型参数。- 位置误差（坐标、IoU）与分类误差对网络loss的贡献值是不同的，因此YOLO在计算loss时，使用权重为5的因子来修正位置误差。- 在计算IoU误差时，包含物体的格子与不包含物体的格子，二者的IOU误差对网络loss的贡献是不同的。若采用相同的权值，那么不包含物体的格子的confidence值近似为0，变相放大了包含物体的格子的confidence误差在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用权重为0.5的因子修正IoU误差。（注此处的“包含”是指存在一个物体，它的中心坐标落入到格子内）。- 对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为，计算位置偏差时，大的bounding box上的误差和小的bounding box上的误差对各自的检测精确度影响是不一样的（小误差对应于大检测框，影响很小，而对于小检测框影响较大）。YOLO将物体大小的信息项（w和h）进行求平方根来改进这个问题。（注：这个方法并不能完全解决这个问题）。综上，YOLO在训练过程中Loss计算如下式所示： Limitations of YOLOYOLO的局限性： Bounding box预测上的空间限制，因为每一个网格只预测2个box，并且最终只得到这个网格的目标类别，因为当目标的中心落入网格时，这个网格专门负责这一个目标。这种空间局限性限制了模型预测出那些挨得近的目标的数量（例如一个网格里可能会有多个小目标），YOLO对小目标检测性能并不好。 使用相对粗糙的特征去预测，影响了检测效果。因为网络中对输入图像进行了多次下采样。 loss方程对小bounding box和大bounding box上误差的处理是相同的。一般大边界框里的小误差是良性的，而小边界框里的小误差在IoU上有着更大的影响。虽然采用求平方根方式，但没有根本解决问题，从而降低了物体检测的定位准确性。 Comparison to Other Detection SystemsR-CNN：生成proposal然后卷积网络提取特征，再做分类并且调整bounding box，这种复杂的流程中每一阶段都需要独立且精细地调整，因此速度慢。YOLO：把空间限制放在网格的proposal上，这帮助缓解了同一个目标的重复检测。YOLO提取的bounding box更少（98，而选择性搜索2000）。Fast和Faster R-CNN：致力于通过共享计算以及使用神经网络代替选择性搜索去提取proposal从而加速R-CNN，但是也只是在R-CNN基础上有一定的精度和速度的提升，仍然达不到实时。 YOLO模型相对于之前的物体检测方法有多个优点： YOLO检测物体非常快。因为没有复杂的检测流程，YOLO可以非常快的完成物体检测任务。标准版本的YOLO在Titan X的GPU上能达到45 FPS。更快的Fast YOLO检测速度可以达到155 FPS。而且，YOLO的mAP是之前其他实时物体检测系统的两倍以上。 YOLO可以很好的避免背景错误，避免产生false positives。 不像其他物体检测系统使用了滑窗或region proposal，分类器只能得到图像的局部信息。YOLO在训练和测试时都能够看到一整张图像的信息，因此YOLO在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。和Fast R-CNN相比，YOLO的将背景错误判断为目标的比例不到Fast R-CNN的一半。 YOLO可以学到物体的泛化特征。当YOLO在自然图像上做训练，在艺术作品上做测试时，YOLO表现的性能比DPM、R-CNN等之前的物体检测系统要好很多。因为YOLO可以学习到高度泛化的特征，从而迁移到其他领域。 尽管YOLO有这些优点，它也有一些缺点： YOLO的物体检测精度低于其他state-of-the-art的物体检测系统。 YOLO容易产生物体的定位错误。 YOLO对小物体的检测效果不好（尤其是密集的小物体，因为一个栅格只能负责1个目标）。 Reference 论文原文 YOLO详解 YOLOv1论文理解 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FaceNet:A Unified Embedding for Face Recognition and Clustering]]></title>
    <url>%2F2018%2F04%2F10%2FFaceNet%2F</url>
    <content type="text"><![CDATA[这篇文章是Google发表在CVPR2015上的文章，提出了FaceNet直接学习人脸图像到一个紧凑的欧氏空间的映射，空间距离直接对应于面部相似度的测量。主要的创新点就是提出使用triplet loss，并且得到的是128维的特征。在LFW数据集上达到了99.63%的准确率，在YouTube Faces DB达到了95.12%的准确率。 Introduction这篇文章设计了一个统一的系统，用来做人脸验证和人脸识别，以及聚类。 人脸验证（face verification）：是不是同一个人？一对一 人脸识别（face recognition）：是哪一个人？一对多 人脸聚类（face clustering）：在这些人脸中寻找一类人，比如血亲，双胞胎等 提出使用深度卷积神经网络学习人脸到欧式空间的映射，以使欧式空间中的L2平方距离可以直接对应于人脸相似度：在欧氏空间中同一个人的面部特征有着较小的距离，而不同的人的面部特征有着较大的距离。 上图展示了光照和角度变换下，图像对之间的距离。距离为0的话意味着人脸是相同的，而距离为4.0意味着是两个不同的身份。纵向是不同人的图像对，横向是同一个人的图像对，可以看到，取1.1的阈值，可以正确地分类是否为同一个人。 这样解决前面的三种任务就很直接了：人脸验证就只涉及两个图片之间距离的阈值；人脸识别变成了一个K-NN问题；人脸聚类可以通过现有的（off-the-shelf）技术，比如k-means或者凝聚聚类（agglomerative clustering）。 先前的基于深度网络的人脸识别使用一个分类层在已知人脸身份的训练集上进行学习，然后使用中间的瓶颈层（bottleneck layer）作为特征表达去泛化识别性能。这些方法不够直接也不够高效，因为要寄希望于bottleneck layer，让特征表达对新的人脸有很好的泛化性能，并且使用bottleneck layer，特征表达的尺寸也有1000s的维度。 FaceNet使用基于LMNN的三元组损失（triplet-based loss）直接训练出紧凑的128-D的特征。三元组包括：两个匹配的人脸缩略图、一个不匹配的人脸缩略图。loss的目标就是通过距离间隔（margin）将正样本对和负样本分离开。 Method对比两种核心的网络结构：一个是Visualizing and UnderstandingConvolutional Networks这篇文章中的网络，另一个是Inception。FaceNet的结构如图。包含一个batch input layer和一个深度CNN，而后是L2标准化（L2 Normalization），产生所谓的Face Embedding，这个奇怪的词汇face Embedding就是人脸图像到欧氏空间的映射，前面做的就是特征提取。训练过程中最后还有一个triplet loss。 Triplet Loss这种embedding使用$f(x) \in R^d$表达，将输入图像$x$嵌入到$d$维的欧式空间中。此外，这里限制它在$d$维超球面，即$||f(x)||_2 = 1$。 在该空间内，要确保对于某个特定的人，他的anchor图像$x_i^a$与所有其他的正样本$x_i^p$距离近，与任何负样本$x_i^n$距离远。即类内距离加上间隔小于类间距离。表示为：$$||f(x_i^a) - f(x_i^p)||^2_2 + \alpha &lt; ||f(x_i^a) - f(x_i^n)||^2_2~~~~ \forall(f(x_i^a), f(x_i^p), f(x_i^n)) \in \tau$$$\alpha$是用在正负样本对之间的间隔margin。不仅要使anchor与正样本之间的距离小于它和负样本之间的距离，而且要小到某种程度。（SVM中margin不也是这个作用？）。$\tau$是训练集中所有可能的三元组（triplet），共有$N$组。 损失定义为：$$L = \sum_i^N \left[ ||f(x_i^a) - f(x_i^p)||^2_2 - ||f(x_i^a) - f(x_i^n)||-2^2 + \alpha \right]_+$$这个式子右下角有个$+$号，表示：对于每一组三元组，如果不满足目标不等式时会产生一个为正数的差，损失值就是这个差值，而如果满足目标不等式，损失值会小于0，就取损失值为0。就是对于不满足条件的三元组，进行优化，满足的就不管了。所以目标就是最小化这个损失，让anchor靠近postive而远离negtative，如下图： Triplet Selection如何选择三元组是训练的关键。太简单的三元组对训练网络没有帮助，只有那些比较难的三元组才会改善网络性能，加速收敛。所以关键的是选择出那些违背上面不等式的三元组。 给定$x_i^a$，要选择的难的正样本需要使anchor和正样本之间的距离大$$argmax_{x_i^p}||f(x_i^a) - f(x_i^p)||_2^2 $$ 相似地，选择难的负样本要使anchor和负样本之间的距离小$$argmin_{x_i^n}||f(x_i^a) - f(x_i^n)||_2^2$$ 文章中说选择最难的负样本会导致在训练早期得到局部最优解，因此不选择最难的负样本，而是选择较难的（semi-hard），也就是说负样本比正样本远离anchor，同时anchor-negtative的距离接近anchor-postive的距离。 生成三元组时，每个mini-batch中对每个身份选择40个人脸，并且随机采样一些负样本人脸。文中mini-batch是1800个样本。 训练时使用SGD和AdaGrad优化。学习率开始设为0.05，$\alpha$设为0.02。好吧，模型随机初始化，以超强的自信心在CPU集群上丧心病狂地训练了1000~2000小时，也就是41~83天。训练了500个小时后，loss和准确率的变化才减慢。 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepID:Deep Learning Face Representation from Predicting 10,000 Classes]]></title>
    <url>%2F2018%2F04%2F10%2FDeepID%2F</url>
    <content type="text"><![CDATA[这篇文章提出使用深度学习去学习到一个高级的特征表达集合DeepID用于人脸验证。DeepID特征是从深度卷积神经网络的最后一个隐含层神经元激励提取到的。并且这些特征是从人脸的不同区域中提取的，用来形成一个互补的过完备的人脸特征表达。 Introduction当前有着最优表现性能的人脸验证算法采用的是过完备的低级别特征，并且使用的是浅层模型。本文提出使用深度模型来学习高级的人脸特征集，也就是，把一个训练样本分入10000个身份中的一个。高维空间的操作虽然更有难度，但学习到的特征表达有更好的泛化性能。尽管是通过识别任务学习的，但是这些特征也可用于人脸验证或者数据集中没有出现过的人脸。 特征提取过程如下：卷积神经网络通过学习，将训练集中所有人脸根据他们的身份进行分类。使用多个ConvNet，每一个提取最后一个隐含层神经元的激励作为特征（Deep hidden IDentity features, DeepID）。每一个ConvNet取一个人脸patch作为输入并且提取底层（bottom layers）的局部低级特征（low-level），随着更多全局的高级特征逐渐在顶层（top layer）形成，特征的数量沿着特征提取级联（feature extraction cascade）持续减少。在级联的最后形成了一个高度紧凑的160-d DeepID特征，它包含了丰富的身份信息，用来预测一个数量庞大的身份类别。 分类所有的身份而不是训练二分类基于两点考虑 分类训练样本到多类别中的一个比起实现二分类更加困难。这样可以充分利用神经网络强大的学习能力提取有效的人脸特征用于识别。 多分类给卷积神经网络潜在地增加了强正则化，这有助于形成共享的隐含的特征表达，更好地分类所有的身份。 限制DeepID的维度明显小于要预测的类别数，是学习高度紧凑的具有辨识力特征的关键。进而拼接从不同的人脸区域提取到的DeepID形成互补的过完备特征表达。 在仅使用弱对齐的人脸的情况下，在LFW数据集上达到了97.45%的人脸验证准确率。同时也观察到，随着训练身份数量的增加，验证的性能也在稳步提升。 Deep ConvNets这里的卷积神经网络共有4个卷积层，前3个每个都跟随着最大池化。卷积层之后是全连接的DeepID layer和softmax layer。DeepID layer是固定的160维，输出层softmax的维度随着预测类别数目而变化。 最后一个隐含层通过全连接与第三个和第四个卷积层相连，目的是可以得到多尺度的特征，因为第四个卷积层的特征比第三个更加的全局。这对于特征学习很关键，随着级联中连续的下采样，第四个卷积层包含了太少的神经元，变成了信息传播的瓶颈（bottleneck），所以在第三个和最后一个隐含层之间加入这种旁路连接（bypassing connection），即跳过某些层，最后一个隐含层减少了第四个卷积层可能的信息损失。 Feature extraction人脸对齐部分只用了5个点，两个眼睛的中心，鼻尖，两个嘴角。从人脸图像中选取10个区域，3种尺度，RGB和灰度图像一共60（10x3x2）个人脸patch来做特征提取。下图展示了10个人脸区域和具有3种尺度的两个特定人脸区域。 训练60个ConvNet，每一个ConvNet都从特定的patch和它的水平翻转中提取160维的DeepID向量。除了两眼中心和两个嘴角附近的patch不用自身翻转，而是使用对称的patch，也就是说左眼中心的patch的翻转是通过翻转右眼中心的patch得到的。这样每个ConvNet得到的整个DeepID的长度是$160\times2\times60$。 Face verification使用联合贝叶斯（Joint Bayesian）方法进行人脸验证，同时训练了一个神经网络做对比试验。输入层接受DeepID特征，输入的特征被分为60组，每一组包含了使用特定的ConvNet和patch对的640个特征（一个patch是320维的，验证需要两个人脸图像，因此就是一对patch）。同一组的特征是高度相关的。 局部连接层（locally-connected layer）的神经元只和一组特征连接，学习它们的局部相关性，同时降维。第二个隐含层式全连接层，学习全局的联系。输出只有1个神经元，以全连接的方式和前一层相连。隐含层用的激活函数是ReLU，输出层时sigmoid。 由于对输入的神经元不能使用dropout，因为输入的是高度紧凑的特征，并且是分布式表达，必须共同使用从而表达身份。但是高维的特征如果不用dropout会容易出现梯度弥散的问题。所以先训练如下图所示的60个子网络，每一个都以单组特征作为输入。然后使用子网络第一层的权重初始化原始网络中对应的部分，再微调原始网络第二和第三层。 Experiments数据集：在CelebFaces数据集（87628幅图像，每个人平均16张）上训练模型，在LFW上进行测试。训练时，80%的数据即4349张用来学习DeepID，使用剩下的学习人脸验证模型。 人脸验证：在学习联合贝叶斯模型之前，使用PCA降维将特征维度降为150维。验证的性能在很大的维度范围内都可以保持稳定。 Multiscale ConvNets如前面所示，将第三个卷积层最大池化后的部分直接连接到最后一个隐含层，通过去掉这个连接和带上这个连接进行比较，发现准确率从95.35%提升到了96.05。 Learning effective features指数级增加身份类别，通过top-1误差率观察分类能力，根据测试集的确认精确度观察学习到的隐藏层特征的性能。发现同时对大量的身份进行分类对学习到具有判别性和紧致的隐藏特征是关键。 Over-complete representation为了评估少个patch的组合对性能的贡献大。分别选择了1,5,15,30,60个patch组合的特征训练人脸验证模型。结果表明：提取更多的特征，性能更好。 Reference 论文原文 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rapid Object Detection using a Boosted Cascade of Simple Features]]></title>
    <url>%2F2018%2F03%2F07%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[这篇文章是人脸检测的经典，提出一种基于机器学习的视觉目标检测方法，主要有三点贡献：第一，引入“积分图”概念，可以被检测器用来快速计算特征。第二，学习算法基于AdaBoost，可以从很大的集合中筛选出少量的关键视觉特征并形成更加高效的分类器。第三，以“级联”形式逐渐合并复杂分类器的方法，该方法使得图像的背景区域被很快丢弃，从而将更多的计算放在可能是目标的区域上。 Introduction本文建立了一个正面的人脸检测系统，常规700 MHz英特尔奔腾III，人脸检测速度达到了每秒15帧。其他的人脸检测系统提高帧率是利用视频序列中的图像差异，或者彩色图像中像素的颜色，本文的系统仅仅利用灰度图像信息就实现了高帧率。 三点贡献： 提出积分图，快速地计算特征。在一幅图像中，每个像素使用很少的一些操作，就可以计算得到积分图。任何一个Haar-like特征可以在任何尺度或位置上被计算出来，且是在固定时间内。 提出了一种通过AdaBoost选择少量重要特征的方法。在任何的图像子窗口中，Haar-like特征数量是非常大的，通常远大于像素数量。为了获得快速的分类，学习算法必须排除掉获取的特征中的大部分，关注少量关键特征。通过对AdaBoost程序简单修改：约束弱学习器，使得返回的弱分类器只依赖于一个简单特征。因此，boosting过程的每一个阶段，选择一个新的弱分类器，这些阶段可以被视为特征选择过程。 提出了一种以级联的方式逐渐合并更加复杂的分类器的方法，通过关注图像中那些更有希望的区域，这大大地提高了检测速度。 这些没有被最初的分类器排除的子窗口，由接下来的一系列分类器处理，每个分类器都比其前一个更复杂。如果一个子窗口被任何分类器拒绝了，则它就不再被进一步处理。 Features使用三种特征： 双矩形特征：其值定义为两个矩形区域里像素和的差。这两个区域有着相同的尺寸和形状，并且水平或垂直连接。如图A和B。 三矩形特征：其值定义为两个外矩形像素和减去中间矩形像素和。如图C。 四矩形特征：其值定义为对角线上的矩形对的差。如图D。 Integral Image积分图：某个位置上的左边和上边的像素点的和。位置(x,y)上的积分图像包含点(x,y)上边和左边的像素和。如下式，ii(i,y)是(x,y)位置的积分图，i(x,y)是原始图像的像素值。 使用下面的两个式子迭代，其中s是累计行和，s(x,-1)=0，ii(-1,y)=0： 使用积分图像可以把任意一个矩形用四个数组引用计算，例如下图中的D，位置1上的积分图的值是矩形A的像素和，位置2上积分图的值是A+B，位置3则是A+C，位置4是A+B+C+D，矩形D的和可以计算为：4+1-(2+3)： 由上图也可以看出，两个矩形像素和之间的差可以通过8个数组引用来计算。因为双矩形特征涉及到两个相邻矩形的和，所以仅用6个数组引用就可以计算出结果。同理三矩形特征用8个，四矩形特征用9个。 Learning Classification Functions本文中，AdaBoost的一个变体被用于选择一个小集合的特征并且训练分类器。原始的AdaBoost学习算法被用于加强简单（弱）分类器的性能。每一个图像子窗口相关的特征超过180000，远超过像素的数量。而这些特征中只有一小部分可以被组合形成一个有效的分类器。所以主要的挑战是找到这小部分的特征。 弱学习器用来选择能将正负样本最好的分离的单个特征。对于每一个特征，弱学习器确定最优的阈值分类函数，以使被误分类的样本数量最少。弱分类器hj(x)包括：特征fj，阈值θj，和一个正负校验pj，表示不等号的方向，x是24×24的图像子窗口。 学习算法如下： Learning Result 对于人脸检测的任务，由AdaBoost选择的最初的矩形特征是有意义的且容易理解。选定的第一个特征的重点是眼睛区域往往比鼻子和脸颊区域更黑。此特征相对于检测子窗口较大，并且某种程度上不受面部大小和位置的影响。第二个特征选择依赖于眼睛的所在位置比鼻梁更暗 The Attentional Cascade本章提出的构建级联分类器的算法，它能增加检测性能从而从根本上减少计算时间。主要观点是构建一种优化分类器，其规模越小就越高效。这种分类器在检测几乎所有的正样本时剔除许多负子窗口（即，优化分类器阈值可以调整使得false negative率接近零）。在调用较复杂的分类器之前，我们使用相对简单的分类器来剔除大多数子窗口，以实现低false negative率。 在检测过程中，整体形式是一个退化决策树，称之为“级联”(cascade)。从第一个分类得到的有效结果能触发第二个分类器，它已经调整达到非常高的检测率。再得到一个有效结果使得第二个分类器触发第三个分类器，以此类推。在任何一个点的错误结果都导致子窗口立刻被剔除。 级联阶段的构成首先是利用AdaBoost训练分类器，然后调整阈值使得false negative最大限度地减少。注意，默认AdaBoost的阈值旨在训练数据中产生低错误率。一般而言，一个较低的阈值会产生更高的检测速率和更高的false positive率。 一个双特征强分类器通过降低阈值，达到最小的false negatives后，可以构成一个优秀的第一阶段分类器。测量一个训练集时，阈值可以进行调整，最后达到100%的人脸检测率和40%的正误视率。 Training a Cascade of Classifiers在实践中用一个非常简单的框架产生一个有效的高效分类器。级联中的每个阶段降低了false negatives并且减小了检测率。现在的目标旨在最小化false negatives和最大化检测率。调试每个阶段，不断增加特征，直到检测率和false negatives的目标实现（这些比率是通过将探测器在验证集上测试而得的）。同时添加阶段，直到总体目标的false negatives和检测率得到满足为止。 ResultScanning the Detector最终的检测器在多个尺度和位置上扫描图像。尺度缩放是缩放检测器自身而不是缩放图像。这个过程有效是因为特征可以在任意尺度下被评估。使用1.25的间隔可以得到良好结果。 检测器也在位置上扫描。后续位置的获得是通过将窗口平移⊿个像素获得的。这个平移过程受检测器的尺度影响：若当前尺度是s，窗口将移动[s⊿]，这里[]是指取整操作。⊿的选择不仅影响到检测器的速度还影响到检测精度。我们展示的结果是取了⊿=1.0。通过设定⊿=1.5，我们实现一个有意义的加速，而精度只有微弱降低。 Integration of Multiple Detections因为最终检测器对于平移和尺度的微小改变是不敏感的，在扫描一幅图像时每个人脸通常会得到多检测结果，一些类型的false positives也是如此。在实际应用中每个人脸返回一个最终检测结果才显得比较有意义。 在这些试验中，用非常简便的模式合并检测结果。首先把一系列检测分割成许多不相交的子集。若两个检测结果的边界区重叠了，那么它们就是相同子集的。每个不相交的集合产生单个最终检测结果。最后的边界区的角落定义为一个集合中所有检测结果的角落平均值 A simple voting scheme to further improve results运行三个检测器的结果（一个本文描述的38层检测器加上两个类似的检测器），输出投票得票数高的结果。在提高检测率的同时也消除很多false positives率，且随检测器独立性增强而提高。由于它们的误差之间存在关联，所以对于最佳的单一检测器，检测率是有一个适度提高。 Reference 论文原文 AdaBoost中利用Haar特征进行人脸识别算法分析与总结 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
        <tag>face detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks]]></title>
    <url>%2F2018%2F03%2F01%2FFaster-R-CNN%2F</url>
    <content type="text"><![CDATA[这篇论文发表在NIPS2015上。region proposal计算出现瓶颈，因此引入RPN网络，与检测网络共享整幅图像的的卷积特征，所以region proposal几乎是没有代价的。Faster R-CNN能够达到实时，并且精确度高。 IntroductionR-CNN实际扮演的是分类器，它并不能预测出检测框，只能对检测框进行精修，因此它的准确度主要取决于selective search部分。在之前的方法中proposal是预测时间的瓶颈。 本文使用深度卷积神经网络RPN(Region Proposal Network)计算proposal。在卷积特征的顶部，通过增加一些额外的卷积层，能够在一个规则网格上的每个位置同时回归bounding box并且给出目标得分。RPN是一种全卷积网络，并且可以端到端地训练从而生成proposal。 网络结构如下： 与R-CNN，SPP-Net，Fast R-CNN相比，Faster R-CNN的主要不同在于，不是在原始图像上提取proposal，而是先对原始图像进行卷积得到feature map，然后利用RPN在feature map上提取proposal。 Faster R-CNNFaster R-CNN由两个模块构成：用于生成region的全卷积网络RPN；Fast R-CNN检测器。RPN模块告诉Fast R-CNN检测器应该“看”哪里。 Region Proposal Networks (RPN)RPN采用任意尺寸的图像作为输入，然后输出一系列的带有目标得分（是否有目标）的proposal。这一过程使用一个全卷积网络。 为生成region proposal，在最后一个共享的卷积层输出的feature map上使用一个n×n的滑动窗（本文n=3） 在每一个滑动窗的位置，同时预测多个region proposal，每个位置最大可能的region proposal个数定义为k，文中是9。reg层输出就为4k，cls层输出为2k个分数，对应于每一个region proposal为目标或非目标的概率。 每一个anchor被放置在滑动窗的中心。默认使用3个尺度和3个高宽比，所以每一个滑动窗位置上有9个anchor。对于一个W×H的卷积feature map而言，共有WHk个ahchor。 每一个滑动窗被映射为一个低维特征，其实就是一个3x3的卷积核，卷积后得到1个数值，256个通道在一个窗口卷积后就是256-d的特征，后面跟随ReLu激活函数。然后经过其他层，最后被输入进两个并行的全连接层:box-regression layer (reg) 和box-classification layer (cls) 本文的方法具有平移不变性，anchor和相对于anchor的proposal都是平移不变的。也就是说，如果图像中的目标被平移，proposal也应该平移，并且网络依然可以去预测任何位置的proposal。这种平移不变性通过FCN中的方法保证。平移不变性也减少了模型尺寸。 Multi-Scale Anchors as Regression References多尺度预测一般有两种方法： 基于图像/feature金字塔的:SPPnet,Fast R-CNN 在feature map上使用多尺度的滑动窗，如DMP 本文基于anchor金字塔，实现多尺度是根据多尺度和多个高宽比的anchor做分类和回归。这种方法只依赖单尺度的图像和feature map，并且只使用单尺度的filter（feature map上的滑动窗，就是前面说的3x3）。 在SPPnet和Fast R-CNN中，bounding box回归是在从任意大小的RoI池化得到的特征上实现的，回归的权重被所有的region尺寸共享。Faster R-CNN针对不同尺寸，学习k个bounding-box回归器，每一个回归器只负责一个尺度和高宽比，k个回归器之间不共享权重。在后面的Details里会讲到，使用了36个1x1的卷积实现回归的。因为这样的anchor设计，即使特征是固定尺寸或尺度的，也可能预测出不同尺寸的box。 Loss Function 为训练RPN，对每一个anchor分配一个二分类标签：目标或非目标。 把一个正标签分配给两种anchor：与一个ground-truth box有着最高IoU的一个或多个anchor；与任一个ground-truth box的IoU大于0.7的一个anchor。注意一个ground-truth box可能给多个anchor分配了正的标签。（这里我的理解是：每一个ground truth对应的正样本anchor应该与它IoU最高的，但是同时也把那些与它IoU大于0.7的anchor也当做正的，因为涉及到训练深度神经网络需要样本量比较大，因此需要放松条件，不仅是非常符合要求的被选为正样本，也要考虑那些比较符合要求的） 负标签分配给那些与所有的ground-truth box的IoU都低于0.3的anchor。那些与真实值IoU为[0,0.2]的anchor不参与训练。 使用multi-task loss: $$L({p_i},{t_i}) = \frac {1} {N_{cls}} \sum_i L_{cls}(p_i, p_i^*) + \lambda \frac 1 {N_{reg}} \sum_i p_i^* L_{reg}(t_i,t_i^*)$$ 带$p_i^*$（值为1或0）是第i个anchor是否为目标的真实值，只有anchor为正样本时，回归的损失才会被算入。 Training RPNs 一个mini-batch使用一张图像，在其中采样256个anchor，正负anchor的比例是1:1，如果正的样本少于负的，则用负样本去填充这个batch，用这256个anchor计算损失 使用均值为0，标准差为为0.01的高斯分布随机初始化所有新添加的层，其他的共享卷积层使用ImageNet上预训练的模型初始化 Sharing Features for RPN and Fast R-CNN使用交替训练（Alternating Training）实现RPN和Fast R-CNN共享卷积特征，分为4步： 使用ImageNet预训练的模型初始化网络，并且为region proposal任务微调网络。 使用RPN网络生成的proposal，通过Fast R-CNN训练一个单独的检测网络。这个检测网络也是使用ImageNet预训练模型初始化的。到此时，两个网络并没有共享卷积层。 使用检测网络初始化RPN的训练，但是固定共享卷积层，只微调RPN特有的层，实现了两个网络共享卷积层。 固定共享卷积，微调Fast R-CNN特有的层。如此，两个网络共享了相同的卷积层，并且形成了一个统一的网络。 Details 输入图像经过CNN，这里是VGG16，得到feature map feature map首先输入给RPN，做了3x3的卷积，然后分两路：假如前一步3x3卷积后的特征尺寸是是WxHxC，在输入给softmax前使用了18个1x1xC的卷积，得到WxHx18的矩阵，然后分类每个anchor是不是目标；使用36个1x1的卷积，得到WxHx36的矩阵，相当于feature map上每一个位置都有9个anchor，这样得到回归后proposal的位置。 RoI Pooling根据RPN的输出，从feature map里提取proposal对应的特征，并且池化成固定尺寸的输出 最后是全连接层，分类proposal是哪一类目标；回归bounding box Faster R-CNN进行了两次bounding box回归，一次是在RPN网络，针对anchor进行回归，目的是使proposal的位置更加接近真实值；一次是在全连接层之后，进行最后的位置回归。 上图中有4个池化层，VGG的特点是每次都是在池化层改变feature map的尺寸。在RPN开始使用的3x3卷积也是保持feature map尺寸不变的，因此原始图像到RPN的feature map被缩放了16倍，所以要想将proposal映射会原图，只需要乘以一个缩放因子。 对于一幅1000x600的图像，经卷积后得到60x40x9 = 20000个anchor（1000/16,600/16） 在训练阶段，一幅图中会有约6000个anchor会超过图像的边界，这部分被去除掉；在测试阶段，对于越过边界的anchor，把它修正到边界上。 设置0.7的阈值进行NMS后，留下约2000的候选框 再对剩下的排序它们的得分，提取top-N个作为最终的proposal。文中经过试验选取前300个效果最好。 Reference 论文原文 faster-RCNN算法原理详解 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast-R-CNN]]></title>
    <url>%2F2018%2F02%2F28%2FFast-R-CNN%2F</url>
    <content type="text"><![CDATA[这篇文章发表在ICCV2015上，为了改进R-CNN，SPPnet多阶段训练的缺点，以及SPPnet限制了误差的反向传播的缺点，提出了Fast R-CNN。在训练过程中，使用multi-task loss简化了学习过程并且提高了检测准确率。 IntroductionRCNN的三大缺点： 多阶段训练：首先用交叉熵损失微调卷积神经网络；然后线性SVM拟合卷积特征；最后学习bounding-box回归 训练代价高（空间及时间）：从每幅图中的每个region proposal提取的特征需要存储起来 测试慢 R-CNN之所以慢，就是因为它独立地warp然后处理每一个目标proposal。流程如下： 提取proposal -&gt; CNN提取feature map -&gt; SVM分类器 -&gt; bbox回归 SPPnet的提出是为了加速R-CNN。但是具有以下缺点： 同R-CNN一样，多阶段，特征需要被写入磁盘 不同于R-CNN的是：微调算法只更新那些跟随在SPP layer后的全连接层。 Fast R-CNN的贡献 比R-CNN更高的检测质量（mAP） 训练时单阶段的，使用multi-task loss 在训练过程中，所有的网络层都可以更新 不需要对特征存入磁盘R-CNN，SPPNet在检测器的训练上都是多阶段的训练，训练起来困难并且消耗时间。SPP-Net限制了训练过程中误差的反向传播，潜在地限制了精确度；目标候选位置需要被精修，过去的精修是在一个单独的学习过程中训练的，Fast-RCNN是对检测器的训练是单阶段的。 Fast R-CNN Training网络结构上：卷积+池化层 -&gt; RoI pooling layer -&gt; 全连接层。两个并行的层：一个输出类别概率，一个输出四个实值即bounding box。 RoI pooling layerRoI pooling layer是SPPnet中SPP layer的简化版本，相当于金字塔只有一级。SPP-Net中设置了不同样子的网格，比如4x4，2x2，1x1的。 RoI pooling layer的输入是N个feature map和R个感兴趣的区域构成的列表， R&gt;&gt;N N个feature map是由网络的最后一个卷积层提供的，并且每一个都是多维矩阵H×W×C。 每一个RoI是一个元组（n,r,c,h,w），指定了feature map的索引n（n为0~N-1）和RoI的左上角位置（r,c）以及高和宽（h,w）。 RoI pooling层输出H&#39;× W&#39;的feature map，通道数和原始的feature map一样（其中，H’ &lt;= H, W’ &lt;=W）。 RoI pooling的具体操作如下： 首先将RoI映射到feature map对应的位置 将映射后的区域划分为一定大小的块（bin），尺寸为h/H&#39; × w/W&#39;，h是feature map中ROI的高，H&#39;是要求输出的feature map的高。 对每一个块进行max pooling操作 如下图，输入8×8的feature map，一个RoI（黑色的大框），希望的输出是2×2的。 首先找到RoI在feature map中的位置，其大小为7x5；映射后的区域划分为3×2（7/2=3,5/2=2）的块，可能出现如图中，不能整除的情况；最后对每一个块单独进行max pooling，得到要求尺寸的输出。 整个检测框架表示为： 总结一下，Fast R-CNN先用基础网络提取完整图像的feature map，将selective search提取的候选框作为RoI，把feature map和RoI输入给RoI pooling layer，在feature map中找到每一个RoI的位置，根据需要的输出尺寸，把那部分feature map划分网格，对每一个网格应用最大池化，就得到了固定尺寸的输出特征。 Using pretrained networks使用预训练的网络初始化Fast R-CNN，要经历三个转变： 最后一个最大池化层使用RoI池化层替代。通过设置RoI pooling layer的输出尺寸H&#39;和W&#39;与网络第一个全连接层兼容。 网络的最后一个全连接层和softmax被替代为两个并行的层。 网络采取两个数据输入：batch size为N的输入图像和R个RoIs的列表。 SPP-Net最后是一个3层的softmax分类器用于检测（SPP layer后面是两个全连接层，和一个输出层）。由于卷积特征是离线计算的，所以微调过程不能向SPP layer以下的层反向传播误差。以VGG16为例，前13层固定在初始化的值，只有最后3层会被更新。 在Fast R-CNN中，mini-batch被分层次地采样，首先采样图像，然后采样这些图像的RoIs。来自同一幅图的RoI共享计算和内存，使得训练高效。 Multi-task lossFast R-CNN是并行地进行类别的确定和位置的精修的，整体的loss由两部分组成，一部分是分类的损失，另一部分是位置回归的损失，因此定义的损失如下，$k^*$为真实的类别标签，$[k^* \ge 1]$表明只对目标类别计算损失，背景类别的$k^*=0$，提取出的RoI是背景的话，就忽略掉：$$L(p,k^*,t,t*) = L_{cls}(p,k^*) + \lambda [k^* \ge 1] L_{loc}(t, t^*)$$ $$L_{loc}(t,t^*) = \sum_{i \in {x,y,w,h}} smooth_{L1}(t_i,t^*_i)$$ $$smooth_{L1}(x)=\begin{cases} 0.5x^2, &amp; if~|x|&lt;1 \ |x|-0.5, &amp; otherwise \end{cases}$$ 对于bounding box回归使用Smooth L1 loss是因为，比起R-CNN中使用的L2 loss，Smooth L1 loss对于离群值不敏感。归一化了ground truth的回归目标$t^*$使其具有0均值和单位方差，这样的情况下设置$\lambda = 1$在实验中效果很好。 Detail 微调中，batch size N = 2，R=128，也就是每一幅图采样了64个RoI N张完整图片以50%概率水平翻转 R个候选框的构成：与某个真实值IoU在[0.5,1]的候选框被选为RoIs；与真实值IoU在[0.1,0.5]的候选框作为背景， 标记类别为$k^*=0$ 多尺度训练中，和SPP-Net一样，随机采样一个尺度，每一次采样一幅图 Reference 论文原文 ROI Pooling层详解 Training R-CNNs of various velocities var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPPNet:Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition]]></title>
    <url>%2F2018%2F02%2F18%2FSPPNet%2F</url>
    <content type="text"><![CDATA[为了解决现有CNN需要固定输入大小的问题，提出了SPP-net，使得针对任意尺寸的图像生成固定长度的特征表示。输入一张图，只需要对整张图进行一次feature map的计算，避免了像R-CNN那样重复地计算卷积特征。SPP-net不仅可以应用在分类任务上，而且在检测任务上也有很大的性能提升。 Introduction在CNN的训练和测试阶段都有一个技术问题：CNN需要固定输入图像的尺寸，这些图片或者经过裁切（crop）或者经过变形缩放（warp），都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。 如下图所示，上面是CNN一般的做法，对不符合网络输入大小的图像直接进行crop或warp，下面是SPP-net的工作方式。SPP-net加在最后一个卷积层的输出后面，使得不同输入尺寸的图像在经过前面的卷积池化过程后，再经过SPP-net，得到相同大小的feature map，最后再经过全连接层进行分类。 Spatital Pyramid PoolingCNN为什么需要固定输入尺寸？卷积层是不需要输入固定大小的图片，而且还可以生成任意大小的特征图，只是全连接层需要固定大小的输入。因此，固定输入大小约束仅来源于全连接层。在本文中提出了Spatial Pyramid Pooling layer来解决这一问题，输入任意尺寸的图像，SPP layer对特征进行池化并生成固定大小的输出，以输入给全连接层或分类器。 以AlexNet为例，经CNN得到Conv5输出的任意尺寸的Feature map，图中256是conv5卷积核的数量。 将最后一个池化层pool5替代成SPP layer。以不同网格来提取特征，分别是4x4，2x2，1x1，将这三张网格放到feature map上，就可以得到16+4+1=21种不同的块(Spatial bins)，对这21个块应用max pooling，每个块就提取出一个特征值，这样就组成了21维特征向量。 这种用不同大小的格子划分feature map，然后对每一个块应用最大池化，将池化后的特征拼接成一个固定维度的特征的方式就是空间金字塔池化。 总结而言，当网络输入一张任意大小的图片，进行卷积、池化，直到即将与全连接层连接的时候，就要使用空间金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义（不同度特征提取出固定大小的特征向量）。 Training理论上，无论输入什么尺寸的图像，都可以用标准的反向传播训练。但是实际上由于GPU实现中，更适合在固定尺寸的输入图像上，因此提出了一些训练策略。 Single-size training:使用固定的224x224的输入，是从原始图像中裁切得到的，目的是为了数据扩增；对于给定的输入尺寸，可以预先计算出空间金字塔池化需要的bin size，假如feature map是axa的大小，那么在SPP layer中，窗口尺寸$win = \frac a n$，步长$stride = \frac a n$。 Multi-size training：考虑两种输入，180x180和224x224，这里不再用裁切，而是直接进行缩放，比如把224x224的图像直接缩放为180x180，它们之间的区别只是分辨率不同。实现两个固定输入尺寸的网络，训练过程中先在1号网络上训练一个epoch，然后用它的权重去初始化2号网络，训练下一个epoch；如此转换训练。通过共享两种尺寸输入的网络参数，实现了不同输入尺寸的SPP-Net的训练。 这样single/multi-size的训练只是在训练中，预测阶段，直接将不同尺寸的图像输入给SPP-Net。 SPP-net for Object Detection对于R-CNN，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出约2000个候选窗口。 把这2000个候选框都缩放到227x227，然后分别输入CNN中，利用CNN对每个proposal进行提取特征向量。 把上面每个候选窗口的对应特征向量，利用SVM算法进行分类识别。 可以看出R-CNN的计算量是非常大的，因为2k个候选窗口都要输入到CNN中，分别进行特征提取。 而对于SPP-Net，整个过程是： 首先通过选择性搜索，对待检测的图片生成2000个候选窗口。这一步和R-CNN一样。 特征提取阶段。与R-CNN不同，把整张待检测的图片，输入CNN中，进行一次特征提取，得到整个图像的feature maps（可能是在多尺度下），然后在feature maps中找到各个候选框对应的区域，对各个候选框采用空间金字塔池化，提取出固定长度的特征向量。因为SPP-Net只需要对整张图片进行一次特征提取，速度会大大提升。文中是几十到一百倍以上。 最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。 下图为SPP-net进行目标检测的完整步骤： Mapping a Window to Feature MapsSPP-Net在提取完整图像的feature map后，要将候选框的位置映射到feature map中得到对应特征。候选框是在原始图像上得到的，而feature maps是经过原始图片卷积、下采样等一系列操作后得到的，所以feature maps的大小和原始图片的大小是不同的。 假设$(x,y)$是原始图像上的坐标点，$(x’,y’)$是特征图上的坐标，S是CNN中所有的步长的乘积，那么左上角的点转换公式如下：$$x’ = \frac x S + 1$$右下角的点转换公式为：$$x’ = \frac x S - 1$$ Reference 论文原文 SPPnet论文总结 SPP-Net论文详解 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-CNN:Rich feature hierarchies for accurate object detection and semantic segmentation]]></title>
    <url>%2F2018%2F02%2F17%2FR-CNN%2F</url>
    <content type="text"><![CDATA[这篇文章发表在CVPR2014上面，是第一篇展示CNN可以在PASCAL VOC数据集上带来明显更好的目标检测性能的文章。提出了一种使用selective search + CNN的两阶段的目标检测方法。创新点在于使用CNN提取特征，在大数据集下有监督的预训练，小数据集上微调解决样本数量少难以训练的问题。 Introduction使用CNN做目标检测需要解决两个问题：一是使用深度网络定位目标；二是使用少量标记数据训练一个高性能的模型。与图像分类不同，目标检测是要在一幅图像中检测出目标，可能有多个。一种方法是将其作为回归问题，比如构建一个滑动窗检测器。 本文提出的R-CNN的整体框架如图，也因为将region和CNN feature组合，所以作者定义为R-CNN（region with CNN features）。 输入一幅图像，产生约2000个类别独立的region proposal 然后使用仿射图像扭曲（affine image warping）将每一个region proposal转换为固定尺寸的CNN输入 再使用CNN从每一个proposal中提取固定长度的特征向量 对每一个类别训练一个SVM，然后使用这些特定类别的线性SVM对每一个区域进行分类。 传统的方法解决数据稀缺问题时，使用无监督的预训练和有监督的微调。这篇文章第二个贡献就是当数据稀缺时，在更大辅助数据集(ILSVRC)上使用监督式预训练，然后在小的数据集（PASCAL）上进行特定域的微调，是一种学习高性能的CNN的有效范例。简而言之，就是使用图像分类中的经典网络作为基础网络，然后在目标检测这种任务上，进行微调。 Object detection with R-CNN分为3个模块： 第一个模块生成类别独立的region proposals，使用selective search生成。就是采取过分割手段，将图像分割成小区域，再通过颜色直方图，梯度直方图相近等规则进行合并，最后生成约2000个候选框 第二个模块是一个大的卷积神经网络，从每一个区域中提取固定长度的特征向量。通过前向传播227x227的图像（减均值），经过5个卷积层和2个全连接层，对每一个region proposal计算得到4096的特征向量 第三个模块是一系列的特定类别的线性SVM。 Test 选择性搜索提取候选区域框，每个候选框周围加上16个像素值为候选框像素平均值的边框，再直接缩放到网络输入227x227 每一个候选框输入到CNN之前，先减去均值，经AlexNet网络提取4096维的特征，2000个候选框就组成2000x4096的矩阵 将2000×4096维特征与20个SVM组成的权值矩阵4096×20相乘，获得每个候选框对应类别的得分，Pascal VOC数据集有20类目标 每一类都有多个候选框，因此要进行非极大值抑制，去掉重叠候选框 用20个线性回归器对得到的每个类别的候选框进行回归，获取目标位置 Training 监督式预训练，在ImageNet数据集上进行，使用图像分类的数据，只有类别标签，没有bounding box。 特定域的微调 只使用来源于VOC的数据做随机梯度下降，以训练CNN参数，初始学习率设为0.001，每一次SGD迭代(这里指的就是mini-batch梯度下降)，在所有类别中均匀采样32个正样本，和96个背景区域，组成一个128的mini-batch 用21way的分类层替代原来的1000way分类层。 根据bounding box的所属的类别，把那些与ground truth的IoU大于0.5的region proposal作为正样本，其余的作为负样本。 目标分类器 一个图像区域紧紧包围住一辆车，则这个区域就是正样本。所以正样本就是bounding box的ground truth。文中选择0.3作为IoU阈值，低于这个阈值的区域作为负样本。 提取好样本的特征，就可以优化每一个类别的SVM。由于训练数据太多，采用标准的难负样本挖掘方法（hard negative mining），可以使训练快速收敛。 这里也可以看出，在微调CNN和训练SVM时，对于正负样本IoU阈值的限定不一样，前者的限定更加宽松，这是因为CNN需要大量的样本，否则会过拟合，而SVM就可以使用相对少量的样本，故限制更严格。 Detail bounding box回归：选择性搜索产生的region proposal输入到CNN中，文章中CNN使用的是AlexNet，将AlexNet的Pool5产生的特征用来训练线性回归模型，从而预测bounding box的位置。在回归中，如果候选框与ground truth距离太远，训练是很困难的几乎没有希望，所以在样本对的选择上，只选择那些与ground truth离得近的，文中通过设置proposal和IoU阈值为0.6，低于阈值的proposal就没有匹配的ground truth，被忽略掉。 目标类别：使用倒数第二个全连接层fc7输出的特征，训练分类器SVM或者softmax 两者的结合：一个region proposal送入卷积神经网络，Pool5和倒数第二个全连接层fc7的特征会先保存下来；然后使用所有类别的SVM对这个proposal预测类别分数，决定是否为某种目标；如果是某种目标，则用相应类别的线性回归去预测得到bounding box的位置。 问题 处理速度慢，主要是因为一张图片产生的约2k个候选框由CNN提取特征时，有很多区域的重复计算 整个测试过程也比较繁琐，要经过两阶段，而且单独进行分类和回归，这些不连续的过程在训练和测试中必然会涉及到特征的存储，因为会浪费磁盘空间。 Reference 论文原文 R-CNN论文详解 var gitment = new Gitment({ id: '页面 ID', // 可选。默认为 location.href 比如我本人的就删除了 owner: 'zhangting2020', //比如我的叫anTtutu repo: 'GitComment', //比如我的叫anTtutu.github.io oauth: { client_id: '60737b1014bda221b290', //比如我的828*********** client_secret: 'ce34df0ac4253419bfaa84df9363844ed0e6f9b8', //比如我的49e************************ }, }) gitment.render('container')]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>object detection</tag>
        <tag>paper</tag>
      </tags>
  </entry>
</search>
